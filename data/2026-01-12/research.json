{
  "category": "research",
  "date": "2026-01-12",
  "category_summary": "Today's research features significant theoretical advances and critical AI safety findings. **PaCoRe** [addresses test-time compute scaling](/?date=2026-01-12&category=research#item-b53216c67c22) through parallel coordinated reasoning with message-passing across multiple rollouts. A foundational result [reveals transformers inherently encode](/?date=2026-01-12&category=research#item-1deb1986fe19) time-delayed causal structures, while formal analysis [proves fundamental self-improvement limits](/?date=2026-01-12&category=research#item-b3d20f2c3e67) via entropy decay and variance amplification.\n\n- **GenCtrl** provides [formal controllability guarantees](/?date=2026-01-12&category=research#item-621bd8c34696) for generative models with algorithmic bounds on controllable output sets\n- **Facade of Truth** [demonstrates LLM vulnerability](/?date=2026-01-12&category=research#item-adf18a27a17f) to sophisticated hard-to-falsify deceptive evidence\n- **Circular Reasoning** [characterizes self-reinforcing loops](/?date=2026-01-12&category=research#item-81ebfa5898ee) as a distinct failure mode in large reasoning models\n- Deanonymization attack [successfully re-identifies participants](/?date=2026-01-12&category=research#item-45ee8c9fcaca) in Anthropic's Interviewer dataset using web search\n\n**Hierarchical Speculative Decoding** [achieves provably lossless verification](/?date=2026-01-12&category=research#item-e98233c50eea) for inference efficiency. Multi-agent analysis [reveals systematic conformity bias](/?date=2026-01-12&category=research#item-506473c4bd45) under social pressure, with direct implications for deployment safety.",
  "category_summary_html": "<p>Today's research features significant theoretical advances and critical AI safety findings. <strong>PaCoRe</strong> <a href=\"/?date=2026-01-12&category=research#item-b53216c67c22\" class=\"internal-link\">addresses test-time compute scaling</a> through parallel coordinated reasoning with message-passing across multiple rollouts. A foundational result <a href=\"/?date=2026-01-12&category=research#item-1deb1986fe19\" class=\"internal-link\">reveals transformers inherently encode</a> time-delayed causal structures, while formal analysis <a href=\"/?date=2026-01-12&category=research#item-b3d20f2c3e67\" class=\"internal-link\">proves fundamental self-improvement limits</a> via entropy decay and variance amplification.</p>\n<ul>\n<li><strong>GenCtrl</strong> provides <a href=\"/?date=2026-01-12&category=research#item-621bd8c34696\" class=\"internal-link\">formal controllability guarantees</a> for generative models with algorithmic bounds on controllable output sets</li>\n<li><strong>Facade of Truth</strong> <a href=\"/?date=2026-01-12&category=research#item-adf18a27a17f\" class=\"internal-link\">demonstrates LLM vulnerability</a> to sophisticated hard-to-falsify deceptive evidence</li>\n<li><strong>Circular Reasoning</strong> <a href=\"/?date=2026-01-12&category=research#item-81ebfa5898ee\" class=\"internal-link\">characterizes self-reinforcing loops</a> as a distinct failure mode in large reasoning models</li>\n<li>Deanonymization attack <a href=\"/?date=2026-01-12&category=research#item-45ee8c9fcaca\" class=\"internal-link\">successfully re-identifies participants</a> in Anthropic's Interviewer dataset using web search</li>\n</ul>\n<p><strong>Hierarchical Speculative Decoding</strong> <a href=\"/?date=2026-01-12&category=research#item-e98233c50eea\" class=\"internal-link\">achieves provably lossless verification</a> for inference efficiency. Multi-agent analysis <a href=\"/?date=2026-01-12&category=research#item-506473c4bd45\" class=\"internal-link\">reveals systematic conformity bias</a> under social pressure, with direct implications for deployment safety.</p>",
  "themes": [
    {
      "name": "AI Safety & Alignment",
      "description": "Research on controlling, monitoring, and ensuring safe behavior of AI systems including jailbreaking, unlearning, controllability, and failure mode analysis",
      "item_count": 12,
      "example_items": [],
      "importance": 85
    },
    {
      "name": "AI Safety & Security",
      "description": "Research on LLM vulnerabilities including jailbreaking attacks, deception susceptibility, hallucination detection, and harmful content identification",
      "item_count": 12,
      "example_items": [],
      "importance": 82
    },
    {
      "name": "LLM Reasoning & Scaling",
      "description": "Advances in language model reasoning capabilities, test-time compute scaling, and policy optimization for reasoning tasks",
      "item_count": 9,
      "example_items": [],
      "importance": 82
    },
    {
      "name": "AI Safety & LLM Security",
      "description": "Research on jailbreaking attacks, defense certification, self-improvement limits, and adversarial robustness of language models",
      "item_count": 8,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "Reinforcement Learning for LLMs",
      "description": "Applications of RL for post-training optimization, reasoning enhancement, and capability improvement in language models",
      "item_count": 17,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "Interpretability & Alignment",
      "description": "Understanding AI representations, moral foundations in LLMs, and steering model behavior",
      "item_count": 5,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "Causal & Interpretable ML",
      "description": "Causal discovery, interpretability, algorithm extraction, and formal reasoning in neural networks",
      "item_count": 6,
      "example_items": [],
      "importance": 72
    },
    {
      "name": "LLM Evaluation & Benchmarking",
      "description": "New benchmarks and evaluation methodologies for assessing LLM capabilities, limitations, and biases across various domains",
      "item_count": 10,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "Multimodal Learning",
      "description": "Vision-language models, visual reasoning, hallucination mitigation, and cross-modal learning",
      "item_count": 13,
      "example_items": [],
      "importance": 70
    },
    {
      "name": "LLM Reliability & Safety",
      "description": "Methods for detecting hallucinations, measuring truthfulness robustness, and ensuring reliable LLM behavior including citation verification and belief consistency",
      "item_count": 7,
      "example_items": [],
      "importance": 70
    }
  ],
  "total_items": 30,
  "items": [
    {
      "id": "b53216c67c22",
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "content": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
      "url": "http://arxiv.org/abs/2601.05593",
      "author": "Jingcheng Hu, Yinmin Zhang, Shijie Shang, Xiaobo Yang, Yue Peng, Zhewei Huang, Hebin Zhou, Xin Wu, Jie Cheng, Fanqi Wan, Xiangwen Kong, Chengyuan Yao, Kaiwen Yan, Ailin Huang, Hongyu Zhou, Qi Han, Zheng Ge, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces PaCoRe, a framework for scaling test-time compute through massive parallel exploration with message-passing coordination across multiple rounds. Trained end-to-end with outcome-based RL to master synthesis across parallel reasoning trajectories.",
      "importance_score": 82,
      "reasoning": "Addresses critical limitation of sequential reasoning under fixed context windows. Novel parallel coordination architecture with end-to-end RL training. High potential impact on inference-time scaling research.",
      "themes": [
        "Test-Time Compute",
        "Reasoning",
        "Reinforcement Learning",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces PaCoRe, a framework for scaling test-time compute through massive parallel exploration with message-passing coordination across multiple rounds. Trained end-to-end with outcome-based RL to master synthesis across parallel reasoning trajectories.</p>",
      "content_html": "<p>We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.</p>"
    },
    {
      "id": "1deb1986fe19",
      "title": "Transformer Is Inherently a Causal Learner",
      "content": "We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.",
      "url": "http://arxiv.org/abs/2601.05647",
      "author": "Xinyue Wang, Stephen Wang, Biwei Huang",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Reveals that autoregressive transformers naturally encode time-delayed causal structures in learned representations. Gradient sensitivities directly recover underlying causal graphs without explicit causal objectives.",
      "importance_score": 78,
      "reasoning": "Potentially significant theoretical finding connecting transformers to causal discovery. Strong claims backed by theoretical analysis under identifiability conditions. Could impact interpretability research.",
      "themes": [
        "Causal Discovery",
        "Transformers",
        "Interpretability",
        "Machine Learning Theory"
      ],
      "continuation": null,
      "summary_html": "<p>Reveals that autoregressive transformers naturally encode time-delayed causal structures in learned representations. Gradient sensitivities directly recover underlying causal graphs without explicit causal objectives.</p>",
      "content_html": "<p>We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.</p>"
    },
    {
      "id": "e98233c50eea",
      "title": "Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding",
      "content": "Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.",
      "url": "http://arxiv.org/abs/2601.05724",
      "author": "Yuxuan Zhou, Fei Huang, Heng Li, Fengyi Wu, Tianyu Wang, Jianwei Zhang, Junyang Lin, Zhi-Qi Cheng",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes Hierarchical Speculative Decoding (HSD), a provably lossless verification method that increases accepted tokens by balancing probability mass across branches to overcome joint intractability.",
      "importance_score": 76,
      "reasoning": "Important theoretical and practical contribution to inference efficiency. Provable losslessness is significant guarantee. Consistent improvements demonstrated at scale.",
      "themes": [
        "Inference Efficiency",
        "Speculative Decoding",
        "Language Models"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Hierarchical Speculative Decoding (HSD), a provably lossless verification method that increases accepted tokens by balancing probability mass across branches to overcome joint intractability.</p>",
      "content_html": "<p>Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.</p>"
    },
    {
      "id": "b3d20f2c3e67",
      "title": "On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis",
      "content": "We formalise recursive self-training in Large Language Models (LLMs) and Generative AI as a discrete-time dynamical system and prove that, as training data become increasingly self-generated ($\\alpha_t \\to 0$), the system undergoes inevitably degenerative dynamics. We derive two fundamental failure modes: (1) Entropy Decay, where finite sampling effects cause a monotonic loss of distributional diversity (mode collapse), and (2) Variance Amplification, where the loss of external grounding causes the model's representation of truth to drift as a random walk, bounded only by the support diameter. We show these behaviours are not contingent on architecture but are consequences of distributional learning on finite samples. We further argue that Reinforcement Learning with imperfect verifiers suffers similar semantic collapse. To overcome these limits, we propose a path involving symbolic regression and program synthesis guided by Algorithmic Probability. The Coding Theorem Method (CTM) allows for identifying generative mechanisms rather than mere correlations, escaping the data-processing inequality that binds standard statistical learning. We conclude that while purely distributional learning leads to model collapse, hybrid neurosymbolic approaches offer a coherent framework for sustained self-improvement.",
      "url": "http://arxiv.org/abs/2601.05280",
      "author": "Hector Zenil",
      "published": "2026-01-12",
      "source": "arXiv (cs.IT)",
      "source_type": "arxiv",
      "tags": [
        "cs.IT"
      ],
      "summary": "Formalizes recursive self-training in LLMs as dynamical system and proves two fundamental failure modes: entropy decay (mode collapse) and variance amplification (truth drift). Argues these limit self-improvement without external grounding.",
      "importance_score": 75,
      "reasoning": "Significant theoretical contribution on fundamental limitations of LLM self-improvement. Directly relevant to AGI/ASI debates with rigorous mathematical analysis.",
      "themes": [
        "AI Theory",
        "Language Models",
        "Self-improvement",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Formalizes recursive self-training in LLMs as dynamical system and proves two fundamental failure modes: entropy decay (mode collapse) and variance amplification (truth drift). Argues these limit self-improvement without external grounding.</p>",
      "content_html": "<p>We formalise recursive self-training in Large Language Models (LLMs) and Generative AI as a discrete-time dynamical system and prove that, as training data become increasingly self-generated ($\\alpha_t \\to 0$), the system undergoes inevitably degenerative dynamics. We derive two fundamental failure modes: (1) Entropy Decay, where finite sampling effects cause a monotonic loss of distributional diversity (mode collapse), and (2) Variance Amplification, where the loss of external grounding causes the model's representation of truth to drift as a random walk, bounded only by the support diameter. We show these behaviours are not contingent on architecture but are consequences of distributional learning on finite samples. We further argue that Reinforcement Learning with imperfect verifiers suffers similar semantic collapse. To overcome these limits, we propose a path involving symbolic regression and program synthesis guided by Algorithmic Probability. The Coding Theorem Method (CTM) allows for identifying generative mechanisms rather than mere correlations, escaping the data-processing inequality that binds standard statistical learning. We conclude that while purely distributional learning leads to model collapse, hybrid neurosymbolic approaches offer a coherent framework for sustained self-improvement.</p>"
    },
    {
      "id": "506473c4bd45",
      "title": "Conformity and Social Impact on AI Agents",
      "content": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "url": "http://arxiv.org/abs/2601.05384",
      "author": "Alessandro Bellina, Giordano De Marzo, David Garcia",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Studies conformity bias in AI agents operating in multi-agent environments, finding they systematically align with group opinions under social pressure similar to humans. This has critical implications for understanding manipulation vulnerabilities in AI systems deployed in social contexts.",
      "importance_score": 75,
      "reasoning": "Novel empirical finding with direct implications for AI safety and multi-agent system design. Bridges social psychology with AI behavior analysis.",
      "themes": [
        "Multi-Agent Systems",
        "AI Safety",
        "Social Psychology of AI"
      ],
      "continuation": null,
      "summary_html": "<p>Studies conformity bias in AI agents operating in multi-agent environments, finding they systematically align with group opinions under social pressure similar to humans. This has critical implications for understanding manipulation vulnerabilities in AI systems deployed in social contexts.</p>",
      "content_html": "<p>As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.</p>"
    },
    {
      "id": "7d88005ae556",
      "title": "Tracing Moral Foundations in Large Language Models",
      "content": "Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical...",
      "url": "http://arxiv.org/abs/2601.05437",
      "author": "Chenxiao Yu, Bowen Yi, Farzan Karimi-Malekabadi, Suhaib Abdurahman, Jinyi Ye, Shrikanth Narayanan, Yue Zhao, Morteza Dehghani",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Investigates how moral foundations are encoded in LLMs using layer-wise analysis, sparse autoencoders, and causal steering interventions. Examines whether LLMs have genuine moral representations or just mimic moral language.",
      "importance_score": 75,
      "reasoning": "Important interpretability and alignment research using rigorous methodology. Directly relevant to understanding AI values and potential for steering.",
      "themes": [
        "AI Alignment",
        "Interpretability",
        "Ethics in AI",
        "Mechanistic Interpretability"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates how moral foundations are encoded in LLMs using layer-wise analysis, sparse autoencoders, and causal steering interventions. Examines whether LLMs have genuine moral representations or just mimic moral language.</p>",
      "content_html": "<p>Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical...</p>"
    },
    {
      "id": "adf18a27a17f",
      "title": "The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence",
      "content": "To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.",
      "url": "http://arxiv.org/abs/2601.05478",
      "author": "Herun Wan, Jiaying Wu, Minnan Luo, Fanxiao Li, Zhi Zeng, and Min-Yen Kan",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Reveals that LLMs are highly vulnerable to sophisticated, hard-to-falsify deceptive evidence even when robust to explicit misinformation. Introduces MisBelief framework using multi-agent collaboration to generate such evidence.",
      "importance_score": 75,
      "reasoning": "Critical safety finding showing LLMs can be manipulated by sophisticated deception. Important implications for reliability in high-stakes applications.",
      "themes": [
        "AI Safety",
        "LLM Robustness",
        "Misinformation",
        "Adversarial AI"
      ],
      "continuation": null,
      "summary_html": "<p>Reveals that LLMs are highly vulnerable to sophisticated, hard-to-falsify deceptive evidence even when robust to explicit misinformation. Introduces MisBelief framework using multi-agent collaboration to generate such evidence.</p>",
      "content_html": "<p>To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.</p>"
    },
    {
      "id": "621bd8c34696",
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "content": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
      "url": "http://arxiv.org/abs/2601.05637",
      "author": "Emily Cheng, Carmen Amo Alonso, Federico Danieli, Arno Blaas, Luca Zappella, Pau Rodriguez, Xavier Suau",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Provides theoretical framework for formally answering whether generative models are truly controllable. Proposes algorithm to estimate controllable sets in dialogue with PAC bounds that are distribution-free.",
      "importance_score": 75,
      "reasoning": "Important theoretical contribution for AI safety and alignment. Formal guarantees on controllability estimation addresses fundamental question about model behavior.",
      "themes": [
        "AI Safety",
        "Controllability",
        "Formal Methods",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Provides theoretical framework for formally answering whether generative models are truly controllable. Proposes algorithm to estimate controllable sets in dialogue with PAC bounds that are distribution-free.</p>",
      "content_html": "<p>As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.</p>"
    },
    {
      "id": "81ebfa5898ee",
      "title": "Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models",
      "content": "Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.",
      "url": "http://arxiv.org/abs/2601.05693",
      "author": "Zenghao Duan, Liang Pang, Zihao Wei, Wenbin Duan, Yuxin Tian, Shicheng Xu, Jingcheng Deng, Zhiyi Yin, Xueqi Cheng",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Identifies 'Circular Reasoning' as distinct failure mode in Large Reasoning Models where generated content acts as logical premise for its own recurrence. Introduces LoopBench dataset for analysis.",
      "importance_score": 74,
      "reasoning": "Important characterization of failure mode in reasoning models with practical implications. Mechanistic analysis of state collapse provides actionable insights.",
      "themes": [
        "Reasoning Models",
        "Failure Analysis",
        "AI Safety",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Identifies 'Circular Reasoning' as distinct failure mode in Large Reasoning Models where generated content acts as logical premise for its own recurrence. Introduces LoopBench dataset for analysis.</p>",
      "content_html": "<p>Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.</p>"
    },
    {
      "id": "3c4e888eb540",
      "title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility",
      "content": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.",
      "url": "http://arxiv.org/abs/2601.05739",
      "author": "G M Shahariar, Zabir Al Nazi, Md Olid Hasan Bhuiyan, Zhouxing Shi",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces PII-VisBench benchmark with 4000 probes across visibility continuum to evaluate VLM safety for personally identifiable information, stratifying 200 subjects by online presence level.",
      "importance_score": 73,
      "reasoning": "Important safety benchmark addressing underexplored PII risks in VLMs. Novel stratification by online presence reveals nuanced model behaviors.",
      "themes": [
        "AI Safety",
        "Privacy",
        "Vision-Language Models",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces PII-VisBench benchmark with 4000 probes across visibility continuum to evaluate VLM safety for personally identifiable information, stratifying 200 subjects by online presence level.</p>",
      "content_html": "<p>Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.</p>"
    },
    {
      "id": "c11e42f1f6f5",
      "title": "Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models",
      "content": "Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind...",
      "url": "http://arxiv.org/abs/2601.05445",
      "author": "Songze Li, Ruishi He, Xiaojun Jia, Jun Wang, Zhihui Fu",
      "published": "2026-01-12",
      "source": "arXiv (cs.CR)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Introduces Mastermind, a multi-turn jailbreak framework that uses dynamic planning, execution, and reflection loops to progressively steer conversations toward eliciting harmful outputs from LLMs.",
      "importance_score": 72,
      "reasoning": "Important security research revealing vulnerabilities in current LLM defenses. Novel dynamic approach addresses limitations of existing attack methods.",
      "themes": [
        "AI Safety",
        "LLM Security",
        "Adversarial ML",
        "Jailbreaking"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Mastermind, a multi-turn jailbreak framework that uses dynamic planning, execution, and reflection loops to progressively steer conversations toward eliciting harmful outputs from LLMs.</p>",
      "content_html": "<p>Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind...</p>"
    },
    {
      "id": "c7a4507b6fe1",
      "title": "Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs",
      "content": "As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.",
      "url": "http://arxiv.org/abs/2601.05641",
      "author": "Alireza Dehghanpour Farashah, Aditi Khandelwal, Marylou Fauchard, Zhuan Shi, Negar Rostamzadeh, and Golnoosh Farnadi",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies multilingual unlearning in Aya-Expanse 8B across 10 languages spanning 5 language families, examining cross-lingual transfer of data and concept unlearning for safety and fairness.",
      "importance_score": 72,
      "reasoning": "Important extension of unlearning research to multilingual setting. Addresses critical gap in AI safety for non-English languages with comprehensive evaluation.",
      "themes": [
        "Machine Unlearning",
        "Multilingual NLP",
        "AI Safety",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Studies multilingual unlearning in Aya-Expanse 8B across 10 languages spanning 5 language families, examining cross-lingual transfer of data and concept unlearning for safety and fairness.</p>",
      "content_html": "<p>As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.</p>"
    },
    {
      "id": "3c89866fae4d",
      "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor",
      "content": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.",
      "url": "http://arxiv.org/abs/2601.05752",
      "author": "Shu Yang, Jingyu Hu, Tong Li, Hanqi Yan, Wenxuan Wang, Di Wang",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces AutoMonitor-Bench, first benchmark for evaluating LLM-based misbehavior monitors across 3,010 samples spanning QA, code, and reasoning with Miss Rate and False Alarm Rate metrics.",
      "importance_score": 71,
      "reasoning": "Important infrastructure for AI safety research. First systematic benchmark for monitor reliability with comprehensive coverage.",
      "themes": [
        "AI Safety",
        "Benchmarks",
        "LLM Monitoring",
        "Evaluation"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces AutoMonitor-Bench, first benchmark for evaluating LLM-based misbehavior monitors across 3,010 samples spanning QA, code, and reasoning with Miss Rate and False Alarm Rate metrics.</p>",
      "content_html": "<p>We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.</p>"
    },
    {
      "id": "692c4f8e3576",
      "title": "Automating Deception: Scalable Multi-Turn LLM Jailbreaks",
      "content": "Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.",
      "url": "http://arxiv.org/abs/2511.19517",
      "author": "Adarsh Kumarappan, Ananya Mujoo",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Introduces automated pipeline for generating large-scale psychologically-grounded multi-turn jailbreak datasets using Foot-in-the-Door techniques. Creates benchmark of 1,500 scenarios evaluating seven models across three LLM families.",
      "importance_score": 70,
      "reasoning": "Significant AI safety contribution enabling scalable red-teaming research. Systematic operationalization of psychological manipulation techniques with comprehensive evaluation.",
      "themes": [
        "AI Safety",
        "LLM Security",
        "Adversarial Attacks",
        "Red-teaming"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces automated pipeline for generating large-scale psychologically-grounded multi-turn jailbreak datasets using Foot-in-the-Door techniques. Creates benchmark of 1,500 scenarios evaluating seven models across three LLM families.</p>",
      "content_html": "<p>Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.</p>"
    },
    {
      "id": "8e85eedfbbc7",
      "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions",
      "content": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for...",
      "url": "http://arxiv.org/abs/2601.05414",
      "author": "Minda Zhao, Yilun Du, Mengyu Wang",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Conducts the first large-scale audit of LLMs' ability to generate random samples from statistical distributions, testing 11 models across 15 distributions. Reveals fundamental limitations in probabilistic reasoning.",
      "importance_score": 70,
      "reasoning": "Important finding exposing fundamental LLM limitations with implications for any application requiring stochastic behavior. Rigorous methodology with dual-protocol design.",
      "themes": [
        "LLM Evaluation",
        "Probabilistic Reasoning",
        "LLM Limitations"
      ],
      "continuation": null,
      "summary_html": "<p>Conducts the first large-scale audit of LLMs' ability to generate random samples from statistical distributions, testing 11 models across 15 distributions. Reveals fundamental limitations in probabilistic reasoning.</p>",
      "content_html": "<p>As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for...</p>"
    },
    {
      "id": "e3ea27148c9c",
      "title": "Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning",
      "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\\underline{i}nteractive \\underline{M}ulti-step \\underline{P}rogre\\underline{s}sive \\underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.",
      "url": "http://arxiv.org/abs/2601.05466",
      "author": "Zhaoqi Wang, Zijian Zhang, Daqing He, Pengtao Kou, Xin Li, Jiamou Liu, Jincheng An, and Yong Liu",
      "published": "2026-01-12",
      "source": "arXiv (cs.CR)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Proposes iMIST, a jailbreak method that disguises malicious queries as tool invocations while using interactive progressive optimization via RL to bypass LLM content filters.",
      "importance_score": 70,
      "reasoning": "Novel attack vector exploiting tool-use interfaces. Important for understanding emerging vulnerabilities as LLMs gain more capabilities.",
      "themes": [
        "AI Safety",
        "LLM Security",
        "Adversarial ML",
        "Tool-Use AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes iMIST, a jailbreak method that disguises malicious queries as tool invocations while using interactive progressive optimization via RL to bypass LLM content filters.</p>",
      "content_html": "<p>Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\\underline{i}nteractive \\underline{M}ulti-step \\underline{P}rogre\\underline{s}sive \\underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.</p>"
    },
    {
      "id": "7f262397ddde",
      "title": "ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging",
      "content": "Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as \"Reasoning + X\", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes \"Reasoning + X\" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.",
      "url": "http://arxiv.org/abs/2601.05560",
      "author": "Junyao Yang, Chen Qian, Dongrui Liu, Wen Shen, Yong Liu, Jing Shao",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes ReasonAny for incorporating reasoning capabilities into domain-specialized models via model merging, discovering that reasoning ability resides in low gradient-sensitivity parameters contrary to assumptions.",
      "importance_score": 70,
      "reasoning": "Important finding about reasoning parameter locations with practical method for democratizing reasoning capabilities.",
      "themes": [
        "LLM Reasoning",
        "Model Merging",
        "Transfer Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes ReasonAny for incorporating reasoning capabilities into domain-specialized models via model merging, discovering that reasoning ability resides in low gradient-sensitivity parameters contrary to assumptions.</p>",
      "content_html": "<p>Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as \"Reasoning + X\", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes \"Reasoning + X\" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.</p>"
    },
    {
      "id": "e81c128dafae",
      "title": "SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes",
      "content": "Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.",
      "url": "http://arxiv.org/abs/2601.05600",
      "author": "Chuhan Wang, Xintong Li, Jennifer Yuntong Zhang, Junda Wu, Chengkai Huang, Lina Yao, Julian McAuley, Jingbo Shang",
      "published": "2026-01-12",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Proposes SceneAlign framework using scene graphs for controllable structural interventions to address hallucination in multimodal reasoning. Identifies four failure modes: hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning.",
      "importance_score": 70,
      "reasoning": "Addresses critical hallucination problem in MLLMs with structured intervention approach. Scene graphs provide interpretable mechanism for improving visual grounding.",
      "themes": [
        "Multimodal Learning",
        "Hallucination",
        "Visual Reasoning",
        "AI Safety"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes SceneAlign framework using scene graphs for controllable structural interventions to address hallucination in multimodal reasoning. Identifies four failure modes: hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning.</p>",
      "content_html": "<p>Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.</p>"
    },
    {
      "id": "71283037d2b5",
      "title": "The Echo Chamber Multi-Turn LLM Jailbreak",
      "content": "The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.",
      "url": "http://arxiv.org/abs/2601.05742",
      "author": "Ahmad Alobaid (NeuralTrust), Mart\\'i Jord\\`a Roca (NeuralTrust), Carlos Castillo (ICREA and UPF), Joan Vendrell (NeuralTrust)",
      "published": "2026-01-12",
      "source": "arXiv (cs.CR)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Introduces Echo Chamber, a multi-turn jailbreak attack using gradual escalation method. Evaluates against multiple state-of-the-art models demonstrating vulnerability to carefully crafted interaction chains.",
      "importance_score": 70,
      "reasoning": "Important security research on emerging multi-turn attack vectors. Practical implications for deployed chatbots.",
      "themes": [
        "AI Safety",
        "Jailbreaking",
        "LLM Security",
        "Red Teaming"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Echo Chamber, a multi-turn jailbreak attack using gradual escalation method. Evaluates against multiple state-of-the-art models demonstrating vulnerability to carefully crafted interaction chains.</p>",
      "content_html": "<p>The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.</p>"
    },
    {
      "id": "45ee8c9fcaca",
      "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "content": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "url": "http://arxiv.org/abs/2601.05918",
      "author": "Tianshi Li",
      "published": "2026-01-12",
      "source": "arXiv (cs.CR)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Demonstrates that widely available LLMs with web search can re-identify participants in Anthropic's Interviewer dataset by cross-referencing interview details. Shows modern agents lower technical barriers to deanonymization attacks.",
      "importance_score": 70,
      "reasoning": "Critical privacy/security finding with immediate implications. Shows real vulnerability in publicly released dataset from major AI lab. Important warning for data release practices.",
      "themes": [
        "Privacy",
        "AI Safety",
        "Data Release",
        "LLM Agents"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates that widely available LLMs with web search can re-identify participants in Anthropic's Interviewer dataset by cross-referencing interview details. Shows modern agents lower technical barriers to deanonymization attacks.</p>",
      "content_html": "<p>On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.</p>"
    },
    {
      "id": "ffecbaf851e0",
      "title": "SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More",
      "content": "Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\\%...",
      "url": "http://arxiv.org/abs/2601.05688",
      "author": "Muye Huang, Lingling Zhang, Yifei Li, Yaqiang Wu, Jun Liu",
      "published": "2026-01-12",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces SketchVL with FinePO, a new RL algorithm for fine-grained credit assignment in chart understanding by drawing intermediate reasoning steps as visual sketches for trajectory-level distinction.",
      "importance_score": 69,
      "reasoning": "Novel approach to credit assignment in MLLM training using visual sketches. Addresses important limitation of trajectory-level advantage estimation.",
      "themes": [
        "Multimodal Learning",
        "Reinforcement Learning",
        "Chart Understanding",
        "Credit Assignment"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces SketchVL with FinePO, a new RL algorithm for fine-grained credit assignment in chart understanding by drawing intermediate reasoning steps as visual sketches for trajectory-level distinction.</p>",
      "content_html": "<p>Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\\%...</p>"
    },
    {
      "id": "989e155c8657",
      "title": "The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm",
      "content": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.   Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
      "url": "http://arxiv.org/abs/2601.05500",
      "author": "Aparna Elangovan, Lei Xu, Mahsa Elyasi, Ismail Akdulum, Mehmet Aksakal, Enes Gurun, Brian Hur, Saab Mansour, Ravid Shwartz Ziv, Karin Verspoor, Dan Roth",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces a probabilistic framework for AI evaluation that accounts for uncertainty in ground truth labels, showing how ignoring this uncertainty leads to misleading performance comparisons, especially in medicine.",
      "importance_score": 68,
      "reasoning": "Important methodological contribution with broad implications for AI evaluation. Addresses fundamental issue often overlooked.",
      "themes": [
        "AI Evaluation",
        "Medical AI",
        "Uncertainty Quantification",
        "Benchmarking"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces a probabilistic framework for AI evaluation that accounts for uncertainty in ground truth labels, showing how ignoring this uncertainty leads to misleading performance comparisons, especially in medicine.</p>",
      "content_html": "<p>Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.   Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.</p>"
    },
    {
      "id": "663df44c4157",
      "title": "Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism",
      "content": "Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \\textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \\emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \\textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\\textbf{5.3}\\times$ on LLaMA3.3-70B and $\\textbf{2.8}\\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.",
      "url": "http://arxiv.org/abs/2601.05524",
      "author": "Yuhao Shen, Tianyu Liu, Junyi Shen, Jinyang Wu, Quan Kong, Li Huan, Cong Wang",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces Double, a speculative decoding method using double retrieval to break the theoretical speedup ceiling of parallel speculative decoding, addressing rejection-induced pipeline stalls.",
      "importance_score": 68,
      "reasoning": "Important efficiency contribution addressing fundamental limitation in speculative decoding. Directly relevant to LLM deployment.",
      "themes": [
        "LLM Efficiency",
        "Inference Optimization",
        "Speculative Decoding"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Double, a speculative decoding method using double retrieval to break the theoretical speedup ceiling of parallel speculative decoding, addressing rejection-induced pipeline stalls.</p>",
      "content_html": "<p>Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \\textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \\emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \\textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\\textbf{5.3}\\times$ on LLaMA3.3-70B and $\\textbf{2.8}\\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.</p>"
    },
    {
      "id": "386942171df7",
      "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature",
      "content": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.",
      "url": "http://arxiv.org/abs/2601.05567",
      "author": "Tengxiao Liu, Deepak Nathani, Zekun Li, Kevin Yang, William Yang Wang",
      "published": "2026-01-12",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces WildSci, a dataset of scientific questions from peer-reviewed literature across 9 disciplines, enabling scalable RL training for domain-specific reasoning with well-defined rewards.",
      "importance_score": 68,
      "reasoning": "Addresses important gap in scientific reasoning data. Large-scale coverage with practical training setup.",
      "themes": [
        "Scientific Reasoning",
        "LLM Training",
        "Reinforcement Learning",
        "Domain Adaptation"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces WildSci, a dataset of scientific questions from peer-reviewed literature across 9 disciplines, enabling scalable RL training for domain-specific reasoning with well-defined rewards.</p>",
      "content_html": "<p>Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.</p>"
    },
    {
      "id": "848607be34bb",
      "title": "Good Allocations from Bad Estimates",
      "content": "Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $\\epsilon > 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/\\epsilon^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $\\epsilon$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/\\epsilon)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.",
      "url": "http://arxiv.org/abs/2601.05597",
      "author": "S\\'ilvia Casacuberta, Moritz Hardt",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Shows that coarse CATE estimates suffice for near-optimal treatment allocations, reducing sample complexity from O(M/) to O(M/). Demonstrates practical improvement for budget-constrained treatment allocation.",
      "importance_score": 68,
      "reasoning": "Strong theoretical contribution with clear practical implications. Challenges conventional wisdom about precision requirements. Clean mathematical result with direct applications.",
      "themes": [
        "Causal Inference",
        "Treatment Effects",
        "Sample Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Shows that coarse CATE estimates suffice for near-optimal treatment allocations, reducing sample complexity from O(M/) to O(M/). Demonstrates practical improvement for budget-constrained treatment allocation.</p>",
      "content_html": "<p>Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $\\epsilon > 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/\\epsilon^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $\\epsilon$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/\\epsilon)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.</p>"
    },
    {
      "id": "02e8c24d3647",
      "title": "Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models",
      "content": "The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.",
      "url": "http://arxiv.org/abs/2601.05663",
      "author": "Gianmario Voria, Moses Openja, Foutse Khomh, Gemma Catolino, Fabio Palomba",
      "published": "2026-01-12",
      "source": "arXiv (cs.SE)",
      "source_type": "arxiv",
      "tags": [
        "cs.SE"
      ],
      "summary": "Investigates biased neurons in pre-trained transformers that encode stereotypical associations. Builds dataset of biased relations across nine bias types and proposes methods to trace and modify these neurons.",
      "importance_score": 68,
      "reasoning": "Important contribution to understanding and mitigating bias in LLMs at the mechanistic level. Extends knowledge neurons concept to fairness domain.",
      "themes": [
        "AI Fairness",
        "Interpretability",
        "Bias Detection",
        "Transformers"
      ],
      "continuation": null,
      "summary_html": "<p>Investigates biased neurons in pre-trained transformers that encode stereotypical associations. Builds dataset of biased relations across nine bias types and proposes methods to trace and modify these neurons.</p>",
      "content_html": "<p>The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.</p>"
    },
    {
      "id": "31f3032c205d",
      "title": "SceneFoundry: Generating Interactive Infinite 3D Worlds",
      "content": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.",
      "url": "http://arxiv.org/abs/2601.05810",
      "author": "ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, and YuanFu Yang",
      "published": "2026-01-12",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents SceneFoundry, language-guided diffusion framework generating apartment-scale 3D worlds with articulated furniture for robotic training, using LLM for layout and diffusion for asset placement.",
      "importance_score": 68,
      "reasoning": "Important contribution for embodied AI training data. Addresses gap in functional articulated object generation.",
      "themes": [
        "3D Generation",
        "Embodied AI",
        "Robotic Training",
        "Procedural Generation"
      ],
      "continuation": null,
      "summary_html": "<p>Presents SceneFoundry, language-guided diffusion framework generating apartment-scale 3D worlds with articulated furniture for robotic training, using LLM for layout and diffusion for asset placement.</p>",
      "content_html": "<p>The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.</p>"
    },
    {
      "id": "4e49eb99892c",
      "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.",
      "url": "http://arxiv.org/abs/2601.05607",
      "author": "Zijun Min, Bingshuai Liu, Ante Wang, Long Zhang, Anxiang Zeng, Haibo Zhang, Jinsong Su",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Dynamic Hybrid Policy Optimization (DHPO) that bridges token-level GRPO and sequence-level GSPO within a unified clipped objective for RLVR, addressing credit assignment granularity trade-offs.",
      "importance_score": 67,
      "reasoning": "Addresses important tension in RLVR between credit assignment precision and reward matching. Practical improvement for LLM reasoning training.",
      "themes": [
        "Reinforcement Learning",
        "Language Models",
        "Policy Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Dynamic Hybrid Policy Optimization (DHPO) that bridges token-level GRPO and sequence-level GSPO within a unified clipped objective for RLVR, addressing credit assignment granularity trade-offs.</p>",
      "content_html": "<p>Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.</p>"
    },
    {
      "id": "c8840032ca95",
      "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer",
      "content": "Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.",
      "url": "http://arxiv.org/abs/2601.05770",
      "author": "Yifan Zhang, Wei Bi, Kechi Zhang, Dongming Jin, Jie Fu, Zhi Jin",
      "published": "2026-01-12",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Discrete Transformer architecture for extracting executable programs from trained models by enforcing functional disentanglement between attention (routing) and MLP (arithmetic) with temperature-annealed sampling.",
      "importance_score": 67,
      "reasoning": "Novel approach to algorithm extraction and interpretability. Addresses superposition problem with architectural constraints. Enables de novo algorithm discovery.",
      "themes": [
        "Interpretability",
        "Algorithm Extraction",
        "Neural Networks",
        "Symbolic AI"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Discrete Transformer architecture for extracting executable programs from trained models by enforcing functional disentanglement between attention (routing) and MLP (arithmetic) with temperature-annealed sampling.</p>",
      "content_html": "<p>Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.</p>"
    },
    {
      "id": "0067fcd9fc82",
      "title": "GIFT: Games as Informal Training for Generalizable LLMs",
      "content": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
      "url": "http://arxiv.org/abs/2601.05633",
      "author": "Nuoyan Lyu, Bingbing Xu, Weihao Meng, Yige Yuan, Yang Zhang, Zhiyong Huang, Tat-Seng Chua, Huawei Shen",
      "published": "2026-01-12",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes using games as informal learning environments for LLMs with a Nested Training Framework that addresses multi-task performance degradation through an 'AND' objective rather than implicit 'OR'.",
      "importance_score": 66,
      "reasoning": "Novel framing of game-based training for developing generalizable intelligence. Addresses important gap between formal and informal learning in LLMs.",
      "themes": [
        "LLM Training",
        "Game AI",
        "Multi-Task Learning",
        "Generalization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using games as informal learning environments for LLMs with a Nested Training Framework that addresses multi-task performance degradation through an 'AND' objective rather than implicit 'OR'.</p>",
      "content_html": "<p>While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.</p>"
    }
  ],
  "notice": {
    "type": "info",
    "title": "Monday Edition",
    "message": "Today's arXiv papers cover submissions from the entire weekend (Friday through Sunday) due to arXiv's publishing schedule."
  }
}