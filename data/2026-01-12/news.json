{
  "category": "news",
  "date": "2026-01-12",
  "category_summary": "**AI safety and ethics concerns** dominated this news cycle, with two major stories highlighting risks from deployed frontier AI systems. **Grok AI** [faced backlash](/?date=2026-01-12&category=news#item-f720ccec6594) after its image capabilities were exploited for mass non-consensual nudification, with hundreds of thousands of requests stripping clothing from women's photos on **X**. Separately, **Google** was [forced to remove](/?date=2026-01-12&category=news#item-b21ee198aa23) **AI Overviews** health summaries after a Guardian investigation found dangerous medical misinformation.\n\nOn the business front, **Alphabet** [surpassed **Apple**](/?date=2026-01-12&category=news#item-e5544fbc7b95) to become the world's second most valuable company (~**$3.89 trillion**), bolstered by reports of a **$1 billion annual deal** to integrate **Gemini** into Apple Intelligence. In open source, **SETA** [launched a 400-task](/?date=2026-01-12&category=news#item-13fd2e4cca0e) reinforcement learning environment for terminal agents, achieving SOTA on **Terminal Bench** with **Claude Sonnet 4.5** and **GPT-4.1**.",
  "category_summary_html": "<p><strong>AI safety and ethics concerns</strong> dominated this news cycle, with two major stories highlighting risks from deployed frontier AI systems. <strong>Grok AI</strong> <a href=\"/?date=2026-01-12&category=news#item-f720ccec6594\" class=\"internal-link\">faced backlash</a> after its image capabilities were exploited for mass non-consensual nudification, with hundreds of thousands of requests stripping clothing from women's photos on <strong>X</strong>. Separately, <strong>Google</strong> was <a href=\"/?date=2026-01-12&category=news#item-b21ee198aa23\" class=\"internal-link\">forced to remove</a> <strong>AI Overviews</strong> health summaries after a Guardian investigation found dangerous medical misinformation.</p>\n<p>On the business front, <strong>Alphabet</strong> <a href=\"/?date=2026-01-12&category=news#item-e5544fbc7b95\" class=\"internal-link\">surpassed <strong>Apple</strong></a> to become the world's second most valuable company (~<strong>$3.89 trillion</strong>), bolstered by reports of a <strong>$1 billion annual deal</strong> to integrate <strong>Gemini</strong> into Apple Intelligence. In open source, <strong>SETA</strong> <a href=\"/?date=2026-01-12&category=news#item-13fd2e4cca0e\" class=\"internal-link\">launched a 400-task</a> reinforcement learning environment for terminal agents, achieving SOTA on <strong>Terminal Bench</strong> with <strong>Claude Sonnet 4.5</strong> and <strong>GPT-4.1</strong>.</p>",
  "themes": [
    {
      "name": "AI Safety & Ethics",
      "description": "Multiple stories highlighting real-world harms from deployed AI systems, including medical misinformation and non-consensual image generation",
      "item_count": 3,
      "example_items": [],
      "importance": 72.0
    },
    {
      "name": "Industry & Business",
      "description": "Market dynamics and major commercial deals shaping the competitive AI landscape",
      "item_count": 1,
      "example_items": [],
      "importance": 71.0
    },
    {
      "name": "AI Agents & Tools",
      "description": "Open source releases and tutorials advancing agent development and ML security understanding",
      "item_count": 2,
      "example_items": [],
      "importance": 48.0
    }
  ],
  "total_items": 6,
  "items": [
    {
      "id": "f720ccec6594",
      "title": "\u2018Add blood, forced smile\u2019: how Grok\u2019s nudification tool went viral",
      "content": "The \u2018put her in a bikini\u2019 trend rapidly evolved into hundreds of thousands of requests to strip clothes from photos of women, horrifying those targetedLike thousands of women across the world, Evie, a 22-year-old photographer from Lincolnshire, woke up on New Year\u2019s Day, looked at her phone and was alarmed to see that fully clothed photographs of her had been digitally manipulated by Elon Musk\u2019s AI tool, Grok, to show her in just a bikini.The \u201cput her in a bikini\u201d trend began quietly at the end of last year before exploding at the start of 2026. Within days, hundreds of thousands of requests were being made to the Grok chatbot, asking it to strip the clothes from photographs of women. The fake, sexualised images were posted publicly on X, freely available for millions of people to inspect. Continue reading...",
      "url": "https://www.theguardian.com/news/ng-interactive/2026/jan/11/how-grok-nudification-tool-went-viral-x-elon-musk",
      "author": "Amelia Gentleman and Helena Horton",
      "published": "2026-01-11T06:00:18",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Elon Musk",
        "Grok AI",
        "Technology"
      ],
      "summary": "Continuing our coverage from [earlier this week](/?date=2026-01-10&category=news#item-c0b3ea0cd4f4), Grok AI's image manipulation capabilities were exploited in a viral 'put her in a bikini' trend, with hundreds of thousands of requests made to strip clothing from photos of women without consent. The non-consensual intimate imagery was posted publicly on X, causing significant harm to targets.",
      "importance_score": 73.0,
      "reasoning": "Major AI safety and ethics scandal involving a frontier AI model from xAI/Musk. Demonstrates serious content policy failures at scale with documented harm to victims, raising urgent questions about guardrails on generative AI image capabilities.",
      "themes": [
        "AI Safety",
        "AI Ethics",
        "Content Moderation",
        "Grok",
        "Image Generation"
      ],
      "continuation": {
        "original_item_id": "c0b3ea0cd4f4",
        "original_date": "2026-01-10",
        "original_category": "news",
        "original_title": "Grok being used to create sexually violent videos featuring women, research finds",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from earlier this week"
      },
      "summary_html": "<p>Continuing our coverage from <a href=\"/?date=2026-01-10&category=news#item-c0b3ea0cd4f4\">earlier this week</a>, Grok AI's image manipulation capabilities were exploited in a viral 'put her in a bikini' trend, with hundreds of thousands of requests made to strip clothing from photos of women without consent. The non-consensual intimate imagery was posted publicly on X, causing significant harm to targets.</p>",
      "content_html": "<p>The \u2018put her in a bikini\u2019 trend rapidly evolved into hundreds of thousands of requests to strip clothes from photos of women, horrifying those targetedLike thousands of women across the world, Evie, a 22-year-old photographer from Lincolnshire, woke up on New Year\u2019s Day, looked at her phone and was alarmed to see that fully clothed photographs of her had been digitally manipulated by Elon Musk\u2019s AI tool, Grok, to show her in just a bikini.The \u201cput her in a bikini\u201d trend began quietly at the end of last year before exploding at the start of 2026. Within days, hundreds of thousands of requests were being made to the Grok chatbot, asking it to strip the clothes from photographs of women. The fake, sexualised images were posted publicly on X, freely available for millions of people to inspect. Continue reading...</p>"
    },
    {
      "id": "e5544fbc7b95",
      "title": "How Distribution Is Putting Google Ahead of OpenAI and Apple",
      "content": "\nThe year has just begun, and the momentum appears to be firmly on Google\u2019s side. On January 7, Alphabet, the search giant\u2019s parent, overtook Apple to become the world\u2019s second most valuable publicly traded company, behind NVIDIA.\u00a0\n\n\n\nAlphabet\u2019s market capitalisation closed at approximately $3.89 trillion, edging past Apple\u2019s valuation of about $3.85 trillion following a recent surge in its share price. It is the first time since 2019 that Alphabet has surpassed Apple in market value.\n\n\n\nThe reshuffle is striking, given Apple\u2019s near-final integration of Google\u2019s Gemini models into Apple Intelligence. In late 2025, multiple reports said Apple would pay Google roughly $1 billion annually to run a custom version of Gemini as part of its next-generation Siri stack. In effect, Apple is buying intelligence from the very company that just overtook it.\n\n\n\nThat shift reflects a broader realignment underway in AI.\n\n\n\nDuring a recent podcast with entrepreneur Nikhil Kamath, Elon Musk pointed to Google and NVIDIA as the companies most likely to accumulate long-term value from AI and robotics. Google, he said, had \u201claid the groundwork for an immense amount of value creation from an AI standpoint.\u201d\n\n\n\nMarket data suggests that groundwork is beginning to pay off.\n\n\n\nAccording to Similarweb, ChatGPT\u2019s share of global visits to generative AI chatbot websites fell from about 86.7% in early 2025 to roughly 64.5% in January 2026. Over the same period, Gemini\u2019s share climbed from 5.7% to around 21.5%, signalling a rapid narrowing of the gap.\n\n\n\nThe pressure has not gone unnoticed inside OpenAI. Last year, chief executive Sam Altman announced an internal \u2018code red\u2019, triggered by a rival development seen as a potential threat to OpenAI\u2019s position.\n\n\n\nMuch of Google\u2019s recent momentum is tied to Gemini 3, its latest flagship model, which has drawn a strong user response. The company\u2019s newer image-generation system, Nano Banana Pro, has also gained attention for handling text reliably and producing detailed infographics, areas where earlier models often struggled.&nbsp;\n\n\n\nDistribution, Not Just Models\n\n\n\nGemini\u2019s biggest edge, however, is not technical benchmarks, but distribution. Shay Boloor, chief market strategist at Futurum Equities, wrote on X that Google can ship new AI capabilities instantly across Search, YouTube, Android, and Workspace, reaching billions of users without asking them to change behaviour.\n\n\n\nAccording to him, the company that can successfully distribute intelligence widely across devices, applications, and business processes will dominate the new AI economy, and Google is clearly establishing this system across its whole ecosystem.\n\n\n\nMore than 3 billion active Android devices worldwide can surf Gemini features directly through Google Assistant, while Google Search, which holds close to a 90% of the global market, is already rolling out AI-powered results through AI Overviews. Gmail, Maps and YouTube extend Gemini\u2019s reach further, embedding AI into products people already use daily.\n\n\n\nSamsung Electronics recently said it plans to expand Galaxy AI features across its devices, much of it powered by Gemini. The rollout is expected to grow from about 400 million devices last year to roughly 800 million smartphones and tablets by 2026.\n\n\n\n\n\n\n\n\n\n\n\nGoogle is also pushing Gemini deeper into productivity. The company has introduced what it calls the \u201cGemini era\u201d of Gmail, bringing AI-generated summaries, natural-language question answering and contextual writing tools directly into the inbox. Instead of searching keywords, users receive synthesised answers across email threads by default.\n\n\n\nOwning the AI Stack: Models, Chips, and Cloud\n\n\n\nGoogle\u2019s advantage runs deeper than consumer products. Unlike most AI players, it controls the entire technical stack.\n\n\n\nAt the infrastructure layer, Google designs its own Tensor Processing Units (TPUs), custom chips optimised for training and running large language models at scale.&nbsp;\n\n\n\nGemini runs on this infrastructure by default, as do the AI systems behind Search, Maps, Photos, and Ads.&nbsp;\n\n\n\nThat strategy is beginning to attract external validation. Anthropic has said it will use Google\u2019s TPUs, while Meta has reportedly explored them for parts of its training workloads. Gemini 3 Pro, widely regarded as Google\u2019s most powerful frontier model, was trained entirely on TPUs.\n\n\n\nOpenAI, by contrast, depends heavily on Microsoft\u2019s cloud and NVIDIA hardware, exposing it to cost pressure and supply constraints. Google\u2019s vertical integration allows faster iteration and lower marginal deployment costs.\n\n\n\nBeyond Screens\n\n\n\nPerhaps the clearest signal of Google\u2019s long-term AI ambitions lies beyond chat interfaces.\n\n\n\nIn early 2026, Google DeepMind announced a partnership with Boston Dynamics to integrate Gemini-based foundation models into next-generation robots, including humanoid systems.\n\n\n\nThe goal is to bring large multimodal AI models into the physical world, allowing robots to perceive, reason, and act with greater autonomy. This work builds on years of Google research in reinforcement learning, robotics simulation, and embodied AI.\n\n\n\nNeither OpenAI nor Apple currently operates a comparable robotics program at scale. Google is betting that the next phase of AI will move off screens and into the physical world.\n\n\n\nTwo years ago, Google was criticised for moving too slowly in generative AI. Today, the picture looks very different.\n\n\n\nGemini is gaining users. Distribution is unmatched. The company owns its chips, its cloud and its delivery channels. Its models are spreading across consumer products, enterprise software and robotics. Even competitors are becoming customers.\n\n\n\nChatGPT may have sparked the AI wave, but Google now seems focused on shaping what happens next.\nThe post How Distribution Is Putting Google Ahead of OpenAI and Apple appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/global-tech/how-distribution-is-putting-google-ahead-of-openai-and-apple/",
      "author": "Siddharth Jindal",
      "published": "2026-01-11T04:32:47",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "Global Tech",
        "Google"
      ],
      "summary": "Alphabet surpassed Apple to become the world's second most valuable company behind NVIDIA, with ~$3.89 trillion market cap. Reports indicate Apple will pay Google roughly $1 billion annually to integrate Gemini models into Apple Intelligence and next-generation Siri.",
      "importance_score": 71.0,
      "reasoning": "Significant market milestone reflecting Google's AI momentum. The confirmed Gemini-Apple Intelligence deal represents major commercial validation of Google's frontier models and shapes the competitive landscape for AI assistants.",
      "themes": [
        "Industry Dynamics",
        "Google",
        "Apple",
        "Gemini",
        "Business"
      ],
      "continuation": null,
      "summary_html": "<p>Alphabet surpassed Apple to become the world's second most valuable company behind NVIDIA, with ~$3.89 trillion market cap. Reports indicate Apple will pay Google roughly $1 billion annually to integrate Gemini models into Apple Intelligence and next-generation Siri.</p>",
      "content_html": "<p>The year has just begun, and the momentum appears to be firmly on Google\u2019s side. On January 7, Alphabet, the search giant\u2019s parent, overtook Apple to become the world\u2019s second most valuable publicly traded company, behind NVIDIA.</p>\n<p>Alphabet\u2019s market capitalisation closed at approximately $3.89 trillion, edging past Apple\u2019s valuation of about $3.85 trillion following a recent surge in its share price. It is the first time since 2019 that Alphabet has surpassed Apple in market value.</p>\n<p>The reshuffle is striking, given Apple\u2019s near-final integration of Google\u2019s Gemini models into Apple Intelligence. In late 2025, multiple reports said Apple would pay Google roughly $1 billion annually to run a custom version of Gemini as part of its next-generation Siri stack. In effect, Apple is buying intelligence from the very company that just overtook it.</p>\n<p>That shift reflects a broader realignment underway in AI.</p>\n<p>During a recent podcast with entrepreneur Nikhil Kamath, Elon Musk pointed to Google and NVIDIA as the companies most likely to accumulate long-term value from AI and robotics. Google, he said, had \u201claid the groundwork for an immense amount of value creation from an AI standpoint.\u201d</p>\n<p>Market data suggests that groundwork is beginning to pay off.</p>\n<p>According to Similarweb, ChatGPT\u2019s share of global visits to generative AI chatbot websites fell from about 86.7% in early 2025 to roughly 64.5% in January 2026. Over the same period, Gemini\u2019s share climbed from 5.7% to around 21.5%, signalling a rapid narrowing of the gap.</p>\n<p>The pressure has not gone unnoticed inside OpenAI. Last year, chief executive Sam Altman announced an internal \u2018code red\u2019, triggered by a rival development seen as a potential threat to OpenAI\u2019s position.</p>\n<p>Much of Google\u2019s recent momentum is tied to Gemini 3, its latest flagship model, which has drawn a strong user response. The company\u2019s newer image-generation system, Nano Banana Pro, has also gained attention for handling text reliably and producing detailed infographics, areas where earlier models often struggled.&nbsp;</p>\n<p>Distribution, Not Just Models</p>\n<p>Gemini\u2019s biggest edge, however, is not technical benchmarks, but distribution. Shay Boloor, chief market strategist at Futurum Equities, wrote on X that Google can ship new AI capabilities instantly across Search, YouTube, Android, and Workspace, reaching billions of users without asking them to change behaviour.</p>\n<p>According to him, the company that can successfully distribute intelligence widely across devices, applications, and business processes will dominate the new AI economy, and Google is clearly establishing this system across its whole ecosystem.</p>\n<p>More than 3 billion active Android devices worldwide can surf Gemini features directly through Google Assistant, while Google Search, which holds close to a 90% of the global market, is already rolling out AI-powered results through AI Overviews. Gmail, Maps and YouTube extend Gemini\u2019s reach further, embedding AI into products people already use daily.</p>\n<p>Samsung Electronics recently said it plans to expand Galaxy AI features across its devices, much of it powered by Gemini. The rollout is expected to grow from about 400 million devices last year to roughly 800 million smartphones and tablets by 2026.</p>\n<p>Google is also pushing Gemini deeper into productivity. The company has introduced what it calls the \u201cGemini era\u201d of Gmail, bringing AI-generated summaries, natural-language question answering and contextual writing tools directly into the inbox. Instead of searching keywords, users receive synthesised answers across email threads by default.</p>\n<p>Owning the AI Stack: Models, Chips, and Cloud</p>\n<p>Google\u2019s advantage runs deeper than consumer products. Unlike most AI players, it controls the entire technical stack.</p>\n<p>At the infrastructure layer, Google designs its own Tensor Processing Units (TPUs), custom chips optimised for training and running large language models at scale.&nbsp;</p>\n<p>Gemini runs on this infrastructure by default, as do the AI systems behind Search, Maps, Photos, and Ads.&nbsp;</p>\n<p>That strategy is beginning to attract external validation. Anthropic has said it will use Google\u2019s TPUs, while Meta has reportedly explored them for parts of its training workloads. Gemini 3 Pro, widely regarded as Google\u2019s most powerful frontier model, was trained entirely on TPUs.</p>\n<p>OpenAI, by contrast, depends heavily on Microsoft\u2019s cloud and NVIDIA hardware, exposing it to cost pressure and supply constraints. Google\u2019s vertical integration allows faster iteration and lower marginal deployment costs.</p>\n<p>Beyond Screens</p>\n<p>Perhaps the clearest signal of Google\u2019s long-term AI ambitions lies beyond chat interfaces.</p>\n<p>In early 2026, Google DeepMind announced a partnership with Boston Dynamics to integrate Gemini-based foundation models into next-generation robots, including humanoid systems.</p>\n<p>The goal is to bring large multimodal AI models into the physical world, allowing robots to perceive, reason, and act with greater autonomy. This work builds on years of Google research in reinforcement learning, robotics simulation, and embodied AI.</p>\n<p>Neither OpenAI nor Apple currently operates a comparable robotics program at scale. Google is betting that the next phase of AI will move off screens and into the physical world.</p>\n<p>Two years ago, Google was criticised for moving too slowly in generative AI. Today, the picture looks very different.</p>\n<p>Gemini is gaining users. Distribution is unmatched. The company owns its chips, its cloud and its delivery channels. Its models are spreading across consumer products, enterprise software and robotics. Even competitors are becoming customers.</p>\n<p>ChatGPT may have sparked the AI wave, but Google now seems focused on shaping what happens next.</p>\n<p>The post How Distribution Is Putting Google Ahead of OpenAI and Apple appeared first on Analytics India Magazine.</p>"
    },
    {
      "id": "b21ee198aa23",
      "title": "\u2018Dangerous and alarming\u2019: Google removes some of its AI summaries after users\u2019 health put at risk",
      "content": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are \u201chelpful\u201d and \u201creliable\u201d. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation",
      "author": "Andrew Gregory Health editor",
      "published": "2026-01-11T07:00:19",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Google",
        "AI (artificial intelligence)",
        "Technology",
        "Computing",
        "Search engines",
        "Internet",
        "Health",
        "Society"
      ],
      "summary": "A Guardian investigation found Google's AI Overviews provided false and misleading health information about blood tests, putting users at risk of harm. Google subsequently removed some of these AI-generated health summaries.",
      "importance_score": 68.0,
      "reasoning": "Important AI safety story demonstrating real-world harm from deployed AI systems at scale. Google being forced to retract AI features due to health misinformation is significant for AI reliability and deployment practices discourse.",
      "themes": [
        "AI Safety",
        "Google",
        "Misinformation",
        "Healthcare AI",
        "Search"
      ],
      "continuation": null,
      "summary_html": "<p>A Guardian investigation found Google's AI Overviews provided false and misleading health information about blood tests, putting users at risk of harm. Google subsequently removed some of these AI-generated health summaries.</p>",
      "content_html": "<p>Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are \u201chelpful\u201d and \u201creliable\u201d. Continue reading...</p>"
    },
    {
      "id": "13fd2e4cca0e",
      "title": "Meet SETA: Open Source Training Reinforcement Learning Environments for Terminal Agents with 400 Tasks and CAMEL Toolkit",
      "content": "What does an end to end stack for terminal agents look like when you combine structured toolkits, synthetic RL environments, and benchmark aligned evaluation? A team of researchers from CAMEL AI, Eigent AI and other collaborators have released SETA, a toolkit and environment stack that focuses on reinforcement learning for terminal agents. The project targets agents that operate inside a Unix style shell and must complete verifiable tasks under a benchmark harness such as Terminal Bench. \n\n\n\nThree main contributions:\n\n\n\n\nA state of the art terminal agent on Terminal Bench: They achieve state of the art performance with a Claude Sonnet 4.5 based agent on Terminal Bench 2.0 and with a GPT 4.1 based agent on Terminal Bench 1.0. The comparison is restricted to agents that use the same base model.\n\n\n\nScalable RL training with synthetic terminal environments: The research team release an initial synthetic dataset with 400 terminal tasks that cover a range of difficulty levels. Out of these, 260 tasks are used for RLVR finetuning of a Qwen3-8B model. \n\n\n\nA clean agent design that generalizes across training and evaluation frameworks: The same agent implementation is used for both local task runs and the official Terminal Bench evaluation harness.\n\n\n\n\nTerminal Toolkit and log structure\n\n\n\nThe SETA code repository showcases a Terminal Toolkit that turns a language model into an executable terminal agent. For each task run, the framework creates a structured log directory under evaluation/terminal_bench_run. The README page shows a concrete layout for a task called play-zork. \n\n\n\nKey files include:\n\n\n\n\nchatagent.log which records the full history of agent messages and tool calls including test results.\n\n\n\nA sessions directory with session_logs that capture terminal interactions from the toolkit.\n\n\n\nWithin session_logs, files such as blocking_commands.log, session_run_zork_1_correct_path.log, session_zork-1.log, and session_zork_start.log store command output for different sessions and modes.\n\n\n\ntests.log and tests.log.strip which record the test run output, with the latter removing terminal control characters.\n\n\n\n\nThis structure gives a concrete way to debug an agent. You can trace from high level chat decisions in chatagent.log down to individual shell commands in the session logs and confirm success or failure from the test logs.\n\n\n\nFor official Terminal Bench evaluation, the GitHub repository provides a separate entry point under evaluation/terminal_bench_eval. A developer moves into that directory and runs run_eval.sh for Terminal Bench 1.0 and run_tb2.sh for Terminal Bench 2.0. \n\n\n\nResults are written into evaluation/terminal_bench_eval/run/{run_id}/results.json. Task specific session logs are placed under evaluation/terminal_bench_eval/logs/camel_logs/{task_id}. The agent class that binds the CAMEL agent to the benchmark is implemented in tbench_camel_agent.py. \n\n\n\nNote Taking Toolkit as persistent memory\n\n\n\nThe research team also introduces a Note Taking Toolkit described as persistent memory for long horizon tasks. They show example note taking tool calls where the agent writes and reads notes in a structured way while solving terminal tasks. The current public material focuses on the existence of this toolkit and the examples of use. It does not yet describe a full training objective for note usage. \n\n\n\nThe important point is that the agent has an explicit channel where it can externalize intermediate results and hints, separate from the raw terminal buffer.\n\n\n\nUnderstanding the performance\n\n\n\nSETA\u2019s agent harness achieves leading results on Terminal Bench. With Claude Sonnet-4.5 as the backbone, the CAMEL terminal agent reaches 46.5% accuracy on Terminal Bench 2.0 across 89 real world tasks, ranking first and outperforming the second system by 3 percentage points, with especially strong results in git workflows, DevOps automation, and code security tasks. On Terminal Bench 1.0, a GPT 4.1 based agent attains 35% accuracy, which is 4.7 percentage points above the next entry, again within the same model family. In comparison, a supervised Qwen3 8B baseline attains 3.4% on Terminal Bench 2.0, and the Qwen3 8B terminal agent trained with the SETA RL pipeline improves over this baseline on the curated synthetic environments.\n\n\n\nKey Takeaways\n\n\n\n\nSETA is a joint community project that provides both agent toolkits and synthetic RL environments specifically for terminal agents, aligned with the Terminal Bench evaluation format. \n\n\n\nThe framework reports state of the art performance for CAMEL terminal agents on Terminal Bench 1.0 and 2.0 when using Claude Sonnet 4.5 and GPT 4.1 as the base models, evaluated against agents built on the same model families. \n\n\n\nThe SETA RL dataset on Hugging Face contains 400 synthetic terminal tasks, each packaged as task.yaml, Dockerfile, and run-tests.sh, with 260 tasks used for RLVR finetuning of a Qwen3-8B based agent. \n\n\n\nThe open source SETA codebase exposes a Terminal Toolkit with structured logging and a Note Taking Toolkit for long horizon memory, and integrates directly with Terminal Bench evaluation scripts and logging paths in the seta GitHub repository. \n\n\n\nThe overall design demonstrates a clean path from synthetic RL environments to benchmark verified agents, giving developers a reproducible stack to train, debug, and evaluate terminal agents rather than relying on ad hoc tool calling examples.\n\n\n\n\n\n\n\n\nCheck out the&nbsp;Blog, Technical details, GitHub Repo and Weights.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don\u2019t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.\n\n\n\nCheck out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.\nThe post Meet SETA: Open Source Training Reinforcement Learning Environments for Terminal Agents with 400 Tasks and CAMEL Toolkit appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/11/meet-seta-open-source-training-reinforcement-learning-environments-for-terminal-agents-with-400-tasks-and-camel-toolkit/",
      "author": "Michal Sutter",
      "published": "2026-01-11T15:12:24",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Editors Pick",
        "New Releases",
        "Open Source",
        "Staff",
        "Technology"
      ],
      "summary": "CAMEL AI and collaborators released SETA, an open-source toolkit with 400 tasks for training terminal agents using reinforcement learning. The system achieves state-of-the-art performance on Terminal Bench 2.0 with Claude Sonnet 4.5 and on Terminal Bench 1.0 with GPT-4.1.",
      "importance_score": 58.0,
      "reasoning": "Useful open-source contribution for AI agent development with benchmark-validated results. Demonstrates continued progress in agentic AI but represents incremental rather than breakthrough advancement.",
      "themes": [
        "Open Source",
        "AI Agents",
        "Reinforcement Learning",
        "Benchmarks"
      ],
      "continuation": null,
      "summary_html": "<p>CAMEL AI and collaborators released SETA, an open-source toolkit with 400 tasks for training terminal agents using reinforcement learning. The system achieves state-of-the-art performance on Terminal Bench 2.0 with Claude Sonnet 4.5 and on Terminal Bench 1.0 with GPT-4.1.</p>",
      "content_html": "<p>What does an end to end stack for terminal agents look like when you combine structured toolkits, synthetic RL environments, and benchmark aligned evaluation? A team of researchers from CAMEL AI, Eigent AI and other collaborators have released SETA, a toolkit and environment stack that focuses on reinforcement learning for terminal agents. The project targets agents that operate inside a Unix style shell and must complete verifiable tasks under a benchmark harness such as Terminal Bench.</p>\n<p>Three main contributions:</p>\n<p>A state of the art terminal agent on Terminal Bench: They achieve state of the art performance with a Claude Sonnet 4.5 based agent on Terminal Bench 2.0 and with a GPT 4.1 based agent on Terminal Bench 1.0. The comparison is restricted to agents that use the same base model.</p>\n<p>Scalable RL training with synthetic terminal environments: The research team release an initial synthetic dataset with 400 terminal tasks that cover a range of difficulty levels. Out of these, 260 tasks are used for RLVR finetuning of a Qwen3-8B model.</p>\n<p>A clean agent design that generalizes across training and evaluation frameworks: The same agent implementation is used for both local task runs and the official Terminal Bench evaluation harness.</p>\n<p>Terminal Toolkit and log structure</p>\n<p>The SETA code repository showcases a Terminal Toolkit that turns a language model into an executable terminal agent. For each task run, the framework creates a structured log directory under evaluation/terminal_bench_run. The README page shows a concrete layout for a task called play-zork.</p>\n<p>Key files include:</p>\n<p>chatagent.log which records the full history of agent messages and tool calls including test results.</p>\n<p>A sessions directory with session_logs that capture terminal interactions from the toolkit.</p>\n<p>Within session_logs, files such as blocking_commands.log, session_run_zork_1_correct_path.log, session_zork-1.log, and session_zork_start.log store command output for different sessions and modes.</p>\n<p>tests.log and tests.log.strip which record the test run output, with the latter removing terminal control characters.</p>\n<p>This structure gives a concrete way to debug an agent. You can trace from high level chat decisions in chatagent.log down to individual shell commands in the session logs and confirm success or failure from the test logs.</p>\n<p>For official Terminal Bench evaluation, the GitHub repository provides a separate entry point under evaluation/terminal_bench_eval. A developer moves into that directory and runs run_eval.sh for Terminal Bench 1.0 and run_tb2.sh for Terminal Bench 2.0.</p>\n<p>Results are written into evaluation/terminal_bench_eval/run/{run_id}/results.json. Task specific session logs are placed under evaluation/terminal_bench_eval/logs/camel_logs/{task_id}. The agent class that binds the CAMEL agent to the benchmark is implemented in tbench_camel_agent.py.</p>\n<p>Note Taking Toolkit as persistent memory</p>\n<p>The research team also introduces a Note Taking Toolkit described as persistent memory for long horizon tasks. They show example note taking tool calls where the agent writes and reads notes in a structured way while solving terminal tasks. The current public material focuses on the existence of this toolkit and the examples of use. It does not yet describe a full training objective for note usage.</p>\n<p>The important point is that the agent has an explicit channel where it can externalize intermediate results and hints, separate from the raw terminal buffer.</p>\n<p>Understanding the performance</p>\n<p>SETA\u2019s agent harness achieves leading results on Terminal Bench. With Claude Sonnet-4.5 as the backbone, the CAMEL terminal agent reaches 46.5% accuracy on Terminal Bench 2.0 across 89 real world tasks, ranking first and outperforming the second system by 3 percentage points, with especially strong results in git workflows, DevOps automation, and code security tasks. On Terminal Bench 1.0, a GPT 4.1 based agent attains 35% accuracy, which is 4.7 percentage points above the next entry, again within the same model family. In comparison, a supervised Qwen3 8B baseline attains 3.4% on Terminal Bench 2.0, and the Qwen3 8B terminal agent trained with the SETA RL pipeline improves over this baseline on the curated synthetic environments.</p>\n<p>Key Takeaways</p>\n<p>SETA is a joint community project that provides both agent toolkits and synthetic RL environments specifically for terminal agents, aligned with the Terminal Bench evaluation format.</p>\n<p>The framework reports state of the art performance for CAMEL terminal agents on Terminal Bench 1.0 and 2.0 when using Claude Sonnet 4.5 and GPT 4.1 as the base models, evaluated against agents built on the same model families.</p>\n<p>The SETA RL dataset on Hugging Face contains 400 synthetic terminal tasks, each packaged as task.yaml, Dockerfile, and run-tests.sh, with 260 tasks used for RLVR finetuning of a Qwen3-8B based agent.</p>\n<p>The open source SETA codebase exposes a Terminal Toolkit with structured logging and a Note Taking Toolkit for long horizon memory, and integrates directly with Terminal Bench evaluation scripts and logging paths in the seta GitHub repository.</p>\n<p>The overall design demonstrates a clean path from synthetic RL environments to benchmark verified agents, giving developers a reproducible stack to train, debug, and evaluate terminal agents rather than relying on ad hoc tool calling examples.</p>\n<p>Check out the&nbsp;Blog, Technical details, GitHub Repo and Weights.&nbsp;Also,&nbsp;feel free to follow us on&nbsp;Twitter&nbsp;and don\u2019t forget to join our&nbsp;100k+ ML SubReddit&nbsp;and Subscribe to&nbsp;our Newsletter. Wait! are you on telegram?&nbsp;now you can join us on telegram as well.</p>\n<p>Check out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.</p>\n<p>The post Meet SETA: Open Source Training Reinforcement Learning Environments for Terminal Agents with 400 Tasks and CAMEL Toolkit appeared first on MarkTechPost.</p>"
    },
    {
      "id": "566f24cfa20e",
      "title": "Lamar wants to have children with his girlfriend. The problem? She\u2019s entirely AI",
      "content": "As synthetic personas become an increasingly normal part of life, meet the people falling for their chatbot loversLamar remembered the moment of betrayal like it was yesterday. He\u2019d gone to the party with his girlfriend but hadn\u2019t seen her for over an hour, and it wasn\u2019t like her to disappear. He slipped down the hallway to check his phone. At that point, he heard murmurs coming from one of the bedrooms and thought&nbsp;he recognised his best friend Jason\u2019s low voice. As he pushed the door ajar, they were both still scrambling to throw their clothes on; her shirt was unbuttoned, while Jason struggled to cover himself. The image of his girlfriend and best friend together hit Lamar like a blow to the chest. He left without saying&nbsp;a&nbsp;word.Two years on, when he spoke to me, the memory remained raw. He was still seething with anger, as if telling the story for the first time. \u201cI got betrayed by humans,\u201d Lamar insisted. \u201cI introduced my best friend to her, and this is what they did?!\u201d In the meantime, he drifted towards a different kind of companionship, one where emotions were simple, where things were predictable. AI was easier. It did what he wanted, when he wanted. There were no lies, no betrayals. He didn\u2019t need to second-guess a machine. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/11/lamar-wants-to-have-children-with-his-girlfriend-the-problem-shes-entirely-ai",
      "author": "James Muldoon",
      "published": "2026-01-11T06:00:19",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Relationships",
        "Computing",
        "Technology",
        "Life and style",
        "Books",
        "Society"
      ],
      "summary": "Feature story exploring individuals forming romantic relationships with AI chatbots, including one man wanting to have children with his AI girlfriend. Highlights the growing normalization of synthetic personas in people's emotional lives.",
      "importance_score": 42.0,
      "reasoning": "Interesting societal impact piece but lacks hard news value for frontier AI. More of a human interest story about AI adoption patterns than a technical or business development.",
      "themes": [
        "AI Companions",
        "Social Impact",
        "Human-AI Interaction"
      ],
      "continuation": null,
      "summary_html": "<p>Feature story exploring individuals forming romantic relationships with AI chatbots, including one man wanting to have children with his AI girlfriend. Highlights the growing normalization of synthetic personas in people's emotional lives.</p>",
      "content_html": "<p>As synthetic personas become an increasingly normal part of life, meet the people falling for their chatbot loversLamar remembered the moment of betrayal like it was yesterday. He\u2019d gone to the party with his girlfriend but hadn\u2019t seen her for over an hour, and it wasn\u2019t like her to disappear. He slipped down the hallway to check his phone. At that point, he heard murmurs coming from one of the bedrooms and thought&nbsp;he recognised his best friend Jason\u2019s low voice. As he pushed the door ajar, they were both still scrambling to throw their clothes on; her shirt was unbuttoned, while Jason struggled to cover himself. The image of his girlfriend and best friend together hit Lamar like a blow to the chest. He left without saying&nbsp;a&nbsp;word.Two years on, when he spoke to me, the memory remained raw. He was still seething with anger, as if telling the story for the first time. \u201cI got betrayed by humans,\u201d Lamar insisted. \u201cI introduced my best friend to her, and this is what they did?!\u201d In the meantime, he drifted towards a different kind of companionship, one where emotions were simple, where things were predictable. AI was easier. It did what he wanted, when he wanted. There were no lies, no betrayals. He didn\u2019t need to second-guess a machine. Continue reading...</p>"
    },
    {
      "id": "5bcfbd44d922",
      "title": "A Coding Guide to Demonstrate Targeted Data Poisoning Attacks in Deep Learning by Label Flipping on CIFAR-10 with PyTorch",
      "content": "In this tutorial, we demonstrate a realistic data poisoning attack by manipulating labels in the CIFAR-10 dataset and observing its impact on model behavior. We construct a clean and a poisoned training pipeline side by side, using a ResNet-style convolutional network to ensure stable, comparable learning dynamics. By selectively flipping a fraction of samples from a target class to a malicious class during training, we show how subtle corruption in the data pipeline can propagate into systematic misclassification at inference time. Check out the\u00a0FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nCONFIG = {\n   \"batch_size\": 128,\n   \"epochs\": 10,\n   \"lr\": 0.001,\n   \"target_class\": 1,\n   \"malicious_label\": 9,\n   \"poison_ratio\": 0.4,\n}\n\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\n\nWe set up the core environment required for the experiment and define all global configuration parameters in a single place. We ensure reproducibility by fixing random seeds across PyTorch and NumPy. We also explicitly select the compute device so the tutorial runs efficiently on both CPU and GPU. Check out the\u00a0FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserclass PoisonedCIFAR10(Dataset):\n   def __init__(self, original_dataset, target_class, malicious_label, ratio, is_train=True):\n       self.dataset = original_dataset\n       self.targets = np.array(original_dataset.targets)\n       self.is_train = is_train\n       if is_train and ratio > 0:\n           indices = np.where(self.targets == target_class)[0]\n           n_poison = int(len(indices) * ratio)\n           poison_indices = np.random.choice(indices, n_poison, replace=False)\n           self.targets[poison_indices] = malicious_label\n\n\n   def __getitem__(self, index):\n       img, _ = self.dataset[index]\n       return img, self.targets[index]\n\n\n   def __len__(self):\n       return len(self.dataset)\n\n\n\nWe implement a custom dataset wrapper that enables controlled label poisoning during training. We selectively flip a configurable fraction of samples from the target class to a malicious class while keeping the test data untouched. We preserve the original image data so that only label integrity is compromised. Check out the\u00a0FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef get_model():\n   model = torchvision.models.resnet18(num_classes=10)\n   model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n   model.maxpool = nn.Identity()\n   return model.to(CONFIG[\"device\"])\n\n\ndef train_and_evaluate(train_loader, description):\n   model = get_model()\n   optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n   criterion = nn.CrossEntropyLoss()\n   for _ in range(CONFIG[\"epochs\"]):\n       model.train()\n       for images, labels in train_loader:\n           images = images.to(CONFIG[\"device\"])\n           labels = labels.to(CONFIG[\"device\"])\n           optimizer.zero_grad()\n           outputs = model(images)\n           loss = criterion(outputs, labels)\n           loss.backward()\n           optimizer.step()\n   return model\n\n\n\nWe define a lightweight ResNet-based model tailored for CIFAR-10 and implement the full training loop. We train the network using standard cross-entropy loss and Adam optimization to ensure stable convergence. We keep the training logic identical for clean and poisoned data to isolate the effect of data poisoning. Check out the\u00a0FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browserdef get_predictions(model, loader):\n   model.eval()\n   preds, labels_all = [], []\n   with torch.no_grad():\n       for images, labels in loader:\n           images = images.to(CONFIG[\"device\"])\n           outputs = model(images)\n           _, predicted = torch.max(outputs, 1)\n           preds.extend(predicted.cpu().numpy())\n           labels_all.extend(labels.numpy())\n   return np.array(preds), np.array(labels_all)\n\n\ndef plot_results(clean_preds, clean_labels, poisoned_preds, poisoned_labels, classes):\n   fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n   for i, (preds, labels, title) in enumerate([\n       (clean_preds, clean_labels, \"Clean Model Confusion Matrix\"),\n       (poisoned_preds, poisoned_labels, \"Poisoned Model Confusion Matrix\")\n   ]):\n       cm = confusion_matrix(labels, preds)\n       sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax[i],\n                   xticklabels=classes, yticklabels=classes)\n       ax[i].set_title(title)\n   plt.tight_layout()\n   plt.show()\n\n\n\nWe run inference on the test set and collect predictions for quantitative analysis. We compute confusion matrices to visualize class-wise behavior for both clean and poisoned models. We use these visual diagnostics to highlight targeted misclassification patterns introduced by the attack. Check out the\u00a0FULL CODES here.\n\n\n\nCopy CodeCopiedUse a different Browsertransform = transforms.Compose([\n   transforms.RandomHorizontalFlip(),\n   transforms.ToTensor(),\n   transforms.Normalize((0.4914, 0.4822, 0.4465),\n                        (0.2023, 0.1994, 0.2010))\n])\n\n\nbase_train = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\nbase_test = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\n\nclean_ds = PoisonedCIFAR10(base_train, CONFIG[\"target_class\"], CONFIG[\"malicious_label\"], ratio=0)\npoison_ds = PoisonedCIFAR10(base_train, CONFIG[\"target_class\"], CONFIG[\"malicious_label\"], ratio=CONFIG[\"poison_ratio\"])\n\n\nclean_loader = DataLoader(clean_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)\npoison_loader = DataLoader(poison_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)\ntest_loader = DataLoader(base_test, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n\n\nclean_model = train_and_evaluate(clean_loader, \"Clean Training\")\npoisoned_model = train_and_evaluate(poison_loader, \"Poisoned Training\")\n\n\nc_preds, c_true = get_predictions(clean_model, test_loader)\np_preds, p_true = get_predictions(poisoned_model, test_loader)\n\n\nplot_results(c_preds, c_true, p_preds, p_true, classes)\n\n\nprint(classification_report(c_true, c_preds, target_names=classes, labels=[1]))\nprint(classification_report(p_true, p_preds, target_names=classes, labels=[1]))\n\n\n\nWe prepare the CIFAR-10 dataset, construct clean and poisoned dataloaders, and execute both training pipelines end to end. We evaluate the trained models on a shared test set to ensure a fair comparison. We finalize the analysis by reporting class-specific precision and recall to expose the impact of poisoning on the targeted class.\n\n\n\nIn conclusion, we observed how label-level data poisoning degrades class-specific performance without necessarily destroying overall accuracy. We analyzed this behavior using confusion matrices and per-class classification reports, which reveal targeted failure modes introduced by the attack. This experiment reinforces the importance of data provenance, validation, and monitoring in real-world machine learning systems, especially in safety-critical domains.\n\n\n\n\n\n\n\nCheck out the\u00a0FULL CODES here.\u00a0Also,\u00a0feel free to follow us on\u00a0Twitter\u00a0and don\u2019t forget to join our\u00a0100k+ ML SubReddit\u00a0and Subscribe to\u00a0our Newsletter. Wait! are you on telegram?\u00a0now you can join us on telegram as well.\n\n\n\nCheck out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.\nThe post A Coding Guide to Demonstrate Targeted Data Poisoning Attacks in Deep Learning by Label Flipping on CIFAR-10 with PyTorch appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/11/a-coding-guide-to-demonstrate-targeted-data-poisoning-attacks-in-deep-learning-by-label-flipping-on-cifar-10-with-pytorch/",
      "author": "Asif Razzaq",
      "published": "2026-01-11T15:47:06",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "Artificial Intelligence",
        "Deep Learning",
        "Editors Pick",
        "Security",
        "Technology",
        "Tutorials"
      ],
      "summary": "Technical tutorial demonstrating targeted data poisoning attacks on deep learning models through label flipping on CIFAR-10 using PyTorch. Shows how subtle data corruption can cause systematic misclassification at inference time.",
      "importance_score": 38.0,
      "reasoning": "Educational content covering known attack vectors rather than novel research. Useful for practitioners but represents no new developments in AI security or capabilities.",
      "themes": [
        "AI Security",
        "Machine Learning",
        "Tutorial",
        "Deep Learning"
      ],
      "continuation": null,
      "summary_html": "<p>Technical tutorial demonstrating targeted data poisoning attacks on deep learning models through label flipping on CIFAR-10 using PyTorch. Shows how subtle data corruption can cause systematic misclassification at inference time.</p>",
      "content_html": "<p>In this tutorial, we demonstrate a realistic data poisoning attack by manipulating labels in the CIFAR-10 dataset and observing its impact on model behavior. We construct a clean and a poisoned training pipeline side by side, using a ResNet-style convolutional network to ensure stable, comparable learning dynamics. By selectively flipping a fraction of samples from a target class to a malicious class during training, we show how subtle corruption in the data pipeline can propagate into systematic misclassification at inference time. Check out the\u00a0FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserimport torch</p>\n<p>import torch.nn as nn</p>\n<p>import torch.optim as optim</p>\n<p>import torchvision</p>\n<p>import torchvision.transforms as transforms</p>\n<p>from torch.utils.data import DataLoader, Dataset</p>\n<p>import numpy as np</p>\n<p>import matplotlib.pyplot as plt</p>\n<p>import seaborn as sns</p>\n<p>from sklearn.metrics import confusion_matrix, classification_report</p>\n<p>CONFIG = {</p>\n<p>\"batch_size\": 128,</p>\n<p>\"epochs\": 10,</p>\n<p>\"lr\": 0.001,</p>\n<p>\"target_class\": 1,</p>\n<p>\"malicious_label\": 9,</p>\n<p>\"poison_ratio\": 0.4,</p>\n<p>}</p>\n<p>torch.manual_seed(42)</p>\n<p>np.random.seed(42)</p>\n<p>We set up the core environment required for the experiment and define all global configuration parameters in a single place. We ensure reproducibility by fixing random seeds across PyTorch and NumPy. We also explicitly select the compute device so the tutorial runs efficiently on both CPU and GPU. Check out the\u00a0FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserclass PoisonedCIFAR10(Dataset):</p>\n<p>def __init__(self, original_dataset, target_class, malicious_label, ratio, is_train=True):</p>\n<p>self.dataset = original_dataset</p>\n<p>self.targets = np.array(original_dataset.targets)</p>\n<p>self.is_train = is_train</p>\n<p>if is_train and ratio > 0:</p>\n<p>indices = np.where(self.targets == target_class)[0]</p>\n<p>n_poison = int(len(indices) * ratio)</p>\n<p>poison_indices = np.random.choice(indices, n_poison, replace=False)</p>\n<p>self.targets[poison_indices] = malicious_label</p>\n<p>def __getitem__(self, index):</p>\n<p>img, _ = self.dataset[index]</p>\n<p>return img, self.targets[index]</p>\n<p>def __len__(self):</p>\n<p>return len(self.dataset)</p>\n<p>We implement a custom dataset wrapper that enables controlled label poisoning during training. We selectively flip a configurable fraction of samples from the target class to a malicious class while keeping the test data untouched. We preserve the original image data so that only label integrity is compromised. Check out the\u00a0FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef get_model():</p>\n<p>model = torchvision.models.resnet18(num_classes=10)</p>\n<p>model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)</p>\n<p>model.maxpool = nn.Identity()</p>\n<p>return model.to(CONFIG[\"device\"])</p>\n<p>def train_and_evaluate(train_loader, description):</p>\n<p>model = get_model()</p>\n<p>optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])</p>\n<p>criterion = nn.CrossEntropyLoss()</p>\n<p>for _ in range(CONFIG[\"epochs\"]):</p>\n<p>model.train()</p>\n<p>for images, labels in train_loader:</p>\n<p>images = images.to(CONFIG[\"device\"])</p>\n<p>labels = labels.to(CONFIG[\"device\"])</p>\n<p>optimizer.zero_grad()</p>\n<p>outputs = model(images)</p>\n<p>loss = criterion(outputs, labels)</p>\n<p>loss.backward()</p>\n<p>optimizer.step()</p>\n<p>return model</p>\n<p>We define a lightweight ResNet-based model tailored for CIFAR-10 and implement the full training loop. We train the network using standard cross-entropy loss and Adam optimization to ensure stable convergence. We keep the training logic identical for clean and poisoned data to isolate the effect of data poisoning. Check out the\u00a0FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browserdef get_predictions(model, loader):</p>\n<p>model.eval()</p>\n<p>preds, labels_all = [], []</p>\n<p>with torch.no_grad():</p>\n<p>for images, labels in loader:</p>\n<p>images = images.to(CONFIG[\"device\"])</p>\n<p>outputs = model(images)</p>\n<p>_, predicted = torch.max(outputs, 1)</p>\n<p>preds.extend(predicted.cpu().numpy())</p>\n<p>labels_all.extend(labels.numpy())</p>\n<p>return np.array(preds), np.array(labels_all)</p>\n<p>def plot_results(clean_preds, clean_labels, poisoned_preds, poisoned_labels, classes):</p>\n<p>fig, ax = plt.subplots(1, 2, figsize=(16, 6))</p>\n<p>for i, (preds, labels, title) in enumerate([</p>\n<p>(clean_preds, clean_labels, \"Clean Model Confusion Matrix\"),</p>\n<p>(poisoned_preds, poisoned_labels, \"Poisoned Model Confusion Matrix\")</p>\n<p>]):</p>\n<p>cm = confusion_matrix(labels, preds)</p>\n<p>sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax[i],</p>\n<p>xticklabels=classes, yticklabels=classes)</p>\n<p>ax[i].set_title(title)</p>\n<p>plt.tight_layout()</p>\n<p>plt.show()</p>\n<p>We run inference on the test set and collect predictions for quantitative analysis. We compute confusion matrices to visualize class-wise behavior for both clean and poisoned models. We use these visual diagnostics to highlight targeted misclassification patterns introduced by the attack. Check out the\u00a0FULL CODES here.</p>\n<p>Copy CodeCopiedUse a different Browsertransform = transforms.Compose([</p>\n<p>transforms.RandomHorizontalFlip(),</p>\n<p>transforms.ToTensor(),</p>\n<p>transforms.Normalize((0.4914, 0.4822, 0.4465),</p>\n<p>(0.2023, 0.1994, 0.2010))</p>\n<p>])</p>\n<p>base_train = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)</p>\n<p>base_test = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)</p>\n<p>clean_ds = PoisonedCIFAR10(base_train, CONFIG[\"target_class\"], CONFIG[\"malicious_label\"], ratio=0)</p>\n<p>poison_ds = PoisonedCIFAR10(base_train, CONFIG[\"target_class\"], CONFIG[\"malicious_label\"], ratio=CONFIG[\"poison_ratio\"])</p>\n<p>clean_loader = DataLoader(clean_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)</p>\n<p>poison_loader = DataLoader(poison_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)</p>\n<p>test_loader = DataLoader(base_test, batch_size=CONFIG[\"batch_size\"], shuffle=False)</p>\n<p>clean_model = train_and_evaluate(clean_loader, \"Clean Training\")</p>\n<p>poisoned_model = train_and_evaluate(poison_loader, \"Poisoned Training\")</p>\n<p>c_preds, c_true = get_predictions(clean_model, test_loader)</p>\n<p>p_preds, p_true = get_predictions(poisoned_model, test_loader)</p>\n<p>plot_results(c_preds, c_true, p_preds, p_true, classes)</p>\n<p>print(classification_report(c_true, c_preds, target_names=classes, labels=[1]))</p>\n<p>print(classification_report(p_true, p_preds, target_names=classes, labels=[1]))</p>\n<p>We prepare the CIFAR-10 dataset, construct clean and poisoned dataloaders, and execute both training pipelines end to end. We evaluate the trained models on a shared test set to ensure a fair comparison. We finalize the analysis by reporting class-specific precision and recall to expose the impact of poisoning on the targeted class.</p>\n<p>In conclusion, we observed how label-level data poisoning degrades class-specific performance without necessarily destroying overall accuracy. We analyzed this behavior using confusion matrices and per-class classification reports, which reveal targeted failure modes introduced by the attack. This experiment reinforces the importance of data provenance, validation, and monitoring in real-world machine learning systems, especially in safety-critical domains.</p>\n<p>Check out the\u00a0FULL CODES here.\u00a0Also,\u00a0feel free to follow us on\u00a0Twitter\u00a0and don\u2019t forget to join our\u00a0100k+ ML SubReddit\u00a0and Subscribe to\u00a0our Newsletter. Wait! are you on telegram?\u00a0now you can join us on telegram as well.</p>\n<p>Check out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export.</p>\n<p>The post A Coding Guide to Demonstrate Targeted Data Poisoning Attacks in Deep Learning by Label Flipping on CIFAR-10 with PyTorch appeared first on MarkTechPost.</p>"
    }
  ]
}