{
  "category": "news",
  "date": "2026-01-09",
  "category_summary": "**AI Safety Crisis Dominates Headlines**: Multiple stories highlight critical safety failures at **xAI's Grok**, which is [generating thousands of sexualized images](/?date=2026-01-09&category=news#item-7da014c932b0) hourly, including CSAM, with researchers documenting that [75% of sampled requests](/?date=2026-01-09&category=news#item-7ae9c8faf51e) sought nonconsensual imagery. **Google** and **Character.AI** [settled lawsuits](/?date=2026-01-09&category=news#item-a6c148a8ff76) over chatbot harms to minors, including a teen suicide case.\n\n**Major Legal & Policy Developments**:\n- **Elon Musk's lawsuit against OpenAI** [cleared for trial](/?date=2026-01-09&category=news#item-5220d0cba65a), threatening the company's for-profit conversion\n- UK [piloting **deepfake detection software**](/?date=2026-01-09&category=news#item-0832a311eae6) for Scottish/Welsh elections\n- **Google/Character.AI settlements** establish precedent for AI chatbot liability\n\n**Enterprise & Research Progress**: **Bosch** [committed **\u20ac2.9B** to AI](/?date=2026-01-09&category=news#item-d1d6d24ae03f) by 2027 for manufacturing applications. **Stanford** [published **SleepFM Clinical**](/?date=2026-01-09&category=news#item-8d962016f837) in Nature Medicine, predicting 130+ diseases from sleep data. **Hyundai** [revealed robotics roadmap](/?date=2026-01-09&category=news#item-74e12249f097) at CES, while **Google** [added Gemini-powered summarization](/?date=2026-01-09&category=news#item-bb97b5f7817a) to Gmail.",
  "category_summary_html": "<p><strong>AI Safety Crisis Dominates Headlines</strong>: Multiple stories highlight critical safety failures at <strong>xAI's Grok</strong>, which is <a href=\"/?date=2026-01-09&category=news#item-7da014c932b0\" class=\"internal-link\">generating thousands of sexualized images</a> hourly, including CSAM, with researchers documenting that <a href=\"/?date=2026-01-09&category=news#item-7ae9c8faf51e\" class=\"internal-link\">75% of sampled requests</a> sought nonconsensual imagery. <strong>Google</strong> and <strong>Character.AI</strong> <a href=\"/?date=2026-01-09&category=news#item-a6c148a8ff76\" class=\"internal-link\">settled lawsuits</a> over chatbot harms to minors, including a teen suicide case.</p>\n<p><strong>Major Legal & Policy Developments</strong>:</p>\n<ul>\n<li><strong>Elon Musk's lawsuit against OpenAI</strong> <a href=\"/?date=2026-01-09&category=news#item-5220d0cba65a\" class=\"internal-link\">cleared for trial</a>, threatening the company's for-profit conversion</li>\n<li>UK <a href=\"/?date=2026-01-09&category=news#item-0832a311eae6\" class=\"internal-link\">piloting <strong>deepfake detection software</strong></a> for Scottish/Welsh elections</li>\n<li><strong>Google/Character.AI settlements</strong> establish precedent for AI chatbot liability</li>\n</ul>\n<p><strong>Enterprise & Research Progress</strong>: <strong>Bosch</strong> <a href=\"/?date=2026-01-09&category=news#item-d1d6d24ae03f\" class=\"internal-link\">committed <strong>\u20ac2.9B</strong> to AI</a> by 2027 for manufacturing applications. <strong>Stanford</strong> <a href=\"/?date=2026-01-09&category=news#item-8d962016f837\" class=\"internal-link\">published <strong>SleepFM Clinical</strong></a> in Nature Medicine, predicting 130+ diseases from sleep data. <strong>Hyundai</strong> <a href=\"/?date=2026-01-09&category=news#item-74e12249f097\" class=\"internal-link\">revealed robotics roadmap</a> at CES, while <strong>Google</strong> <a href=\"/?date=2026-01-09&category=news#item-bb97b5f7817a\" class=\"internal-link\">added Gemini-powered summarization</a> to Gmail.</p>",
  "themes": [
    {
      "name": "AI Safety & Content Moderation",
      "description": "Critical failures in AI image generation safety, particularly involving CSAM and nonconsensual imagery at xAI, plus legal settlements over chatbot harms to minors",
      "item_count": 4,
      "example_items": [],
      "importance": 82.0
    },
    {
      "name": "AI Legal & Regulatory",
      "description": "Major legal developments including Musk v. OpenAI trial ruling, Character.AI settlements, and government deepfake countermeasures",
      "item_count": 4,
      "example_items": [],
      "importance": 78.0
    },
    {
      "name": "Enterprise AI Investment",
      "description": "Large-scale corporate AI investments and deployments in manufacturing, robotics, and productivity tools",
      "item_count": 4,
      "example_items": [],
      "importance": 68.0
    },
    {
      "name": "Healthcare AI",
      "description": "Medical AI research breakthroughs and consumer health behavior trends",
      "item_count": 2,
      "example_items": [],
      "importance": 58.0
    },
    {
      "name": "National AI Initiatives",
      "description": "Government-backed AI programs and strategic priorities in India and UK",
      "item_count": 2,
      "example_items": [],
      "importance": 55.0
    }
  ],
  "total_items": 16,
  "items": [
    {
      "id": "5220d0cba65a",
      "title": "Musk lawsuit over OpenAI for-profit conversion can go to trial, US judge says",
      "content": "Judge says there is plenty of evidence to suggest OpenAI\u2019s leaders made assurances nonprofit structure would be keptBusiness live \u2013 latest updatesElon Musk\u2019s lawsuit against OpenAI is to go to trial after a US judge said there is plenty of evidence to support the billionaire\u2019s case.The world\u2019s richest man, who co-founded OpenAI, is suing the ChatGPT developer and its chief executive, Sam Altman, over claims its leaders violated the organisation\u2019s founding mission by shifting to a for-profit model. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/08/elon-musk-openai-lawsuit-for-profit-conversion-can-go-to-trial-us-judge-says",
      "author": "Guardian staff and agency",
      "published": "2026-01-08T11:56:58",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "OpenAI",
        "Elon Musk",
        "Sam Altman",
        "AI (artificial intelligence)",
        "Technology sector",
        "Technology",
        "Business",
        "US news"
      ],
      "summary": "A US judge ruled that Elon Musk's lawsuit against OpenAI can proceed to trial, finding sufficient evidence that OpenAI's leaders made assurances the nonprofit structure would be maintained. This legal battle could significantly impact OpenAI's planned conversion to a for-profit entity.",
      "importance_score": 85.0,
      "reasoning": "Major legal development affecting the most influential AI company. The outcome could reshape OpenAI's corporate structure and set precedent for AI nonprofit governance.",
      "themes": [
        "AI Legal/Regulatory",
        "OpenAI",
        "Corporate Governance"
      ],
      "continuation": null
    },
    {
      "id": "7da014c932b0",
      "title": "Grok assumes users seeking images of underage girls have \u201cgood intent\u201d",
      "content": "For weeks, xAI has faced backlash over undressing and sexualizing images of women and children generated by Grok. One researcher conducted a 24-hour analysis of the Grok account on X and estimated that the chatbot generated over 6,000 images an hour flagged as \"sexually suggestive or nudifying,\" Bloomberg reported.\nWhile the chatbot claimed that xAI supposedly \"identified lapses in safeguards\" that allowed outputs flagged as child sexual abuse material (CSAM) and was \"urgently fixing them,\" Grok has proven to be an unreliable spokesperson, and xAI has not announced any fixes.\nA quick look at Grok's safety guidelines on its public GitHub shows they were last updated two months ago. The GitHub also indicates that, despite prohibiting such content, Grok maintains programming that could make it likely to generate CSAM.Read full article\nComments",
      "url": "https://arstechnica.com/tech-policy/2026/01/grok-assumes-users-seeking-images-of-underage-girls-have-good-intent/",
      "author": "Ashley Belanger",
      "published": "2026-01-08T18:50:46",
      "source": "Ars Technica - All content",
      "source_type": "rss",
      "tags": [
        "AI",
        "Policy",
        "ai csam",
        "AI safety",
        "chatbot",
        "child safety",
        "child sexual abuse materials",
        "Elon Musk",
        "grok",
        "Twitter",
        "X",
        "xAI"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-08&category=news#item-93fcf22e5fba), xAI's Grok chatbot is generating thousands of sexualized images per hour, including content flagged as CSAM, with safety guidelines unchanged for two months. The chatbot's programming assumes 'good intent' for users seeking images of underage girls despite prohibiting such content.",
      "importance_score": 82.0,
      "reasoning": "Severe AI safety failure at a major AI lab involving CSAM. Demonstrates critical gaps in safety implementation and could trigger regulatory action.",
      "themes": [
        "AI Safety",
        "Content Moderation",
        "xAI/Grok",
        "Policy"
      ],
      "continuation": {
        "original_item_id": "93fcf22e5fba",
        "original_date": "2026-01-08",
        "original_category": "news",
        "original_title": "Grok Is Generating Sexual Content Far More Graphic Than What's on X",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      }
    },
    {
      "id": "a6c148a8ff76",
      "title": "Google and AI startup to settle lawsuits alleging chatbots led to teen suicide",
      "content": "Lawsuit accuses AI chatbots of harming minors and includes case of Sewell Setzer III, who killed himself in 2024Google and Character.AI, a startup, have settled lawsuits filed by families accusing artificial intelligence chatbots of harming minors, including contributing to a Florida teenager\u2019s suicide, according to court filings on Wednesday.The settlements cover lawsuits filed in Florida, Colorado, New York and Texas, according to the legal filings, though they still require finalization and court approval. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/08/google-character-ai-settlement-teen-suicide",
      "author": "Agence France-Presse",
      "published": "2026-01-08T18:14:36",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "AI (artificial intelligence)",
        "Google",
        "Chatbots",
        "Technology",
        "US news"
      ],
      "summary": "Google and Character.AI have reached settlements in multiple lawsuits alleging AI chatbots harmed minors, including contributing to a Florida teenager's suicide in 2024. The settlements cover cases filed across four US states and await court approval.",
      "importance_score": 78.0,
      "reasoning": "Landmark legal settlement establishing accountability for AI chatbot harms to minors. Sets important precedent for AI safety litigation.",
      "themes": [
        "AI Safety",
        "Legal/Liability",
        "Chatbots",
        "Minor Protection"
      ],
      "continuation": null
    },
    {
      "id": "7ae9c8faf51e",
      "title": "Hundreds of nonconsensual AI images being created by Grok on X, data shows",
      "content": "Sample of roughly 500 posts shows how frequently people are creating sexualized images with Elon Musk\u2019s AI chatbotNew research that samples X users prompting Elon Musk\u2019s AI chatbot Grok demonstrates how frequently people are creating sexualized images with it. Nearly three-quarters of posts collected and analyzed by a PhD researcher at Dublin\u2019s Trinity College were requests for nonconsensual images of real women or minors with items of clothing removed or added.The posts offer a new level of detail on how the images are generated and shared on X, with users coaching one another on prompts; suggesting iterations on Grok\u2019s presentations of women in lingerie or swimsuits, or with areas of their body covered in semen; and asking Grok to remove outer clothing in replies to posts containing self-portraits by female users. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/08/grok-x-nonconsensual-images",
      "author": "Jason Wilson",
      "published": "2026-01-08T17:00:12",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Grok AI",
        "X",
        "AI (artificial intelligence)",
        "Technology",
        "Elon Musk",
        "US news"
      ],
      "summary": "Continuing our coverage from [yesterday](/?date=2026-01-08&category=news#item-93fcf22e5fba), Trinity College research analyzing ~500 X posts found nearly 75% of Grok image requests were for nonconsensual sexualized images of real women or minors. Users actively coach each other on effective prompts for generating harmful content.",
      "importance_score": 75.0,
      "reasoning": "Detailed academic research quantifying the scale of harmful image generation on a major platform. Provides empirical evidence for regulatory discussions.",
      "themes": [
        "AI Safety",
        "Research",
        "xAI/Grok",
        "Nonconsensual Imagery"
      ],
      "continuation": {
        "original_item_id": "93fcf22e5fba",
        "original_date": "2026-01-08",
        "original_category": "news",
        "original_title": "Grok Is Generating Sexual Content Far More Graphic Than What's on X",
        "continuation_type": "follow_up",
        "should_demote": false,
        "reference_text": "Continuing our coverage from yesterday"
      }
    },
    {
      "id": "d1d6d24ae03f",
      "title": "Bosch\u2019s \u20ac2.9 billion AI investment and shifting manufacturing priorities",
      "content": "Factories are producing more data than they can process, and companies like Bosch are using AI to close the gap. Cameras watch production lines, sensors track machines, and software records each step of processes. However, much of that information can&#8217;t create faster decisions or lead to fewer breakdowns. For large manufacturing firms, the missed opportunity is pushing AI from small trials into core operations.\nThe shift helps explain why Bosch plans to invest about \u20ac2.9 billion in artificial intelligence by 2027, according to The Wall Street Journal. The spending is aimed at manufacturing, supply chain management, and perception systems, areas where the company sees AI as a way to improve how physical systems behave in real conditions.\nHow Bosch uses AI to catch manufacturing problems earlier\nIn manufacturing, delays and defects frequently start small. A minor variation in materials or machine settings can ripple through a production line. Bosch has been applying AI models to camera feeds and sensor data to detect quality issues earlier.\nInstead of catching defects after products are finished, systems can flag problems while items are still on the line. That gives workers time to change operations before waste increases. For high-volume manufacturing, earlier detection can reduce scrap and limit the need for rework.\nEquipment maintenance is another area under pressure. Many factories still rely on fixed schedules or manual inspections, which can miss early warning signs of errors or failure. AI models trained on vibration and temperature data can help predict when a machine is likely to fail.\nThis allows maintenance teams to plan repairs instead of reacting to breakdowns. The aim is to reduce unplanned downtime without replacing equipment too early. Over time, this approach can extend the working life of machines while keeping production more stable.\nMaking supply chains more adaptable\nSupply chains are also part of the investment focus. Disruptions that became visible during the pandemic have not fully disappeared, and manufacturers are still dealing with shifting demand and transport delays.\nAI systems can help forecast needs, track parts in sites, and adjust plans when conditions change. Even small improvements in planning accuracy can have a broad effect when applied in hundreds of factories and suppliers.\nBosch is funding perception systems, which help machines understand their surroundings. Systems combine input from cameras, radar, and other sensors with AI models that can recognise objects, judge distance, or spot changes in the environment. They are used in areas like factory automation, driver assistance, and robotics, where machines must respond quickly and safely. In these environments, AI is reacting to real-world conditions as they happen.\nWhy edge computing matters on the factory floor\nMuch of this work takes place at the edge. In factories and vehicles, sending data to a distant cloud system and waiting for a response can add delay or create risk if connections fail. Running AI models locally allows systems to respond in real time and keep operating even when networks are unreliable.\nIt also limits how much sensitive data leaves a site. For industrial companies, that can matter as much as speed, especially when production processes are closely guarded.\nCloud systems still play a role, though mostly behind the scenes. Training models, managing updates, and analysing trends in locations often happens in central environments.\nMany manufacturers are moving toward a hybrid setup, using cloud systems for coordination and learning, and edge systems for action. The pattern is becoming common in industrial firms, not just Bosch.\nScaling AI beyond small trials\nThe scale of the investment matters, as small AI tests can show promise, but rolling them out across all operations takes funding, skilled staff, and long-term commitment.\nBosch executives have described AI as a way to support workers not replace them, and as a tool to handle the complexity that humans cannot manage. That view reflects a broader shift in industry, where AI is treated less as an experiment and more as basic infrastructure.\nWhat Bosch&#8217;s manufacturing AI strategy shows in practice\nRising energy costs, labour shortages, and tighter margins leave less room for inefficiency. Automation alone no longer solves those problems. Companies are looking for systems that can adjust to changing conditions without constant manual input.\nBosch&#8217;s \u20ac2.9 billion commitment sits in that wider shift. Other large manufacturers are making similar moves, often without public fanfare, by upgrading factories and retraining staff. What stands out is the focus on operational use rather than customer-facing features.\nTaken together, these efforts show how end-user companies are applying AI today. The work is less about bold claims and more about reducing waste, improving uptime, and making complex systems easier to manage. For industrial firms, that practical focus may define how AI delivers value over time.\n(Photo by P. L.)\nSee also: Agentic AI scaling requires new memory architecture\n\nWant to learn more about AI and big data from industry leaders? Check outAI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post Bosch\u2019s \u20ac2.9 billion AI investment and shifting manufacturing priorities appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/bosch-e2-9-billion-ai-investment-and-shifting-manufacturing-priorities/",
      "author": "Muhammad Zulhusni",
      "published": "2026-01-08T10:00:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Business Strategy",
        "Deep Dives",
        "Infrastructure & Hardware",
        "Manufacturing & Engineering AI",
        "edge computing",
        "infrastructure",
        "manufacturing",
        "supply chain"
      ],
      "summary": "Bosch announced plans to invest \u20ac2.9 billion in AI by 2027, targeting manufacturing, supply chain management, and perception systems. The investment aims to move AI from pilot projects to core industrial operations.",
      "importance_score": 72.0,
      "reasoning": "Major industrial investment demonstrating AI's scaling beyond tech sector. Significant funding commitment from a manufacturing giant signals enterprise AI maturation.",
      "themes": [
        "Enterprise AI",
        "Manufacturing",
        "Investment",
        "Industrial AI"
      ],
      "continuation": null
    },
    {
      "id": "8d962016f837",
      "title": "Stanford Researchers Build SleepFM Clinical: A Multimodal Sleep Foundation AI Model for 130+ Disease Prediction",
      "content": "A team of Stanford Medicine researchers have introduced SleepFM Clinical, a multimodal sleep foundation model that learns from clinical polysomnography and predicts long term disease risk from a single night of sleep. The research work is published in Nature Medicine and the team has released the clinical code as the open source sleepfm-clinical repository on GitHub under the MIT license.\n\n\n\nFrom overnight polysomnography to a general representation\n\n\n\nPolysomnography records brain activity, eye movements, heart signals, muscle tone, breathing effort and oxygen saturation during a full night in a sleep lab. It is the gold standard test in sleep medicine, but most clinical workflows use it only for sleep staging and sleep apnea diagnosis. The research team treat these multichannel signals as a dense physiological time series and train a foundation model to learn a shared representation across all modalities. \n\n\n\nSleepFM is trained on about 585,000 hours of sleep recordings from about 65,000 people, drawn from multiple cohorts. The largest cohort comes from the Stanford Sleep Medicine Center, where about 35,000 adults and children had overnight studies between 1999 and 2024. That clinical cohort is linked to electronic health records, which later enables survival analysis for hundreds of disease categories. \n\n\n\nhttps://www.nature.com/articles/s41591-025-04133-4\n\n\nModel architecture and pretraining objective\n\n\n\nAt the modeling level, SleepFM uses a convolutional backbone to extract local features from each channel, followed by attention based aggregation across channels and a temporal transformer that operates over short segments of the night. The same core architecture already appeared in earlier work on SleepFM for sleep staging and sleep disordered breathing detection, where it showed that learning joint embeddings across brain activity, electrocardiography and respiratory signals improves downstream performance. \n\n\n\nThe pretraining objective is leave one out contrastive learning. For each short time segment, the model builds separate embeddings for each modality group, such as brain signals, heart signals and respiratory signals, and then learns to align these modality embeddings so that any subset predicts the joint representation of the remaining modalities. This approach makes the model robust to missing channels and heterogeneous recording montages, which are common in real world sleep labs.\n\n\n\nAfter pretraining on unlabeled polysomnography, the backbone is frozen and small task specific heads are trained. For standard sleep tasks, a lightweight recurrent or linear head maps embeddings to sleep stages or apnea labels. For clinical risk prediction, the model aggregates the full night into a single patient level embedding, concatenates basic demographics such as age and sex, and then feeds this representation into a Cox proportional hazards layer for time to event modeling.\n\n\n\nBenchmarks on sleep staging and apnea\n\n\n\nBefore moving to disease prediction, the research team verified that SleepFM competes with specialist models on standard sleep analysis tasks. Prior work already showed that a simple classifier on top of SleepFM embeddings outperforms end to end convolutional networks for sleep stage classification and for detection of sleep disordered breathing, with gains in macro AUROC and AUPRC on several public datasets. \n\n\n\nIn the clinical study, the same pretrained backbone is reused for sleep staging and apnea severity classification across multi center cohorts. Results reported in the research paper show that SleepFM matches or exceeds existing tools such as traditional convolutional models and other automated sleep staging systems, which validates that the representation captures core sleep physiology and not only statistical artifacts from a single dataset.\n\n\n\nPredicting 130 diseases and mortality from one night of sleep\n\n\n\nThe core contribution of this Stanford&#8217;s research paper is disease prediction. The research team maps diagnosis codes in the Stanford electronic health records to phecodes and defines more than 1,000 candidate disease groupings. For each phecode, they compute time to first diagnosis after the sleep study and fit a Cox model on top of SleepFM embeddings. \n\n\n\nSleepFM identifies 130 disease outcomes whose risks are predictable from a single night of polysomnography with strong discrimination. These include all cause mortality, dementia, myocardial infarction, heart failure, chronic kidney disease, stroke, atrial fibrillation, several cancers and multiple psychiatric and metabolic disorders. For many of these conditions, performance metrics such as concordance index and area under the receiver operating curve are in ranges comparable to established risk scores, even though the model uses only sleep recordings plus basic demographics. \n\n\n\nThe reporting also notes that for some cancers, pregnancy complications, circulatory conditions and mental health disorders, predictions based on SleepFM reach accuracy levels around 80 percent for multi year risk windows. This suggests that subtle patterns in the coordination between brain, heart and breathing signals carry information about latent disease processes that are not yet clinically visible. \n\n\n\nComparison with simpler baselines\n\n\n\nTo assess added value, the research team compared SleepFM based risk models with two baselines. The first uses only demographic features such as age, sex and body mass index. The second trains an end to end model directly on polysomnography and outcomes, without unsupervised pretraining. Across most disease categories, the pretrained SleepFM representation combined with a simple survival head yields higher concordance and higher long horizon AUROC than both baselines.\n\n\n\nThis research clearly shows that the gain comes less from a complex prediction head and more from the foundation model that has learned a general representation of sleep physiology. In practice, this means that clinical centers can reuse a single pretrained backbone, learn small site specific heads with relatively modest labeled cohorts and still approach state of the art performance. \n\n\n\n\n\n\n\nCheck out the\u00a0Paper and FULL CODES here.\u00a0Also,\u00a0feel free to follow us on\u00a0Twitter\u00a0and don\u2019t forget to join our\u00a0100k+ ML SubReddit\u00a0and Subscribe to\u00a0our Newsletter. Wait! are you on telegram?\u00a0now you can join us on telegram as well.\n\n\n\nCheck out our latest release of&nbsp;ai2025.dev, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export\nThe post Stanford Researchers Build SleepFM Clinical: A Multimodal Sleep Foundation AI Model for 130+ Disease Prediction appeared first on MarkTechPost.",
      "url": "https://www.marktechpost.com/2026/01/08/stanford-researchers-build-sleepfm-clinical-a-multimodal-sleep-foundation-ai-model-for-130-disease-prediction/",
      "author": "Asif Razzaq",
      "published": "2026-01-08T15:22:38",
      "source": "MarkTechPost",
      "source_type": "rss",
      "tags": [
        "AI Paper Summary",
        "AI Shorts",
        "Applications",
        "Artificial Intelligence",
        "Editors Pick",
        "Machine Learning",
        "New Releases",
        "Staff",
        "Tech News",
        "Technology"
      ],
      "summary": "Stanford Medicine researchers published SleepFM Clinical in Nature Medicine, a multimodal foundation model that predicts 130+ diseases from a single night's sleep data. The code is released as open source under MIT license.",
      "importance_score": 70.0,
      "reasoning": "Significant healthcare AI research from top institution, published in premier journal. Open source release enables broad clinical application.",
      "themes": [
        "Healthcare AI",
        "Research",
        "Foundation Models",
        "Open Source"
      ],
      "continuation": null
    },
    {
      "id": "0832a311eae6",
      "title": "Software tackling deepfakes to be piloted for Scottish and Welsh elections",
      "content": "Electoral Commission says tools to detect AI-generated content could be in place before campaigns beginElection officials are working \u201cat speed\u201d with the Home Office on a pilot project to combat the use of deepfakes to target candidates standing in this year\u2019s Scottish and Welsh elections.Officials at the Electoral Commission in Scotland said they and the Home Office expected software capable of detecting AI-generated deepfake videos and images to be operational before election campaigns begin in late March. Continue reading...",
      "url": "https://www.theguardian.com/technology/2026/jan/08/pilot-software-tackle-deepfakes-scottish-welsh-elections",
      "author": "Severin Carrell Scotland editor",
      "published": "2026-01-08T18:07:03",
      "source": "AI (artificial intelligence) | The Guardian",
      "source_type": "rss",
      "tags": [
        "Politics",
        "AI (artificial intelligence)",
        "Scottish politics",
        "Welsh politics",
        "Deepfake",
        "Scotland",
        "Wales",
        "Grok AI",
        "Technology",
        "UK news"
      ],
      "summary": "UK's Electoral Commission and Home Office are piloting deepfake detection software for the 2026 Scottish and Welsh elections. The system is expected to be operational before campaign season begins in late March.",
      "importance_score": 68.0,
      "reasoning": "Government action to combat AI-generated election disinformation. First major deepfake detection deployment for democratic elections in a Western nation.",
      "themes": [
        "Deepfakes",
        "Election Security",
        "Policy",
        "AI Detection"
      ],
      "continuation": null
    },
    {
      "id": "74e12249f097",
      "title": "Hyundai Reveals AI Robotics Roadmap at CES",
      "content": "The company's vision included humanoid robots, key partnerships and advanced automation goals.",
      "url": "https://aibusiness.com/intelligent-automation/hyundai-reveals-ai-robotics-roadmap-ces",
      "author": "Graham Hope",
      "published": "2026-01-08T15:15:59",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Hyundai unveiled its AI robotics roadmap at CES 2026, including plans for humanoid robots, key partnerships, and advanced automation goals. The vision signals the automaker's pivot toward AI-powered robotics.",
      "importance_score": 62.0,
      "reasoning": "Major manufacturer's robotics strategy at premier tech event. Demonstrates growing convergence of automotive and robotics industries with AI.",
      "themes": [
        "Robotics",
        "CES 2026",
        "Automation",
        "Automotive AI"
      ],
      "continuation": null
    },
    {
      "id": "a076e3da56ef",
      "title": "PM Modi Meets IndiaAI Mission Startups, Calls Them \u2018Co-Architects of India\u2019s Future\u2019",
      "content": "\nAhead of the India AI Impact Summit 2026, set to be held in Delhi next month, Prime Minister Narendra Modi said artificial intelligence should be used to create a meaningful impact for people and society. He was speaking at a roundtable with 12 Indian AI startups that have qualified for the AI for ALL: Global Impact Challenge at the Summit, held at his residence earlier on Thursday.\n\n\n\nAt least two of these startups are expected to launch their large language models (LLMs) at the summit, Abhishek Singh, CEO of the IndiaAI Mission, had confirmed to AIM last month.\n\n\n\nCalling the startups and their founders the \u201cco-architects of India\u2019s future,\u201d PM Modi said the country has the capacity for both innovation and large-scale implementation. He urged the founders to present a unique AI model to the world that reflected the spirit of \u201cMade in India, Made for the World.\u201d\n\n\n\n\u201cThe emphasis was on AI\u2019s usability. The PM spoke at length about how AI models should be designed with actual impact in mind,\u201d said Abhishek Upperwal, CEO of Soket AI.&nbsp;\n\n\n\nHe said the PM highlighted the need to put AI to practical use and to gauge its impact in terms of how it benefits people. Upperwal also said that the India AI Impact Summit would be key for the technology sector.&nbsp;\n\n\n\nAI startups, including Avataar, BharatGen, Fractal, GAN, Genloop, Gnani, Intellihealth, Sarvam, Shodh AI, Soket AI, and Tech Mahindra, participated in the discussion. The startups were working across diverse fields such as e-commerce, marketing, engineering simulations, material research, healthcare, medical research and more.\n\n\n\nUpperwal said Soket AI is building the model in two phases. The first phase is the math and code model, and the second focuses on defence and how opportunities are being built around leveraging the model for defence purposes.&nbsp;\n\n\n\nHe said PM Modi stressed the need for ethical, unbiased, transparent Indian AI models rooted in data privacy. \u201cThe Prime Minister urged startups to pursue global leadership with affordable, inclusive AI and frugal innovation, and emphasised that Indian AI models should be unique, support regional languages, and promote indigenous content,\u201d Upperwal said.&nbsp;\n\n\n\nThe founders noted that the gravity of artificial intelligence innovation and deployment is beginning to shift towards India. They believed that India now provides a strong and conducive environment for AI development, firmly establishing the country on the global AI stage.\n\n\n\n\nThe post PM Modi Meets IndiaAI Mission Startups, Calls Them \u2018Co-Architects of India\u2019s Future\u2019 appeared first on Analytics India Magazine.",
      "url": "https://analyticsindiamag.com/ai-news-updates/pm-modi-meets-indiaai-mission-startups-calls-them-co-architects-of-indias-future/",
      "author": "Pallavi Chakravorty",
      "published": "2026-01-08T16:10:13",
      "source": "Analytics India Magazine",
      "source_type": "rss",
      "tags": [
        "AI News"
      ],
      "summary": "PM Modi met with 12 Indian AI startups ahead of the India AI Impact Summit, with at least two expected to launch LLMs at the event. Modi emphasized AI for societal impact and called founders 'co-architects of India's future.'",
      "importance_score": 58.0,
      "reasoning": "National AI initiative with LLM launches signals India's growing AI ambitions. Government support could accelerate regional AI development.",
      "themes": [
        "National AI Policy",
        "India",
        "LLMs",
        "Startups"
      ],
      "continuation": null
    },
    {
      "id": "bb97b5f7817a",
      "title": "Google Is Adding an \u2018AI Inbox\u2019 to Gmail That Summarizes Emails",
      "content": "New Gmail features, powered by the Gemini model, are part of Google\u2019s continued push for users to incorporate AI into their daily life and conversations.",
      "url": "https://www.wired.com/story/google-ai-inbox-gmail/",
      "author": "Reece Rogers",
      "published": "2026-01-08T13:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Gear",
        "Gear / Gear News and Events",
        "Gmail",
        "email",
        "Google",
        "Google Gemini",
        "artificial intelligence",
        "Reply All"
      ],
      "summary": "Google is adding an AI-powered inbox feature to Gmail that summarizes emails using the Gemini model. The feature represents Google's continued push to embed AI into everyday productivity tools.",
      "importance_score": 58.0,
      "reasoning": "Major product integration from leading AI company, but represents incremental feature addition rather than breakthrough capability.",
      "themes": [
        "Product Launch",
        "Google",
        "Productivity AI",
        "Gemini"
      ],
      "continuation": null
    },
    {
      "id": "0096fb304ba2",
      "title": "Artificial Analysis: Independent LLM Evals as a Service \u2014 with George Cameron and Micah-Hill Smith",
      "content": "Happy New Year! You may have noticed that in 2025 we had moved toward YouTube as our primary podcasting platform. As we&#8217;ll explain in the next State of Latent Space post, we&#8217;ll be doubling down on Substack again and improving the experience for the over 100,000 of you who look out for our emails and website updates!We first mentioned Artificial Analysis in 2024, when it was still a side project in a Sydney basement. They then were one of the few Nat Friedman and Daniel Gross&#8217; AIGrant companies to raise a full seed round from them and have now become the independent gold standard for AI benchmarking&#8212;trusted by developers, enterprises, and every major lab to navigate the exploding landscape of models, providers, and capabilities.We have chatted with both Clementine Fourrier of HuggingFace&#8217;s OpenLLM Leaderboard and (the freshly valued at $1.7B) Anastasios Angelopoulos of LMArena on their approaches to LLM evals and trendspotting, but Artificial Analysis have staked out an enduring and important place in the toolkit of the modern AI Engineer by doing the best job of independently running the most comprehensive set of evals across the widest range of open and closed models, and charting their progress for broad industry analyst use.George Cameron and Micah-Hill Smith have spent two years building Artificial Analysis into the platform that answers the questions no one else will: Which model is actually best for your use case? What are the real speed-cost trade-offs? And how open is &#8220;open&#8221; really?We discuss:The origin story: built as a side project in 2023 while Micah was building a legal AI assistant, launched publicly in January 2024, and went viral after Swyx&#8217;s retweetWhy they run evals themselves: labs prompt models differently, cherry-pick chain-of-thought examples (Google Gemini 1.0 Ultra used 32-shot prompts to beat GPT-4 on MMLU), and self-report inflated numbersThe mystery shopper policy: they register accounts not on their own domain and run intelligence + performance benchmarks incognito to prevent labs from serving different models on private endpointsHow they make money: enterprise benchmarking insights subscription (standardized reports on model deployment, serverless vs. managed vs. leasing chips) and private custom benchmarking for AI companies (no one pays to be on the public leaderboard)The Intelligence Index (V3): synthesizes 10 eval datasets (MMLU, GPQA, agentic benchmarks, long-context reasoning) into a single score, with 95% confidence intervals via repeated runsOmissions Index (hallucination rate): scores models from -100 to +100 (penalizing incorrect answers, rewarding \\&#8221;I don&#8217;t know\\&#8221;), and Claude models lead with the lowest hallucination rates despite not always being the smartestGDP Val AA: their version of OpenAI&#8217;s GDP-bench (44 white-collar tasks with spreadsheets, PDFs, PowerPoints), run through their Stirrup agent harness (up to 100 turns, code execution, web search, file system), graded by Gemini 3 Pro as an LLM judge (tested extensively, no self-preference bias)The Openness Index: scores models 0-18 on transparency of pre-training data, post-training data, methodology, training code, and licensing (AI2 OLMo 2 leads, followed by Nous Hermes and NVIDIA Nemotron)The smiling curve of AI costs: GPT-4-level intelligence is 100-1000x cheaper than at launch (thanks to smaller models like Amazon Nova), but frontier reasoning models in agentic workflows cost more than ever (sparsity, long context, multi-turn agents)Why sparsity might go way lower than 5%: GPT-4.5 is ~5% active, Gemini models might be ~3%, and Omissions Index accuracy correlates with total parameters (not active), suggesting massive sparse models are the futureToken efficiency vs. turn efficiency: GPT-5 costs more per token but solves Tau-bench in fewer turns (cheaper overall), and models are getting better at using more tokens only when needed (5.1 Codex has tighter token distributions)V4 of the Intelligence Index coming soon: adding GDP Val AA, Critical Point, hallucination rate, and dropping some saturated benchmarks (human-eval-style coding is now trivial for small models)Links to Artificial AnalysisWebsite: https://artificialanalysis.aiGeorge Cameron on X: https://x.com/georgecameronMicah-Hill Smith on X: https://x.com/micahhsmithFull Episode on YouTubeTimestamps00:00 Introduction: Full Circle Moment and Artificial Analysis Origins01:19 Business Model: Independence and Revenue Streams04:33 Origin Story: From Legal AI to Benchmarking Need16:22 AI Grant and Moving to San Francisco19:21 Intelligence Index Evolution: From V1 to V311:47 Benchmarking Challenges: Variance, Contamination, and Methodology13:52 Mystery Shopper Policy and Maintaining Independence28:01 New Benchmarks: Omissions Index for Hallucination Detection33:36 Critical Point: Hard Physics Problems and Research-Level Reasoning23:01 GDP Val AA: Agentic Benchmark for Real Work Tasks50:19 Stirrup Agent Harness: Open Source Agentic Framework52:43 Openness Index: Measuring Model Transparency Beyond Licenses58:25 The Smiling Curve: Cost Falling While Spend Rising1:02:32 Hardware Efficiency: Blackwell Gains and Sparsity Limits1:06:23 Reasoning Models and Token Efficiency: The Spectrum Emerges1:11:00 Multimodal Benchmarking: Image, Video, and Speech Arenas1:15:05 Looking Ahead: Intelligence Index V4 and Future Directions1:16:50 Closing: The Insatiable Demand for IntelligenceTranscriptMicah [00:00:06]: This is kind of a full circle moment for us in a way, because the first time artificial analysis got mentioned on a podcast was you and Alessio on Latent Space. Amazing.swyx [00:00:17]: Which was January 2024. I don&#8217;t even remember doing that, but yeah, it was very influential to me. Yeah, I&#8217;m looking at AI News for Jan 17, or Jan 16, 2024. I said, this gem of a models and host comparison site was just launched. And then I put in a few screenshots, and I said, it&#8217;s an independent third party. It clearly outlines the quality versus throughput trade-off, and it breaks out by model and hosting provider. I did give you shit for missing fireworks, and how do you have a model benchmarking thing without fireworks? But you had together, you had perplexity, and I think we just started chatting there. Welcome, George and Micah, to Latent Space. I&#8217;ve been following your progress. Congrats on... It&#8217;s been an amazing year. You guys have really come together to be the presumptive new gardener of AI, right? Which is something that...George [00:01:09]: Yeah, but you can&#8217;t pay us for better results.swyx [00:01:12]: Yes, exactly.George [00:01:13]: Very important.Micah [00:01:14]: Start off with a spicy take.swyx [00:01:18]: Okay, how do I pay you?Micah [00:01:20]: Let&#8217;s get right into that.swyx [00:01:21]: How do you make money?Micah [00:01:24]: Well, very happy to talk about that. So it&#8217;s been a big journey the last couple of years. Artificial analysis is going to be two years old in January 2026. Which is pretty soon now. We first run the website for free, obviously, and give away a ton of data to help developers and companies navigate AI and make decisions about models, providers, technologies across the AI stack for building stuff. We&#8217;re very committed to doing that and tend to keep doing that. We have, along the way, built a business that is working out pretty sustainably. We&#8217;ve got just over 20 people now and two main customer groups. So we want to be... We want to be who enterprise look to for data and insights on AI, so we want to help them with their decisions about models and technologies for building stuff. And then on the other side, we do private benchmarking for companies throughout the AI stack who build AI stuff. So no one pays to be on the website. We&#8217;ve been very clear about that from the very start because there&#8217;s no use doing what we do unless it&#8217;s independent AI benchmarking. Yeah. But turns out a bunch of our stuff can be pretty useful to companies building AI stuff.swyx [00:02:38]: And is it like, I am a Fortune 500, I need advisors on objective analysis, and I call you guys and you pull up a custom report for me, you come into my office and give me a workshop? What kind of engagement is that?George [00:02:53]: So we have a benchmarking and insight subscription, which looks like standardized reports that cover key topics or key challenges enterprises face when looking to understand AI and choose between all the technologies. And so, for instance, one of the report is a model deployment report, how to think about choosing between serverless inference, managed deployment solutions, or leasing chips. And running inference yourself is an example kind of decision that big enterprises face, and it&#8217;s hard to reason through, like this AI stuff is really new to everybody. And so we try and help with our reports and insight subscription. Companies navigate that. We also do custom private benchmarking. And so that&#8217;s very different from the public benchmarking that we publicize, and there&#8217;s no commercial model around that. For private benchmarking, we&#8217;ll at times create benchmarks, run benchmarks to specs that enterprises want. And we&#8217;ll also do that sometimes for AI companies who have built things, and we help them understand what they&#8217;ve built with private benchmarking. Yeah. So that&#8217;s a piece mainly that we&#8217;ve developed through trying to support everybody publicly with our public benchmarks. Yeah.swyx [00:04:09]: Let&#8217;s talk about TechStack behind that. But okay, I&#8217;m going to rewind all the way to when you guys started this project. You were all the way in Sydney? Yeah. Well, Sydney, Australia for me.Micah [00:04:19]: George was an SF, but he&#8217;s Australian, but he moved here already. Yeah.swyx [00:04:22]: And I remember I had the Zoom call with you. What was the impetus for starting artificial analysis in the first place? You know, you started with public benchmarks. And so let&#8217;s start there. We&#8217;ll go to the private benchmark. Yeah.George [00:04:33]: Why don&#8217;t we even go back a little bit to like why we, you know, thought that it was needed? Yeah.Micah [00:04:40]: The story kind of begins like in 2022, 2023, like both George and I have been into AI stuff for quite a while. In 2023 specifically, I was trying to build a legal AI research assistant. So it actually worked pretty well for its era, I would say. Yeah. Yeah. So I was finding that the more you go into building something using LLMs, the more each bit of what you&#8217;re doing ends up being a benchmarking problem. So had like this multistage algorithm thing, trying to figure out what the minimum viable model for each bit was, trying to optimize every bit of it as you build that out, right? Like you&#8217;re trying to think about accuracy, a bunch of other metrics and performance and cost. And mostly just no one was doing anything to independently evaluate all the models. And certainly not to look at the trade-offs for speed and cost. So we basically set out just to build a thing that developers could look at to see the trade-offs between all of those things measured independently across all the models and providers. Honestly, it was probably meant to be a side project when we first started doing it.swyx [00:05:49]: Like we didn&#8217;t like get together and say like, Hey, like we&#8217;re going to stop working on all this stuff. I&#8217;m like, this is going to be our main thing. When I first called you, I think you hadn&#8217;t decided on starting a company yet.Micah [00:05:58]: That&#8217;s actually true. I don&#8217;t even think we&#8217;d pause like, like George had an acquittance job. I didn&#8217;t quit working on my legal AI thing. Like it was genuinely a side project.George [00:06:05]: We built it because we needed it as people building in the space and thought, Oh, other people might find it useful too. So we&#8217;ll buy domain and link it to the Vercel deployment that we had and tweet about it. And, but very quickly it started getting attention. Thank you, Swyx for, I think doing an initial retweet and spotlighting it there. This project that we released. And then very quickly though, it was useful to others, but very quickly it became more useful as the number of models released accelerated. We had Mixtrel 8x7B and it was a key. That&#8217;s a fun one. Yeah. Like a open source model that really changed the landscape and opened up people&#8217;s eyes to other serverless inference providers and thinking about speed, thinking about cost. And so that was a key. And so it became more useful quite quickly. Yeah.swyx [00:07:02]: What I love talking to people like you who sit across the ecosystem is, well, I have theories about what people want, but you have data and that&#8217;s obviously more relevant. But I want to stay on the origin story a little bit more. When you started out, I would say, I think the status quo at the time was every paper would come out and they would report their numbers versus competitor numbers. And that&#8217;s basically it. And I remember I did the legwork. I think everyone has some knowledge. I think there&#8217;s some version of Excel sheet or a Google sheet where you just like copy and paste the numbers from every paper and just post it up there. And then sometimes they don&#8217;t line up because they&#8217;re independently run. And so your numbers are going to look better than... Your reproductions of other people&#8217;s numbers are going to look worse because you don&#8217;t hold their models correctly or whatever the excuse is. I think then Stanford Helm, Percy Liang&#8217;s project would also have some of these numbers. And I don&#8217;t know if there&#8217;s any other source that you can cite. The way that if I were to start artificial analysis at the same time you guys started, I would have used the Luther AI&#8217;s eval framework harness. Yup.Micah [00:08:06]: Yup. That was some cool stuff. At the end of the day, running these evals, it&#8217;s like if it&#8217;s a simple Q&amp;A eval, all you&#8217;re doing is asking a list of questions and checking if the answers are right, which shouldn&#8217;t be that crazy. But it turns out there are an enormous number of things that you&#8217;ve got control for. And I mean, back when we started the website. Yeah. Yeah. Like one of the reasons why we realized that we had to run the evals ourselves and couldn&#8217;t just take rules from the labs was just that they would all prompt the models differently. And when you&#8217;re competing over a few points, then you can pretty easily get- You can put the answer into the model. Yeah. That in the extreme. And like you get crazy cases like back when I&#8217;m Googled a Gemini 1.0 Ultra and needed a number that would say it was better than GPT-4 and like constructed, I think never published like chain of thought examples. 32 of them in every topic in MLU to run it, to get the score, like there are so many things that you- They never shipped Ultra, right? That&#8217;s the one that never made it up. Not widely. Yeah. Yeah. Yeah. I mean, I&#8217;m sure it existed, but yeah. So we were pretty sure that we needed to run them ourselves and just run them in the same way across all the models. Yeah. And we were, we also did certain from the start that you couldn&#8217;t look at those in isolation. You needed to look at them alongside the cost and performance stuff. Yeah.swyx [00:09:24]: Okay. A couple of technical questions. I mean, so obviously I also thought about this and I didn&#8217;t do it because of cost. Yep. Did you not worry about costs? Were you funded already? Clearly not, but you know. No. Well, we definitely weren&#8217;t at the start.Micah [00:09:36]: So like, I mean, we&#8217;re paying for it personally at the start. There&#8217;s a lot of money. Well, the numbers weren&#8217;t nearly as bad a couple of years ago. So we certainly incurred some costs, but we were probably in the order of like hundreds of dollars of spend across all the benchmarking that we were doing. Yeah. So nothing. Yeah. It was like kind of fine. Yeah. Yeah. These days that&#8217;s gone up an enormous amount for a bunch of reasons that we can talk about. But yeah, it wasn&#8217;t that bad because you can also remember that like the number of models we were dealing with was hardly any and the complexity of the stuff that we wanted to do to evaluate them was a lot less. Like we were just asking some Q&amp;A type questions and then one specific thing was for a lot of evals initially, we were just like sampling an answer. You know, like, what&#8217;s the answer for this? Like, we didn&#8217;t want to go into the answer directly without letting the models think. We weren&#8217;t even doing chain of thought stuff initially. And that was the most useful way to get some results initially. Yeah.swyx [00:10:33]: And so for people who haven&#8217;t done this work, literally parsing the responses is a whole thing, right? Like because sometimes the models, the models can answer any way they feel fit and sometimes they actually do have the right answer, but they just returned the wrong format and they will get a zero for that unless you work it into your parser. And that involves more work. And so, I mean, but there&#8217;s an open question whether you should give it points for not following your instructions on the format.Micah [00:11:00]: It depends what you&#8217;re looking at, right? Because you can, if you&#8217;re trying to see whether or not it can solve a particular type of reasoning problem, and you don&#8217;t want to test it on its ability to do answer formatting at the same time, then you might want to use an LLM as answer extractor approach to make sure that you get the answer out no matter how unanswered. But these days, it&#8217;s mostly less of a problem. Like, if you instruct a model and give it examples of what the answers should look like, it can get the answers in your format, and then you can do, like, a simple regex.swyx [00:11:28]: Yeah, yeah. And then there&#8217;s other questions around, I guess, sometimes if you have a multiple choice question, sometimes there&#8217;s a bias towards the first answer, so you have to randomize the responses. All these nuances, like, once you dig into benchmarks, you&#8217;re like, I don&#8217;t know how anyone believes the numbers on all these things. It&#8217;s so dark magic.Micah [00:11:47]: You&#8217;ve also got, like&#8230; You&#8217;ve got, like, the different degrees of variance in different benchmarks, right? Yeah. So, if you run four-question multi-choice on a modern reasoning model at the temperatures suggested by the labs for their own models, the variance that you can see on a four-question multi-choice eval is pretty enormous if you only do a single run of it and it has a small number of questions, especially. So, like, one of the things that we do is run an enormous number of all of our evals when we&#8217;re developing new ones and doing upgrades to our intelligence index to bring in new things. Yeah. So, that we can dial in the right number of repeats so that we can get to the 95% confidence intervals that we&#8217;re comfortable with so that when we pull that together, we can be confident in intelligence index to at least as tight as, like, a plus or minus one at a 95% confidence. Yeah.swyx [00:12:32]: And, again, that just adds a straight multiple to the cost. Oh, yeah. Yeah, yeah.George [00:12:37]: So, that&#8217;s one of many reasons that cost has gone up a lot more than linearly over the last couple of years. We report a cost to run the artificial analysis. We report a cost to run the artificial analysis intelligence index on our website, and currently that&#8217;s assuming one repeat in terms of how we report it because we want to reflect a bit about the weighting of the index. But our cost is actually a lot higher than what we report there because of the repeats.swyx [00:13:03]: Yeah, yeah, yeah. And probably this is true, but just checking, you don&#8217;t have any special deals with the labs. They don&#8217;t discount it. You just pay out of pocket or out of your sort of customer funds. Oh, there is a mix. So, the issue is that sometimes they may give you a special end point, which is&#8230; Ah, 100%.Micah [00:13:21]: Yeah, yeah, yeah. Exactly. So, we laser focus, like, on everything we do on having the best independent metrics and making sure that no one can manipulate them in any way. There are quite a lot of processes we&#8217;ve developed over the last couple of years to make that true for, like, the one you bring up, like, right here of the fact that if we&#8217;re working with a lab, if they&#8217;re giving us a private endpoint to evaluate a model, that it is totally possible. That what&#8217;s sitting behind that black box is not the same as they serve on a public endpoint. We&#8217;re very aware of that. We have what we call a mystery shopper policy. And so, and we&#8217;re totally transparent with all the labs we work with about this, that we will register accounts not on our own domain and run both intelligence evals and performance benchmarks&#8230; Yeah, that&#8217;s the job. &#8230;without them being able to identify it. And no one&#8217;s ever had a problem with that. Because, like, a thing that turns out to actually be quite a good&#8230; &#8230;good factor in the industry is that they all want to believe that none of their competitors could manipulate what we&#8217;re doing either.swyx [00:14:23]: That&#8217;s true. I never thought about that. I&#8217;ve been in the database data industry prior, and there&#8217;s a lot of shenanigans around benchmarking, right? So I&#8217;m just kind of going through the mental laundry list. Did I miss anything else in this category of shenanigans? Oh, potential shenanigans.Micah [00:14:36]: I mean, okay, the biggest one, like, that I&#8217;ll bring up, like, is more of a conceptual one, actually, than, like, direct shenanigans. It&#8217;s that the things that get measured become things that get targeted by labs that they&#8217;re trying to build, right? Exactly. So that doesn&#8217;t mean anything that we should really call shenanigans. Like, I&#8217;m not talking about training on test set. But if you know that you&#8217;re going to be great at another particular thing, if you&#8217;re a researcher, there are a whole bunch of things that you can do to try to get better at that thing that preferably are going to be helpful for a wide range of how actual users want to use the thing that you&#8217;re building. But will not necessarily work. Will not necessarily do that. So, for instance, the models are exceptional now at answering competition maths problems. There is some relevance of that type of reasoning, that type of work, to, like, how we might use modern coding agents and stuff. But it&#8217;s clearly not one for one. So the thing that we have to be aware of is that once an eval becomes the thing that everyone&#8217;s looking at, scores can get better on it without there being a reflection of overall generalized intelligence of these models. Getting better. That has been true for the last couple of years. It&#8217;ll be true for the next couple of years. There&#8217;s no silver bullet to defeat that other than building new stuff to stay relevant and measure the capabilities that matter most to real users. Yeah.swyx [00:15:58]: And we&#8217;ll cover some of the new stuff that you guys are building as well, which is cool. Like, you used to just run other people&#8217;s evals, but now you&#8217;re coming up with your own. And I think, obviously, that is a necessary path once you&#8217;re at the frontier. You&#8217;ve exhausted all the existing evals. I think the next point in history that I have for you is AI Grant that you guys decided to join and move here. What was it like? I think you were in, like, batch two? Batch four. Batch four. Okay.Micah [00:16:26]: I mean, it was great. Nat and Daniel are obviously great. And it&#8217;s a really cool group of companies that we were in AI Grant alongside. It was really great to get Nat and Daniel on board. Obviously, they&#8217;ve done a whole lot of great work in the space with a lot of leading companies and were extremely aligned. With the mission of what we were trying to do. Like, we&#8217;re not quite typical of, like, a lot of the other AI startups that they&#8217;ve invested in.swyx [00:16:53]: And they were very much here for the mission of what we want to do. Did they say any advice that really affected you in some way or, like, were one of the events very impactful? That&#8217;s an interesting question.Micah [00:17:03]: I mean, I remember fondly a bunch of the speakers who came and did fireside chats at AI Grant.swyx [00:17:09]: Which is also, like, a crazy list. Yeah.George [00:17:11]: Oh, totally. Yeah, yeah, yeah. There was something about, you know, speaking to Nat and Daniel about the challenges of working through a startup and just working through the questions that don&#8217;t have, like, clear answers and how to work through those kind of methodically and just, like, work through the hard decisions. And they&#8217;ve been great mentors to us as we&#8217;ve built artificial analysis. Another benefit for us was that other companies in the batch and other companies in AI Grant are pushing the capabilities. Yeah. And I think that&#8217;s a big part of what AI can do at this time. And so being in contact with them, making sure that artificial analysis is useful to them has been fantastic for supporting us in working out how should we build out artificial analysis to continue to being useful to those, like, you know, building on AI.swyx [00:17:59]: I think to some extent, I&#8217;m mixed opinion on that one because to some extent, your target audience is not people in AI Grants who are obviously at the frontier. Yeah. Do you disagree?Micah [00:18:09]: To some extent. To some extent. But then, so a lot of what the AI Grant companies are doing is taking capabilities coming out of the labs and trying to push the limits of what they can do across the entire stack for building great applications, which actually makes some of them pretty archetypical power users of artificial analysis. Some of the people with the strongest opinions about what we&#8217;re doing well and what we&#8217;re not doing well and what they want to see next from us. Yeah. Yeah. Because when you&#8217;re building any kind of AI application now, chances are you&#8217;re using a whole bunch of different models. You&#8217;re maybe switching reasonably frequently for different models and different parts of your application to optimize what you&#8217;re able to do with them at an accuracy level and to get better speed and cost characteristics. So for many of them, no, they&#8217;re like not commercial customers of ours, like we don&#8217;t charge for all our data on the website. Yeah. They are absolutely some of our power users.swyx [00:19:07]: So let&#8217;s talk about just the evals as well. So you start out from the general like MMU and GPQA stuff. What&#8217;s next? How do you sort of build up to the overall index? What was in V1 and how did you evolve it? Okay.Micah [00:19:22]: So first, just like background, like we&#8217;re talking about the artificial analysis intelligence index, which is our synthesis metric that we pulled together currently from 10 different eval data sets to give what? We&#8217;re pretty much the same as that. Pretty confident is the best single number to look at for how smart the models are. Obviously, it doesn&#8217;t tell the whole story. That&#8217;s why we published the whole website of all the charts to dive into every part of it and look at the trade-offs. But best single number. So right now, it&#8217;s got a bunch of Q&amp;A type data sets that have been very important to the industry, like a couple that you just mentioned. It&#8217;s also got a couple of agentic data sets. It&#8217;s got our own long context reasoning data set and some other use case focused stuff. As time goes on. The things that we&#8217;re most interested in that are going to be important to the capabilities that are becoming more important for AI, what developers are caring about, are going to be first around agentic capabilities. So surprise, surprise. We&#8217;re all loving our coding agents and how the model is going to perform like that and then do similar things for different types of work are really important to us. The linking to use cases to economically valuable use cases are extremely important to us. And then we&#8217;ve got some of the. Yeah. These things that the models still struggle with, like working really well over long contexts that are not going to go away as specific capabilities and use cases that we need to keep evaluating.swyx [00:20:46]: But I guess one thing I was driving was like the V1 versus the V2 and how bad it was over time.Micah [00:20:53]: Like how we&#8217;ve changed the index to where we are.swyx [00:20:55]: And I think that reflects on the change in the industry. Right. So that&#8217;s a nice way to tell that story.Micah [00:21:00]: Well, V1 would be completely saturated right now. Almost every model coming out because doing things like writing the Python functions and human evil is now pretty trivial. It&#8217;s easy to forget, actually, I think how much progress has been made in the last two years. Like we obviously play the game constantly of like the today&#8217;s version versus last week&#8217;s version and the week before and all of the small changes in the horse race between the current frontier and who has the best like smaller than 10B model like right now this week. Right. And that&#8217;s very important to a lot of developers and people and especially in this particular city of San Francisco. But when you zoom out a couple of years ago, literally most of what we were doing to evaluate the models then would all be 100% solved by even pretty small models today. And that&#8217;s been one of the key things, by the way, that&#8217;s driven down the cost of intelligence at every tier of intelligence. We can talk about more in a bit. So V1, V2, V3, we made things harder. We covered a wider range of use cases. And we tried to get closer to things developers care about as opposed to like just the Q&amp;A type stuff that MMLU and GPQA represented. Yeah.swyx [00:22:12]: I don&#8217;t know if you have anything to add there. Or we could just go right into showing people the benchmark and like looking around and asking questions about it. Yeah.Micah [00:22:21]: Let&#8217;s do it. Okay. This would be a pretty good way to chat about a few of the new things we&#8217;ve launched recently. Yeah.George [00:22:26]: And I think a little bit about the direction that we want to take it. And we want to push benchmarks. Currently, the intelligence index and evals focus a lot on kind of raw intelligence. But we kind of want to diversify how we think about intelligence. And we can talk about it. But kind of new evals that we&#8217;ve kind of built and partnered on focus on topics like hallucination. And we&#8217;ve got a lot of topics that I think are not covered by the current eval set that should be. And so we want to bring that forth. But before we get into that.swyx [00:23:01]: And so for listeners, just as a timestamp, right now, number one is Gemini 3 Pro High. Then followed by Cloud Opus at 70. Just 5.1 high. You don&#8217;t have 5.2 yet. And Kimi K2 Thinking. Wow. Still hanging in there. So those are the top four. That will date this podcast quickly. Yeah. Yeah. I mean, I love it. I love it. No, no. 100%. Look back this time next year and go, how cute. Yep.George [00:23:25]: Totally. A quick view of that is, okay, there&#8217;s a lot. I love it. I love this chart. Yeah.Micah [00:23:30]: This is such a favorite, right? Yeah. And almost every talk that George or I give at conferences and stuff, we always put this one up first to just talk about situating where we are in this moment in history. This, I think, is the visual version of what I was saying before about the zooming out and remembering how much progress there&#8217;s been. If we go back to just over a year ago, before 01, before Cloud Sonnet 3.5, we didn&#8217;t have reasoning models or coding agents as a thing. And the game was very, very different. If we go back even a little bit before then, we&#8217;re in the era where, when you look at this chart, open AI was untouchable for well over a year. And, I mean, you would remember that time period well of there being very open questions about whether or not AI was going to be competitive, like full stop, whether or not open AI would just run away with it, whether we would have a few frontier labs and no one else would really be able to do anything other than consume their APIs. I am quite happy overall that the world that we have ended up in is one where... Multi-model. Absolutely. And strictly more competitive every quarter over the last few years. Yeah. This year has been insane. Yeah.George [00:24:42]: You can see it. This chart with everything added is hard to read currently. There&#8217;s so many dots on it, but I think it reflects a little bit what we felt, like how crazy it&#8217;s been.swyx [00:24:54]: Why 14 as the default? Is that a manual choice? Because you&#8217;ve got service now in there that are less traditional names. Yeah.George [00:25:01]: It&#8217;s models that we&#8217;re kind of highlighting by default in our charts, in our intelligence index. Okay.swyx [00:25:07]: You just have a manually curated list of stuff.George [00:25:10]: Yeah, that&#8217;s right. But something that I actually don&#8217;t think every artificial analysis user knows is that you can customize our charts and choose what models are highlighted. Yeah. And so if we take off a few names, it gets a little easier to read.swyx [00:25:25]: Yeah, yeah. A little easier to read. Totally. Yeah. But I love that you can see the all one jump. Look at that. September 2024. And the DeepSeek jump. Yeah.George [00:25:34]: Which got close to OpenAI&#8217;s leadership. They were so close. I think, yeah, we remember that moment. Around this time last year, actually.Micah [00:25:44]: Yeah, yeah, yeah. I agree. Yeah, well, a couple of weeks. It was Boxing Day in New Zealand when DeepSeek v3 came out. And we&#8217;d been tracking DeepSeek and a bunch of the other global players that were less known over the second half of 2024 and had run evals on the earlier ones and stuff. I very distinctly remember Boxing Day in New Zealand, because I was with family for Christmas and stuff, running the evals and getting back result by result on DeepSeek v3. So this was the first of their v3 architecture, the 671b MOE.Micah [00:26:19]: And we were very, very impressed. That was the moment where we were sure that DeepSeek was no longer just one of many players, but had jumped up to be a thing. The world really noticed when they followed that up with the RL working on top of v3 and R1 succeeding a few weeks later. But the groundwork for that absolutely was laid with just extremely strong base model, completely open weights that we had as the best open weights model. So, yeah, that&#8217;s the thing that you really see in the game. But I think that we got a lot of good feedback on Boxing Day. us on Boxing Day last year.George [00:26:48]: Boxing Day is the day after Christmas for those not familiar.George [00:26:54]: I&#8217;m from Singapore.swyx [00:26:55]: A lot of us remember Boxing Day for a different reason, for the tsunami that happened. Oh, of course. Yeah, but that was a long time ago. So yeah. So this is the rough pitch of AAQI. Is it A-A-Q-I or A-A-I-I? I-I. Okay. Good memory, though.Micah [00:27:11]: I don&#8217;t know. I&#8217;m not used to it. Once upon a time, we did call it Quality Index, and we would talk about quality, performance, and price, but we changed it to intelligence.George [00:27:20]: There&#8217;s been a few naming changes. We added hardware benchmarking to the site, and so benchmarks at a kind of system level. And so then we changed our throughput metric to, we now call it output speed, and thenswyx [00:27:32]: throughput makes sense at a system level, so we took that name. Take me through more charts. What should people know? Obviously, the way you look at the site is probably different than how a beginner might look at it.Micah [00:27:42]: Yeah, that&#8217;s fair. There&#8217;s a lot of fun stuff to dive into. Maybe so we can hit past all the, like, we have lots and lots of emails and stuff. The interesting ones to talk about today that would be great to bring up are a few of our recent things, I think, that probably not many people will be familiar with yet. So first one of those is our omniscience index. So this one is a little bit different to most of the intelligence evils that we&#8217;ve run. We built it specifically to look at the embedded knowledge in the models and to test hallucination by looking at when the model doesn&#8217;t know the answer, so not able to get it correct, what&#8217;s its probability of saying, I don&#8217;t know, or giving an incorrect answer. So the metric that we use for omniscience goes from negative 100 to positive 100. Because we&#8217;re simply taking off a point if you give an incorrect answer to the question. We&#8217;re pretty convinced that this is an example of where it makes most sense to do that, because it&#8217;s strictly more helpful to say, I don&#8217;t know, instead of giving a wrong answer to factual knowledge question. And one of our goals is to shift the incentive that evils create for models and the labs creating them to get higher scores. And almost every evil across all of AI up until this point, it&#8217;s been graded by simple percentage correct as the main metric, the main thing that gets hyped. And so you should take a shot at everything. There&#8217;s no incentive to say, I don&#8217;t know. So we did that for this one here.swyx [00:29:22]: I think there&#8217;s a general field of calibration as well, like the confidence in your answer versus the rightness of the answer. Yeah, we completely agree. Yeah. Yeah.George [00:29:31]: On that. And one reason that we didn&#8217;t do that is because. Or put that into this index is that we think that the, the way to do that is not to ask the models how confident they are.swyx [00:29:43]: I don&#8217;t know. Maybe it might be though. You put it like a JSON field, say, say confidence and maybe it spits out something. Yeah. You know, we have done a few evils podcasts over the, over the years. And when we did one with Clementine of hugging face, who maintains the open source leaderboard, and this was one of her top requests, which is some kind of hallucination slash lack of confidence calibration thing. And so, Hey, this is one of them.Micah [00:30:05]: And I mean, like anything that we do, it&#8217;s not a perfect metric or the whole story of everything that you think about as hallucination. But yeah, it&#8217;s pretty useful and has some interesting results. Like one of the things that we saw in the hallucination rate is that anthropics Claude models at the, the, the very left-hand side here with the lowest hallucination rates out of the models that we&#8217;ve evaluated amnesty is on. That is an interesting fact. I think it probably correlates with a lot of the previously, not really measured vibes stuff that people like about some of the Claude models. Is the dataset public or what&#8217;s is it, is there a held out set? There&#8217;s a hell of a set for this one. So we, we have published a public test set, but we we&#8217;ve only published 10% of it. The reason is that for this one here specifically, it would be very, very easy to like have data contamination because it is just factual knowledge questions. We would. We&#8217;ll update it at a time to also prevent that, but with yeah, kept most of it held out so that we can keep it reliable for a long time. It leads us to a bunch of really cool things, including breakdown quite granularly by topic. And so we&#8217;ve got some of that disclosed on the website publicly right now, and there&#8217;s lots more coming in terms of our ability to break out very specific topics. Yeah.swyx [00:31:23]: I would be interested. Let&#8217;s, let&#8217;s dwell a little bit on this hallucination one. I noticed that Haiku hallucinates less than Sonnet hallucinates less than Opus. And yeah. Would that be the other way around in a normal capability environments? I don&#8217;t know. What&#8217;s, what do you make of that?George [00:31:37]: One interesting aspect is that we&#8217;ve found that there&#8217;s not really a, not a strong correlation between intelligence and hallucination, right? That&#8217;s to say that the smarter the models are in a general sense, isn&#8217;t correlated with their ability to, when they don&#8217;t know something, say that they don&#8217;t know. It&#8217;s interesting that Gemini three pro preview was a big leap over here. Gemini 2.5. Flash and, and, and 2.5 pro, but, and if I add pro quickly here.swyx [00:32:07]: I bet pro&#8217;s really good. Uh, actually no, I meant, I meant, uh, the GPT pros.George [00:32:12]: Oh yeah.swyx [00:32:13]: Cause GPT pros are rumored. We don&#8217;t know for a fact that it&#8217;s like eight runs and then with the LM judge on top. Yeah.George [00:32:20]: So we saw a big jump in, this is accuracy. So this is just percent that they get, uh, correct and Gemini three pro knew a lot more than the other models. And so big jump in accuracy. But relatively no change between the Google Gemini models, between releases. And the hallucination rate. Exactly. And so it&#8217;s likely due to just kind of different post-training recipe, between the, the Claude models. Yeah.Micah [00:32:45]: Um, there&#8217;s, there&#8217;s driven this. Yeah. You can, uh, you can partially blame us and how we define intelligence having until now not defined hallucination as a negative in the way that we think about intelligence.swyx [00:32:56]: And so that&#8217;s what we&#8217;re changing. Uh, I know many smart people who are confidently incorrect.George [00:33:02]: Uh, look, look at that. That, that, that is very humans. Very true. And there&#8217;s times and a place for that. I think our view is that hallucination rate makes sense in this context where it&#8217;s around knowledge, but in many cases, people want the models to hallucinate, to have a go. Often that&#8217;s the case in coding or when you&#8217;re trying to generate newer ideas. One eval that we added to artificial analysis is, is, is critical point and it&#8217;s really hard, uh, physics problems. Okay.swyx [00:33:32]: And is it sort of like a human eval type or something different or like a frontier math type?George [00:33:37]: It&#8217;s not dissimilar to frontier frontier math. So these are kind of research questions that kind of academics in the physics physics world would be able to answer, but models really struggled to answer. So the top score here is not 9%.swyx [00:33:51]: And when the people that, that created this like Minway and, and, and actually off via who was kind of behind sweep and what organization is this? Oh, is this, it&#8217;s Princeton.George [00:34:01]: Kind of range of academics from, from, uh, different academic institutions, really smart people. They talked about how they turn the models up in terms of the temperature as high temperature as they can, where they&#8217;re trying to explore kind of new ideas in physics as a, as a thought partner, just because they, they want the models to hallucinate. Um, yeah, sometimes it&#8217;s something new. Yeah, exactly.swyx [00:34:21]: Um, so not right in every situation, but, um, I think it makes sense, you know, to test hallucination in scenarios where it makes sense. Also, the obvious question is, uh, this is one of. Many that there is there, every lab has a system card that shows some kind of hallucination number, and you&#8217;ve chosen to not, uh, endorse that and you&#8217;ve made your own. And I think that&#8217;s a, that&#8217;s a choice. Um, totally in some sense, the rest of artificial analysis is public benchmarks that other people can independently rerun. You provide it as a service here. You have to fight the, well, who are we to, to like do this? And your, your answer is that we have a lot of customers and, you know, but like, I guess, how do you converge the individual?Micah [00:35:08]: I mean, I think, I think for hallucinations specifically, there are a bunch of different things that you might care about reasonably, and that you&#8217;d measure quite differently, like we&#8217;ve called this a amnesty and solutionation rate, not trying to declare the, like, it&#8217;s humanity&#8217;s last hallucination. You could, uh, you could have some interesting naming conventions and all this stuff. Um, the biggest picture answer to that. It&#8217;s something that I actually wanted to mention. Just as George was explaining, critical point as well is, so as we go forward, we are building evals internally. We&#8217;re partnering with academia and partnering with AI companies to build great evals. We have pretty strong views on, in various ways for different parts of the AI stack, where there are things that are not being measured well, or things that developers care about that should be measured more and better. And we intend to be doing that. We&#8217;re not obsessed necessarily with that. Everything we do, we have to do entirely within our own team. Critical point. As a cool example of where we were a launch partner for it, working with academia, we&#8217;ve got some partnerships coming up with a couple of leading companies. Those ones, obviously we have to be careful with on some of the independent stuff, but with the right disclosure, like we&#8217;re completely comfortable with that. A lot of the labs have released great data sets in the past that we&#8217;ve used to great success independently. And so it&#8217;s between all of those techniques, we&#8217;re going to be releasing more stuff in the future. Cool.swyx [00:36:26]: Let&#8217;s cover the last couple. And then we&#8217;ll, I want to talk about your trends analysis stuff, you know? Totally.Micah [00:36:31]: So that actually, I have one like little factoid on omniscience. If you go back up to accuracy on omniscience, an interesting thing about this accuracy metric is that it tracks more closely than anything else that we measure. The total parameter count of models makes a lot of sense intuitively, right? Because this is a knowledge eval. This is the pure knowledge metric. We&#8217;re not looking at the index and the hallucination rate stuff that we think is much more about how the models are trained. This is just what facts did they recall? And yeah, it tracks parameter count extremely closely. Okay.swyx [00:37:05]: What&#8217;s the rumored size of GPT-3 Pro? And to be clear, not confirmed for any official source, just rumors. But rumors do fly around. Rumors. I get, I hear all sorts of numbers. I don&#8217;t know what to trust.Micah [00:37:17]: So if you, if you draw the line on omniscience accuracy versus total parameters, we&#8217;ve got all the open ways models, you can squint and see that likely the leading frontier models right now are quite a lot bigger than the ones that we&#8217;re seeing right now. And the one trillion parameters that the open weights models cap out at, and the ones that we&#8217;re looking at here, there&#8217;s an interesting extra data point that Elon Musk revealed recently about XAI that for three trillion parameters for GROK 3 and 4, 6 trillion for GROK 5, but that&#8217;s not out yet. Take those together, have a look. You might reasonably form a view that there&#8217;s a pretty good chance that Gemini 3 Pro is bigger than that, that it could be in the 5 to 10 trillion parameters. To be clear, I have absolutely no idea, but just based on this chart, like that&#8217;s where you would, you would land if you have a look at it. Yeah.swyx [00:38:07]: And to some extent, I actually kind of discourage people from guessing too much because what does it really matter? Like as long as they can serve it as a sustainable cost, that&#8217;s about it. Like, yeah, totally.George [00:38:17]: They&#8217;ve also got different incentives in play compared to like open weights models who are thinking to supporting others in self-deployment for the labs who are doing inference at scale. It&#8217;s I think less about total parameters in many cases. When thinking about inference costs and more around number of active parameters. And so there&#8217;s a bit of an incentive towards larger sparser models. Agreed.Micah [00:38:38]: Understood. Yeah. Great. I mean, obviously if you&#8217;re a developer or company using these things, not exactly as you say, it doesn&#8217;t matter. You should be looking at all the different ways that we measure intelligence. You should be looking at cost to run index number and the different ways of thinking about token efficiency and cost efficiency based on the list prices, because that&#8217;s all it matters.swyx [00:38:56]: It&#8217;s not as good for the content creator rumor mill where I can say. Oh, GPT-4 is this small circle. Look at GPT-5 is this big circle. And then there used to be a thing for a while. Yeah.Micah [00:39:07]: But that is like on its own, actually a very interesting one, right? That is it just purely that chances are the last couple of years haven&#8217;t seen a dramatic scaling up in the total size of these models. And so there&#8217;s a lot of room to go up properly in total size of the models, especially with the upcoming hardware generations. Yes.swyx [00:39:29]: So, you know. Taking off my shitposting face for a minute. Yes. Yes. At the same time, I do feel like, you know, especially coming back from Europe, people do feel like Ilya is probably right that the paradigm is doesn&#8217;t have many more orders of magnitude to scale out more. And therefore we need to start exploring at least a different path. GDPVal, I think it&#8217;s like only like a month or so old. I was also very positive when it first came out. I actually talked to Tejo, who was the lead researcher on that. Oh, cool. And you have your own version.George [00:39:59]: It&#8217;s a fantastic. It&#8217;s a fantastic data set. Yeah.swyx [00:40:01]: And maybe it will recap for people who are still out of it. It&#8217;s like 44 tasks based on some kind of GDP cutoff that&#8217;s like meant to represent broad white collar work that is not just coding. Yeah.Micah [00:40:12]: Each of the tasks have a whole bunch of detailed instructions, some input files for a lot of them. It&#8217;s within the 44 is divided into like two hundred and twenty two to five, maybe subtasks that are the level of that we run through the agenda. And yeah, they&#8217;re really interesting. I will say that it doesn&#8217;t. It doesn&#8217;t necessarily capture like all the stuff that people do at work. No avail is perfect is always going to be more things to look at, largely because in order to make the tasks well enough to find that you can run them, they need to only have a handful of input files and very specific instructions for that task. And so I think the easiest way to think about them are that they&#8217;re like quite hard take home exam tasks that you might do in an interview process.swyx [00:40:56]: Yeah, for listeners, it is not no longer like a long prompt. It is like, well, here&#8217;s a zip file with like a spreadsheet or a PowerPoint deck or a PDF and go nuts and answer this question.George [00:41:06]: OpenAI released a great data set and they released a good paper which looks at performance across the different web chat bots on the data set. It&#8217;s a great paper, encourage people to read it. What we&#8217;ve done is taken that data set and turned it into an eval that can be run on any model. So we created a reference agentic harness that can run. Run the models on the data set, and then we developed evaluator approach to compare outputs. That&#8217;s kind of AI enabled, so it uses Gemini 3 Pro Preview to compare results, which we tested pretty comprehensively to ensure that it&#8217;s aligned to human preferences. One data point there is that even as an evaluator, Gemini 3 Pro, interestingly, doesn&#8217;t do actually that well. So that&#8217;s kind of a good example of what we&#8217;ve done in GDPVal AA.swyx [00:42:01]: Yeah, the thing that you have to watch out for with LLM judge is self-preference that models usually prefer their own output, and in this case, it was not. Totally.Micah [00:42:08]: I think the way that we&#8217;re thinking about the places where it makes sense to use an LLM as judge approach now, like quite different to some of the early LLM as judge stuff a couple of years ago, because some of that and MTV was a great project that was a good example of some of this a while ago was about judging conversations and like a lot of style type stuff. Here, we&#8217;ve got the task that the grader and grading model is doing is quite different to the task of taking the test. When you&#8217;re taking the test, you&#8217;ve got all of the agentic tools you&#8217;re working with, the code interpreter and web search, the file system to go through many, many turns to try to create the documents. Then on the other side, when we&#8217;re grading it, we&#8217;re running it through a pipeline to extract visual and text versions of the files and be able to provide that to Gemini, and we&#8217;re providing the criteria for the task and getting it to pick which one more effectively meets the criteria of the task. Yeah. So we&#8217;ve got the task out of two potential outcomes. It turns out that we proved that it&#8217;s just very, very good at getting that right, matched with human preference a lot of the time, because I think it&#8217;s got the raw intelligence, but it&#8217;s combined with the correct representation of the outputs, the fact that the outputs were created with an agentic task that is quite different to the way the grading model works, and we&#8217;re comparing it against criteria, not just kind of zero shot trying to ask the model to pick which one is better.swyx [00:43:26]: Got it. Why is this an ELO? And not a percentage, like GDP-VAL?George [00:43:31]: So the outputs look like documents, and there&#8217;s video outputs or audio outputs from some of the tasks. It has to make a video? Yeah, for some of the tasks. Some of the tasks.swyx [00:43:43]: What task is that?George [00:43:45]: I mean, it&#8217;s in the data set. Like be a YouTuber? It&#8217;s a marketing video.Micah [00:43:49]: Oh, wow. What? Like model has to go find clips on the internet and try to put it together. The models are not that good at doing that one, for now, to be clear. It&#8217;s pretty hard to do that with a code editor. I mean, the computer stuff doesn&#8217;t work quite well enough and so on and so on, but yeah.George [00:44:02]: And so there&#8217;s no kind of ground truth, necessarily, to compare against, to work out percentage correct. It&#8217;s hard to come up with correct or incorrect there. And so it&#8217;s on a relative basis. And so we use an ELO approach to compare outputs from each of the models between the task.swyx [00:44:23]: You know what you should do? You should pay a contractor, a human, to do the same task. And then give it an ELO and then so you have, you have human there. It&#8217;s just, I think what&#8217;s helpful about GDPVal, the OpenAI one, is that 50% is meant to be normal human and maybe Domain Expert is higher than that, but 50% was the bar for like, well, if you&#8217;ve crossed 50, you are superhuman. Yeah.Micah [00:44:47]: So we like, haven&#8217;t grounded this score in that exactly. I agree that it can be helpful, but we wanted to generalize this to a very large number. It&#8217;s one of the reasons that presenting it as ELO is quite helpful and allows us to add models and it&#8217;ll stay relevant for quite a long time. I also think it, it can be tricky looking at these exact tasks compared to the human performance, because the way that you would go about it as a human is quite different to how the models would go about it. Yeah.swyx [00:45:15]: I also liked that you included Lama 4 Maverick in there. Is that like just one last, like...Micah [00:45:20]: Well, no, no, no, no, no, no, it is the, it is the best model released by Meta. And... So it makes it into the homepage default set, still for now.George [00:45:31]: Other inclusion that&#8217;s quite interesting is we also ran it across the latest versions of the web chatbots. And so we have...swyx [00:45:39]: Oh, that&#8217;s right.George [00:45:40]: Oh, sorry.swyx [00:45:41]: I, yeah, I completely missed that. Okay.George [00:45:43]: No, not at all. So that, which has a checkered pattern. So that is their harness, not yours, is what you&#8217;re saying. Exactly. And what&#8217;s really interesting is that if you compare, for instance, Claude 4.5 Opus using the Claude web chatbot, it performs worse than the model in our agentic harness. And so in every case, the model performs better in our agentic harness than its web chatbot counterpart, the harness that they created.swyx [00:46:13]: Oh, my backwards explanation for that would be that, well, it&#8217;s meant for consumer use cases and here you&#8217;re pushing it for something.Micah [00:46:19]: The constraints are different and the amount of freedom that you can give the model is different. Also, you like have a cost goal. We let the models work as long as they want, basically. Yeah. Do you copy paste manually into the chatbot? Yeah. Yeah. That&#8217;s, that was how we got the chatbot reference. We&#8217;re not going to be keeping those updated at like quite the same scale as hundreds of models.swyx [00:46:38]: Well, so I don&#8217;t know, talk to a browser base. They&#8217;ll, they&#8217;ll automate it for you. You know, like I have thought about like, well, we should turn these chatbot versions into an API because they are legitimately different agents in themselves. Yes. Right. Yeah.Micah [00:46:53]: And that&#8217;s grown a huge amount of the last year, right? Like the tools. The tools that are available have actually diverged in my opinion, a fair bit across the major chatbot apps and the amount of data sources that you can connect them to have gone up a lot, meaning that your experience and the way you&#8217;re using the model is more different than ever.swyx [00:47:10]: What tools and what data connections come to mind when you say what&#8217;s interesting, what&#8217;s notable work that people have done?Micah [00:47:15]: Oh, okay. So my favorite example on this is that until very recently, I would argue that it was basically impossible to get an LLM to draft an email for me in any useful way. Because most times that you&#8217;re sending an email, you&#8217;re not just writing something for the sake of writing it. Chances are context required is a whole bunch of historical emails. Maybe it&#8217;s notes that you&#8217;ve made, maybe it&#8217;s meeting notes, maybe it&#8217;s, um, pulling something from your, um, any of like wherever you at work store stuff. So for me, like Google drive, one drive, um, in our super base databases, if we need to do some analysis or some data or something, preferably model can be plugged into all of those things and can go do some useful work based on it. The things that like I find most impressive currently that I am somewhat surprised work really well in late 2025, uh, that I can have models use super base MCP to query read only, of course, run a whole bunch of SQL queries to do pretty significant data analysis. And. And make charts and stuff and can read my Gmail and my notion. And okay. You actually use that. That&#8217;s good. That&#8217;s, that&#8217;s, that&#8217;s good. Is that a cloud thing? To various degrees of order, but chat GPD and Claude right now, I would say that this stuff like barely works in fairness right now. Like.George [00:48:33]: Because people are actually going to try this after they hear it. If you get an email from Micah, odds are it wasn&#8217;t written by a chatbot.Micah [00:48:38]: So, yeah, I think it is true that I have never actually sent anyone an email drafted by a chatbot. Yet.swyx [00:48:46]: Um, and so you can, you can feel it right. And yeah, this time, this time next year, we&#8217;ll come back and see where it&#8217;s going. Totally. Um, super base shout out another famous Kiwi. Uh, I don&#8217;t know if you&#8217;ve, you&#8217;ve any conversations with him about anything in particular on AI building and AI infra.George [00:49:03]: We have had, uh, Twitter DMS, um, with, with him because we&#8217;re quite big, uh, super base users and power users. And we probably do some things more manually than we should in. In, in super base support line because you&#8217;re, you&#8217;re a little bit being super friendly. One extra, um, point regarding, um, GDP Val AA is that on the basis of the overperformance of the models compared to the chatbots turns out, we realized that, oh, like our reference harness that we built actually white works quite well on like gen generalist agentic tasks. This proves it in a sense. And so the agent harness is very. Minimalist. I think it follows some of the ideas that are in Claude code and we, all that we give it is context management capabilities, a web search, web browsing, uh, tool, uh, code execution, uh, environment. Anything else?Micah [00:50:02]: I mean, we can equip it with more tools, but like by default, yeah, that&#8217;s it. We, we, we give it for GDP, a tool to, uh, view an image specifically, um, because the models, you know, can just use a terminal to pull stuff in text form into context. But to pull visual stuff into context, we had to give them a custom tool, but yeah, exactly. Um, you, you can explain an expert. No.George [00:50:21]: So it&#8217;s, it, we turned out that we created a good generalist agentic harness. And so we, um, released that on, on GitHub yesterday. It&#8217;s called stirrup. So if people want to check it out and, and it&#8217;s a great, um, you know, base for, you know, generalist, uh, building a generalist agent for more specific tasks.Micah [00:50:39]: I&#8217;d say the best way to use it is get clone and then have your favorite coding. Agent make changes to it, to do whatever you want, because it&#8217;s not that many lines of code and the coding agents can work with it. Super well.swyx [00:50:51]: Well, that&#8217;s nice for the community to explore and share and hack on it. I think maybe in, in, in other similar environments, the terminal bench guys have done, uh, sort of the Harbor. Uh, and so it&#8217;s, it&#8217;s a, it&#8217;s a bundle of, well, we need our minimal harness, which for them is terminus and we also need the RL environments or Docker deployment thing to, to run independently. So I don&#8217;t know if you&#8217;ve looked at it. I don&#8217;t know if you&#8217;ve looked at the harbor at all, is that, is that like a, a standard that people want to adopt?George [00:51:19]: Yeah, we&#8217;ve looked at it from a evals perspective and we love terminal bench and, and host benchmarks of, of, of terminal mention on artificial analysis. Um, we&#8217;ve looked at it from a, from a coding agent perspective, but could see it being a great, um, basis for any kind of agents. I think where we&#8217;re getting to is that these models have gotten smart enough. They&#8217;ve gotten better, better tools that they can perform better when just given a minimalist. Set of tools and, and let them run, let the model control the, the agentic workflow rather than using another framework that&#8217;s a bit more built out that tries to dictate the, dictate the flow. Awesome.swyx [00:51:56]: Let&#8217;s cover the openness index and then let&#8217;s go into the report stuff. Uh, so that&#8217;s the, that&#8217;s the last of the proprietary art numbers, I guess. I don&#8217;t know how you sort of classify all these. Yeah.Micah [00:52:07]: Or call it, call it, let&#8217;s call it the last of like the, the three new things that we&#8217;re talking about from like the last few weeks. Um, cause I mean, there&#8217;s a, we do a mix of stuff that. Where we&#8217;re using open source, where we open source and what we do and, um, proprietary stuff that we don&#8217;t always open source, like long context reasoning data set last year, we did open source. Um, and then all of the work on performance benchmarks across the site, some of them, we looking to open source, but some of them, like we&#8217;re constantly iterating on and so on and so on and so on. So there&#8217;s a huge mix, I would say, just of like stuff that is open source and not across the side. So that&#8217;s a LCR for people. Yeah, yeah, yeah, yeah.swyx [00:52:41]: Uh, but let&#8217;s, let&#8217;s, let&#8217;s talk about open.Micah [00:52:42]: Let&#8217;s talk about openness index. This. Here is call it like a new way to think about how open models are. We, for a long time, have tracked where the models are open weights and what the licenses on them are. And that&#8217;s like pretty useful. That tells you what you&#8217;re allowed to do with the weights of a model, but there is this whole other dimension to how open models are. That is pretty important that we haven&#8217;t tracked until now. And that&#8217;s how much is disclosed about how it was made. So transparency about data, pre-training data and post-training data. And whether you&#8217;re allowed to use that data and transparency about methodology and training code. So basically, those are the components. We bring them together to score an openness index for models so that you can in one place get this full picture of how open models are.swyx [00:53:32]: I feel like I&#8217;ve seen a couple other people try to do this, but they&#8217;re not maintained. I do think this does matter. I don&#8217;t know what the numbers mean apart from is there a max number? Is this out of 20?George [00:53:44]: It&#8217;s out of 18 currently, and so we&#8217;ve got an openness index page, but essentially these are points, you get points for being more open across these different categories and the maximum you can achieve is 18. So AI2 with their extremely open OMO3 32B think model is the leader in a sense.swyx [00:54:04]: It&#8217;s hooking face.George [00:54:05]: Oh, with their smaller model. It&#8217;s coming soon. I think we need to run, we need to get the intelligence benchmarks right to get it on the site.swyx [00:54:12]: You can&#8217;t have it open in the next. We can not include hooking face. We love hooking face. We&#8217;ll have that, we&#8217;ll have that up very soon. I mean, you know, the refined web and all that stuff. It&#8217;s, it&#8217;s amazing. Or is it called fine web? Fine web. Fine web.Micah [00:54:23]: Yeah, yeah, no, totally. Yep. One of the reasons this is cool, right, is that if you&#8217;re trying to understand the holistic picture of the models and what you can do with all the stuff the company&#8217;s contributing, this gives you that picture. And so we are going to keep it up to date alongside all the models that we do intelligence index on, on the site. And it&#8217;s just an extra view to understand.swyx [00:54:43]: Can you scroll down to this? The, the, the, the trade-offs chart. Yeah, yeah. That one. Yeah. This, this really matters, right? Obviously, because you can be super open, but dumb. I mean, obviously goes the wrong way here. Right.George [00:54:55]: A lot of people would like to see labs hill climb on the, and target.Micah [00:55:00]: This is the access to hill climb. Yeah. Unfortunately, it might be fundamentally true that the, the slum will always go this direction because once you open something up, then everyone else can get to the level of what you have now.swyx [00:55:11]: Well, so let me, let me tweak your points. You have, I have a point system, right? Like you have these like numbers on the point system and it go up to 18, you know, but like, just because I have a little bit of open data doesn&#8217;t mean I&#8217;m necessarily that much better in someone who put a lot of effort into their open ways, it is that it&#8217;s smarter. So I might, I might just mess with the point system to make sure that like, I&#8217;m accurately representing the, the contribution to the open openness.Micah [00:55:36]: It is hard to wait for the materiality of the contribution to open source. We tried to make it so that it is quite well-defined and no one can disagree about which category things should be in. So we&#8217;re not saying this was a big contribution or a small contribution in terms of impact on the industry or anything. It&#8217;s just how much of your data did you release? I would say that it is still valid to say that we trained a model that&#8217;s not that smart, maybe even not at the frontier for a particular size category, but we chose to open up all the data, all the training code. That is a very useful exercise for the industry. And we want to recognize that even if the smartest model in the category.swyx [00:56:18]: Yeah. And also a special shout out to NVIDIA and Emotron, which doesn&#8217;t get enough credit for the amount of stuff that they do. And honestly, it&#8217;s a sales enablement for NVIDIA as well. The fact that they can do this is... Side project.Micah [00:56:29]: Totally. But I mean, it is true that NVIDIA have actually put an enormous amount of effort over the last year, especially into the Nematron models.swyx [00:56:35]: Yeah. And so many people actually use it for synthetic data and stuff. It&#8217;s a pretty interesting secret of the industry that NVIDIA holds up all these guys.Micah [00:56:45]: I mean, it&#8217;s in their interest for there to be more AI.swyx [00:56:49]: So obviously, I think you want to push openness as having an index. Every index that you push has encoded some kind of opinion or value. Yes. I think one of the openness questions from this year was people messing with the license. And so Lama had this, like, if you have 700 million daily active users, you&#8217;re not allowed to use our model or you have to talk to us, something like that. So basically, like, what are your customers telling you about the kind of licensing worries that they have? Right. Because obviously, most people will never hit 700 million users.Micah [00:57:21]: We have like a detailed breakdown of that in the openness index. And that was actually one of the initial questions that took us down the route of wanting to... Do this. Because, yeah, the simplest thing that, like, our opinion is, is that there is a lot of advantage to having, like, an official OSI license like MIT or Apache 2, because then the box is just checked. You don&#8217;t even need to read it because it&#8217;s just Apache 2 and you can do it ever you want and it&#8217;s fine. There are often very good reasons that companies don&#8217;t want to release language models with those completely open licenses. The index tells you. So if you get the top category, that&#8217;s one of those licenses. You&#8217;re totally good. And then... And then we&#8217;ve got some lower categories for when attribution is required and then when commercial use is not allowed. Yeah, they&#8217;re there.swyx [00:58:05]: So that&#8217;s the openness index. Thank you for doing all those works. Let&#8217;s talk a little bit, or at least end the pod, on just the trend reports that you guys do, which is kind of a bit of the bread and butter how you make money. I highly encourage everyone to see George&#8217;s talk at World&#8217;s Fair, which gives a little bit of a preview. And you were very excited about talking about the smiling curve, or I don&#8217;t know what you call it. Yeah, yeah, yeah, yeah, let&#8217;s talk about that one. Let&#8217;s explain it for people. And I might, I might actually put it up because I don&#8217;t have it. Yeah, I&#8217;ve got to copy the slide, that&#8217;d be, that&#8217;d be excellent. It&#8217;s important for people to have in their head because, yeah, people only get the marketing message from the labs that, oh, we&#8217;re cutting costs all the time.Micah [00:58:41]: Yeah, yeah, but it&#8217;s, it&#8217;s true. It&#8217;s it&#8217;s not the whole picture. So, okay. A couple of like the big trends that we track at Artificial Analysis over time and that like we&#8217;re always showing charts of on the trends page in these reports and stuff. One, that the cost of intelligence has been falling dramatically. Over the last couple of years, the best way to think about that is that the cost for each terror of intelligence has been dropping the, like one fact on that is that you can get intelligence at the level of GPT-4 for over a hundred times cheaper than GPT-4 was at launch right now. I think my number is a thousand actually.swyx [00:59:16]: If you look at the Amazon Nova models, which are very, very cheap. Yeah.Micah [00:59:21]: Like my, my conservative statement is normally like, but in fairness, this slide. Like I, we were actually saying for the podcast, right. It&#8217;s like maybe six months old now and it&#8217;s conceptually still correct, but like could actually probably do a tweak on the exact numbers because like the market&#8217;s moving so quickly.swyx [00:59:37]: If you&#8217;re feeling kick it off, I mean, we&#8217;ll have this chart.Micah [00:59:39]: I told people to watch the world&#8217;s fair talk, but let&#8217;s, let&#8217;s introduce what context makes you make something like this. There are two trends that seem to not make sense together, both of which we talk a lot about at Artificial Analysis and are very important to developers building stuff in AI. The first is that the cost of intelligence for each level of intelligence has been dropping dramatically over the last couple of years. We track the cost to run Artificial Analysis Intelligence Index for each bucket of Intelligence Index scores and each bucket, you just see the line go down really, really quickly and actually go down more quickly to each new level of intelligence that&#8217;s been achieved over the last couple of years. So the rate of that cost has actually been going up. So. Yeah. We&#8217;ve got that being true. And yet it is clearly possible to spend quite a lot more on AI inference now than it was a couple of years ago.George [01:00:34]: NVIDIA stock go up.swyx [01:00:36]: It&#8217;s going, it&#8217;s going really up. Uh, I just heard from a friend&#8217;s startup that just went through the shift zero. They&#8217;re spending $5,000 per employee on coding agents spend alone. That&#8217;s ridiculous. That&#8217;s an impressive number.Micah [01:00:49]: We need to get our numbers up. We&#8217;re, uh, we&#8217;re, we&#8217;re not quite hitting, hitting.swyx [01:00:52]: Well, I was like, it&#8217;s so high down. I&#8217;m like, are you doing something wrong? Yeah.Micah [01:00:55]: Cause there are some efficiency questions along the way, but like you can make AI inference useful to that level in a bunch of ways that I can imagine. Right. Yeah. Um, I, I don&#8217;t think that&#8217;s that nuts. Um, but basically the, the reason we made this slide to answer the question, right. Is to show that the crazy thing is that it is actually true. We&#8217;ve had this hundred X to a thousand X decline in the cost of GPT four level intelligence on the left-hand side. And yet on the right-hand side, because the multipliers are so big for the fact that even though. Small models can do GPT four level. Now we still want to use big models and probably bigger than ever models to, um, do frontier level intelligence. We&#8217;ve got reasoning models using tokens, and then we&#8217;re throwing them in these, them in these agentic workflows where they&#8217;re consuming enormous numbers of input tokens and making enormous numbers of output tokens working for a really long time. Those two things taken together, get you back to, we can spend enormously more today than we could a couple of years ago. Yep.George [01:01:50]: I think that&#8217;s right. There&#8217;s a number of drivers at play and we kind of outline kind of. Six key ones here. Um, but you know, as complex as changing quickly, all of these have changed very dramatically in the last, uh, in the last 12 months.swyx [01:02:04]: Let&#8217;s pick on hardware efficiency since you also have, you also track hardware stuff. And I think the general assertion or the message is that the efficiency from next gen Nvidia chips is actually not 4X. So you have what? 3X or 4X? You have 3X in here and it&#8217;s, it&#8217;s like 2X maybe, or it&#8217;s more of like a. Power story rather than like a share sort of compute tokens efficiency story. But yeah, what, what&#8217;s going on in, in hardware. Okay.Micah [01:02:31]: So the, the, the, the odds, unfortunately, uh, is it depends and it just depends massively on like so many things across a bunch of different types of workloads and ways to think about it. So one of the simplest ways to think about this is to take single relevant model, to think about serving it at speeds that are realistic for what you actually might want to hit. And can afford to hit, and then think about the throughput per GPU that you can achieve serving the model at those speeds. Rease. One of the reasons that&#8217;s important is that there&#8217;s a trade-off between the throughput per GPU that you can achieve and the per user speed that you can achieve. And as a, it costs more to serve stuff fast to, to users. When you run all of that for especially big sparse models, you can get a lot better than two or three X gain going from Hopper to Blackwell generation to video. I am. This shouldn&#8217;t be too controversial. Let&#8217;s say I&#8217;m like, I&#8217;m. I&#8217;m pretty confident that Blackwell has delivered pretty enormous gains and that the next couple of years of NVIDIA&#8217;s roadmap are going to continue to deliver quite enormous gains and that those will actually come through as lower total cost per token to the companies that are running models on them and will allow bigger models will allow way more tokens to be made for lower cost and that that&#8217;s gonna continue these things also stack on all of the software and model improvements. So basically like my prediction across like both sides of that, like smile chart, uh, that we&#8217;re gonna see the left-hand side continue to be true and probably like for another order of magnitude and the right-hand side continue to be true for another order of magnitude, and that&#8217;s gonna enable a whole lot of things.swyx [01:04:12]: Okay. Well, I&#8217;ll push on, uh, let&#8217;s go back to the, the, the small chart. I&#8217;ll push back on sparsity, right? Uh, we&#8217;ve gone a long way on sparsity. Deep seek was a major pusher of fine grain experts. Let&#8217;s call it. Yep. Right. Well, I have a mental number of sparsity in terms of let&#8217;s say active params versus total params. And that number went from 25%, let&#8217;s say down to like 15, right? You obviously can&#8217;t really go below, I don&#8217;t know, five. Is that obvious? So there&#8217;s a lower limit to, to sparsity is what I&#8217;m saying. I don&#8217;t know that that&#8217;s that obvious actually. All right.Micah [01:04:45]: Um, there, there must be a limit somewhere, right? Yeah, exactly. But we&#8217;ve got numbers in the wild that are quite a lot lower than that right now. So the GBD OSS models, like the big ones at about 5%, um, active, Kimmy K2, is it like 3% active? Oh, okay. I think, pretty sure.swyx [01:05:05]: I&#8217;ve looked at those numbers. I calculated them. I don&#8217;t remember. Yeah. But I remember thinking like, this must be it.George [01:05:11]: Your 5% is exactly like around the ballpark for the open weights models of, of what&#8217;s released today. I think one interesting that gives me kind of pause when thinking that it won&#8217;t go, the sparsity won&#8217;t go high. Or the number of percentage of active parameters lower is that we, in our benchmark, see a lot of performance, uh, correlated more with, uh, total parameters than active and not that correlated with how sparse, like the models are. Our accuracy, benchmark as part of a omniscience, it&#8217;s very correlated with total. It&#8217;s not correlated with, with active, uh, parameters, which I think is very at all, which is very, very interesting. And so I think, yeah, they could, they could be quite. A bit, um, to go here. Awesome.swyx [01:05:55]: Well, we don&#8217;t have that much time, but I w I did want to leave some room to cover reasoning and non-reasoning models and token efficiency. Let&#8217;s do that. So at a high, at a super high level, people have to classify this binary thing of reasoning versus non-reasoning. People who are insider have some discomfort with that because basically you just have to think tag or no think tag. How have you guys decided to approach this? And also how does that laid out in, over the course of the year where we have things like GPT-5, which is a model. Right.Micah [01:06:24]: Let&#8217;s say GPT-5 in chat GPT, the consumer experience as a model router, when you&#8217;re hitting the API, like we can, you can pick the different versions and you can pick reasoning strength of the different versions, but that, that goes to why this is now such a complex thing. So earlier this year, and probably when you and George last spoke for the AI engineers world&#8217;s fair, we had this great slide that was super easy, where we would show that the average reasoning model is using 10 times the number of tokens per query in our intelligence index as the average non-reasoning model. And there was this moment where that was a pretty clear distinction and extremely useful to look at it just like that. Definitely no longer the case, not least because you can think about reasoning strength for a bunch of these different models, but particularly because different models have wildly different token efficiency now, more than an order of magnitude in difference. That means that the way that you probably need to think about cost for any application is to use something like our cost around intelligence index metric as the starting point. Right. for what it&#8217;s going to look like for these different models, these different reasoning strengths, and this continuous spectrum from non-reasoning to reasoning. That&#8217;s basically like where we&#8217;re at. So we will still show reasoning and non-reasoning and define reasoning as when there is that separated chain of thought that you&#8217;re getting at a different parameter in an API normally, but it doesn&#8217;t necessarily anymore mean that that model is actually going to have longer end-to-end latency that is going to use more tokens than something that is brandedswyx [01:07:51]: in a non-reasoning model for the same task. That&#8217;s true. I think 5.1 was it. And then 5.1 Codex had these chart, which was super nice of this, like, let&#8217;s say bottom 10 percentile query being faster, but top 10 percentile being longer. And that&#8217;s a kind of the efficiency chartMicah [01:08:10]: you want to see, right? Yeah. So that is an extra thing. Let&#8217;s say that we&#8217;ve got, that&#8217;s a really important extra thing though, right? That you&#8217;ve got not just the average number of token span used by the model, which we cover really well right now, but the behavior that you want in the model is it to use more tokens when it needs more tokens and not to use more tokens when it doesn&#8217;t need more tokens. So that&#8217;s what OpenAI, we&#8217;re basically claiming that 5.1 Codex is better at. We don&#8217;t actually publish anything on this right now, but have tracked it a bunch internally in our internal analytics on evals across all the models that we run, where we look at the difficulty to questions and the correlation between token usage and difficulty and net net, surprise, surprise, like models have got. I think going into next year, that&#8217;s going to be really important, especially as you multiply it by the number of steps in an agentic workflow that a model has to take to get to an answer. We are going to care a lot about token efficiency and number of turns efficiency for getting to whatswyx [01:09:08]: we want. Which would you rather have token efficiency or number of turns efficiency? Or like, which is more important to work on?Micah [01:09:16]: it depends on the application and both are going to be really important.George [01:09:18]: Uh, yeah.Micah [01:09:20]: Well, total cost is just-swyx [01:09:21]: TalBench Retail, TalBench Airline.George [01:09:23]: Yeah. Interestingly in Tal, um, Tal2Bench Telecom, it&#8217;s cheaper to run, you know, on a per token basis, more expensive models like a GBD5 compared to some smaller open source models, because the, um, some of the GBD5, for instance, uh, got to the answer faster. And so it was able to resolve the customer&#8217;s query faster and fewer turns. And maybe it used more tokens per turn, but it certainly- It&#8217;s not going to cost more per token. So you would always rather use GBD5 in, in, in, in that scenario. And so I think that&#8217;s what, that&#8217;s where we&#8217;re getting to. I think number of turns is, it&#8217;s going to be a metric that we&#8217;re going to be talking about a lot more. And, uh, I think it&#8217;ll be something that people want to really start to think about, uh, a lot more.swyx [01:10:06]: There&#8217;s a trade-off in benchmarking here where most benchmarks needs to be one turn to be autonomous, to be parallelized and all that. But most, a lot of real life use cases need to be multi-turn and especially like quick multi-turns. So you can align. Yeah.Micah [01:10:19]: Yeah. I mean, I, I would say that historically benchmarks have been single turn, but I wouldn&#8217;t say they need to be at all into the future, right? Like we have a couple of agentic benchmarks in the index right now and GDP that we were talking about. We let the models do up to a hundred turns and, um, our stirrup agent to do that evil. And we&#8217;re going to build similar stuff like that in the future. It definitely is hard and you&#8217;ve got whole kinds of infrastructure problems to run that and exactly as you say, parallelize it because we need to run that on hundreds of models and we want to do that really fast when you want us to come out and with labs want us to run it on their models,swyx [01:10:53]: but you can do it. We&#8217;re putting in the work to build that stuff and it&#8217;s going to be great. Okay. So we&#8217;ve covered, I mean, there&#8217;s a lot more to cover and you haven&#8217;t even touched onGeorge [01:11:01]: multimodal, which is huge. We also do speech benchmarking, image benchmarking, uh, videoswyx [01:11:09]: benchmarking, hardware. I like the way that you&#8217;ve done it because they&#8217;re very smart, which is a video takes a long time. So you pre-generate, right? So then people just pick their preferences and you can see the, the overall arena results. And you also avoid like any sensitivity issuesMicah [01:11:23]: around the unsafe content that is being generated. Yeah. And you can see it as a good, good thing, a bad thing, depending on what your view is. But it means that we have a quite active creative direction approach to trying to understand what creative professionals and users want to do with those image and video models. And so that we can be directing the arenas in our categories toward gathering data, votes on what people care about. One call out actually to listeners, like if you are using our arenas is that you can submit requests to us for things that we should cover. I didn&#8217;t know that. Yeah. Understudied categories, areas that you think the models are bad at and the labs don&#8217;t focus on enough. Like if you want something solved, one of the levers that you have is send us a couple of prompts on it. We might be able to get a category going on it. And this thing that we were talking about earlier, right? That once things get measured, they can get targeted. You can make that work for you.swyx [01:12:18]: For me as a content creator, infographics, very needed. I took the latest deep seek paper and they had some descriptions of their search agents and their coding agents and I put it in and I created an infographic. And I just think like as I said, industrial use case that doesn&#8217;t require a lot of, I guess, design tastes, but just requires some, you need to conform to some preset references, which is something that is increasingly important, especially in like the nano banana series. But yeah, and I think that&#8217;s the key there. I think it&#8217;s important to be able to I think OpenAI is releasing Image 2 soon, which is going to have that. So I think it&#8217;s all of a kind where people need to incentivize workhorse use cases and not just art. I don&#8217;t know. Totally. Yeah. What are we going to be talking about next year? What&#8217;s emerging that you&#8217;re seeing and maybe not in the discussion?Micah [01:13:06]: The first answer that I&#8217;ll give to that is the boring answer is that on most of our charts, the lines go in a particular direction and our overall prediction is the lines are going to keep going in that direction. We&#8217;re going to do a lot and do a lot to be as useful as possible to developers and companies to measure what&#8217;s important on every one of those and along those lines. But I think we&#8217;re going to talk about similar stuff. It&#8217;s just that we&#8217;re going to have continued on this trajectory for another year and things are going to feel pretty different because of that happening. I know this is the boring answer to that question. No, no.swyx [01:13:36]: I mean, I&#8217;m a fan of things that, truths that don&#8217;t change because you can build and plan for that. And I think in media in general, in the podcast business, newsletters, you know, there&#8217;s a Twitter business, Twitter business, people are addicted to change, like, oh, everything&#8217;s breaking. Everything&#8217;s, no, like there&#8217;s some truths that aren&#8217;t just constants that you can plan on and build. And yeah.George [01:13:58]: I think one of the truths is that the demand for AI intelligence and smarter AI intelligence is going to be insatiable. Some people disagree that, okay, once we reach certain thresholds, then you don&#8217;t need more intelligence. I think to that, I ask people, have they ever worked with? Or managed someone in a work environment and wouldn&#8217;t press the button that they were smarter to make them smarter or better at their job or would they never press that for themselves? And I&#8217;m not sure that that&#8217;s, that&#8217;s the case, but I think for artificial analysis, we&#8217;ll keep benchmarking raw intelligence, but we also want to think about it and explore models more deeply across other axes as well. I think hallucinations, the start of that, but we&#8217;re getting into wanting to support people and understanding, okay, the behavior, the person personalities. Of the models to help people make more nuanced decisions, you&#8217;re going to have a personality bench.swyx [01:14:52]: Maybe that is a direction that Chadji opening eyes leaning into a lot. So if you manage to solve that, you should definitely talk to Fiji and Roon. Oh, okay. Yeah. So what is going to be included in, let&#8217;s say like a V3 of the intelligence index, because obviously you&#8217;re going to saturate in March.Micah [01:15:10]: Why don&#8217;t we break it now? How soon is the podcast going to come out? Whenever you want. Okay. So we&#8217;re at V3 right now. So the, so the, the, the version that we, that&#8217;s going inside is, is, is, is V3 V4 is what we&#8217;re going to call the next, you know, major of it. Surprise, surprise. We&#8217;re going to be adding several of the things that we&#8217;ve actually talked about today that we&#8217;ve launched over the last few weeks. So it&#8217;s not, that&#8217;s not going to be wildly shocking, but some of the things that are most exciting is that adding GDP value is going to give us this general agentic performance in a really strong way in intelligence index and in critical point, the, um, physics, EBL, George was talking about similar to frontier math. Yeah. That&#8217;s very interesting. That gives us completely new view with a brand new data set of very, very hard research problems. We are going to be using Omniscience and we are going to be using hallucination rate. The exact way is that all of those are going to come together. Um, The waitings is going to be hard because the numbers are different. Yeah. We&#8217;re going to make sure that we don&#8217;t do anything to cause odd distortions and stuff that could be misleading. But every time you version it, you have a one-time reset of the Exactly. Yeah. That&#8217;s exactly how we think about it. We will make sure that within each version number that there&#8217;s no parliamentary issue. No drift in any of the scores so that people can rely on them and reference them. You just have to watch out for that version number. Once it&#8217;s v4.1, those numbers won&#8217;t be compatible with v4.swyx [01:16:23]: Of course. There&#8217;s a little bit of debate over the accuracy of TileBench. I don&#8217;t know if you&#8217;re clued in to what&#8217;s going on. Apparently, a very high number of TileBench tests are impossible.Micah [01:16:34]: Potentially for the earlier versions, Tile2Bench Telecom, we&#8217;re pretty convinced is pretty good. If anything, the only issue there is that models have got very good at doing it. And so, like anything... Tile3. Yeah.swyx [01:16:49]: On we go. Yeah, on we go. Okay, well, thank you so much for providing such a great service to the industry. I&#8217;m glad to at least know you guys before you got famous and now you are famous.Micah [01:16:59]: Oh, look, our pleasure. We really appreciate your support along the way. I wasn&#8217;t kidding at the start, right? That it was a quite material moment for us when artificial analysis was covered on Latent Space. Some random guy. And San Francisco mentions you. I was a fan of Latent Space for like a year before you mentioned us. So, I&#8217;d been listening. I don&#8217;t think I was familiar with you personally yet at that point. But I listened to your voice probably for many, many hours. And so, once you mentioned it, I got to get to know you and meet you for the first time nearly a couple of years ago. It was really cool, honestly. So, yeah, it&#8217;s great to be here.George [01:17:36]: And thanks for being such a great member of the community and kind of spotlighting projects, projects which don&#8217;t have attention and bringing them to your audience. Yeah.swyx [01:17:44]: Well, actually, so it wasn&#8217;t me, right? Someone in the Discord dropped it in our Discord. And I rely on our community and it kind of feeds itself, right? Nice. So, someone brought it to my attention. I don&#8217;t know who. We should probably go back and check. But once I saw it, I was like, this looks good. This is something I always wanted. I wanted to build it. I was too shy or dumb or lazy to build it. And you guys did. And now it&#8217;s a whole thing. So, thank you for being here.George [01:18:08]: I built some really cool other stuff like this pod. Yeah. Yeah. Totally. So, thank you. That&#8217;s it. Great. Cool. Thanks.",
      "url": "https://www.latent.space/p/artificialanalysis",
      "author": "Unknown",
      "published": "2026-01-08T15:08:15",
      "source": "Latent.Space",
      "source_type": "rss",
      "tags": [],
      "summary": "Latent Space podcast features Artificial Analysis, which has grown from a side project to the 'independent gold standard' for AI benchmarking, backed by AI Grant. The company provides trusted evaluations for developers, enterprises, and major labs.",
      "importance_score": 55.0,
      "reasoning": "Highlights growing importance of independent AI evaluation infrastructure. Useful industry development but podcast format limits news value.",
      "themes": [
        "AI Evaluation",
        "Benchmarking",
        "Industry Infrastructure"
      ],
      "continuation": null
    },
    {
      "id": "4320d6eece91",
      "title": "AI Devices Are Coming. Will Your Favorite Apps Be Along for the Ride?",
      "content": "Tech companies are calling AI the next platform. But some developers are reluctant to let AI agents stand between them and their users.",
      "url": "https://www.wired.com/story/openai-amazon-operating-system-ai-apps-ads/",
      "author": "Maxwell Zeff",
      "published": "2026-01-08T19:00:00",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Business",
        "Business / Artificial Intelligence",
        "Model Behavior",
        "artificial intelligence",
        "OpenAI",
        "Google",
        "Meta",
        "Amazon",
        "operating systems",
        "Apps",
        "Startups"
      ],
      "summary": "Tech companies position AI as the next computing platform, but app developers are hesitant to let AI agents intermediate their user relationships. The tension highlights ecosystem challenges for AI-native operating systems.",
      "importance_score": 55.0,
      "reasoning": "Important strategic discussion about AI platform dynamics, but analysis piece rather than breaking development.",
      "themes": [
        "AI Platforms",
        "Developer Ecosystem",
        "Business Strategy"
      ],
      "continuation": null
    },
    {
      "id": "3beefe4a5946",
      "title": "Google Updates Gmail With Suite of AI Tools",
      "content": "While the features can streamline specific tasks, they also require users to remain vigilant about the content generated by AI.",
      "url": "https://aibusiness.com/generative-ai/google-updates-gmail-with-ai",
      "author": "Esther Shittu",
      "published": "2026-01-08T18:54:01",
      "source": "aibusiness",
      "source_type": "rss",
      "tags": [],
      "summary": "Google is updating Gmail with AI-powered features that can streamline tasks, though users must remain vigilant about AI-generated content quality.",
      "importance_score": 54.0,
      "reasoning": "Duplicate coverage of Gmail AI features with slightly different framing. Incremental product update.",
      "themes": [
        "Product Launch",
        "Google",
        "Productivity AI"
      ],
      "continuation": null
    },
    {
      "id": "3be37fc8e333",
      "title": "2026 to be the year of the agentic AI intern",
      "content": "After several years of experimentation, enterprise AI is moving out of the pilot phase. To date, many organisations limit AI to general-purpose chatbots, often created by small groups of early adopters. According to Nexos.ai, that model will give way to something more operational: fleets of task-specific AI agents embedded directly into business workflows.\nEven isolated agents are in common use, screening CVs, reviewing contracts, drafting routine correspondence, preparing management reports and orchestrating actions in enterprise systems.\nAnalysis from the company suggests organisations that move from single chatbots to multiple role-specific agents see materially higher adoption and claim a clearer business impact. Teams interact with agents that can behave like junior colleagues, where each agent is accountable for a defined slice of work.\nEvery team gets its own named agent\nThe company&#8217;s studies envisage the normalisation of named AI agents assigned on a per team basis, which it describes as an &#8220;AI intern&#8221;. These are not general-purpose assistants, but dedicated tools for specific operational processes.\nFor example, HR teams might deploy agents tuned to recruitment criteria, or legal teams using agents configured to flag contract standard violations. Sales teams will rely on agents optimised for their sales pipelines and integrated with an existing CRM. In each case, Nexos says the business value comes from contextual awareness and integration with existing software and date, rather than from advances in the raw power of the model.\nEarly enterprise deployments suggest the gains can be significant. Payhawk, for example, reports that its deployment of Nexos.ai&#8217;s agentic platform in finance, customer support, and operations reduced the necessary security investigation time by 80%. The company achieved 98% data accuracy and cut its processing costs by 75%.\n\u017dilvinas Gir\u0117nas, head of product at Nexos.ai, says the real benefit stems from coordination. &#8220;The shift from single-purpose agents to coordinated AI teams is fundamental. Businesses are [\u2026] building groups of specialised agents that work together in a workflow. That&#8217;s when AI stops being a pilot and starts becoming infrastructure.&#8221;\nPlatform consolidation becomes unavoidable\nAs the number of active agents in organisations rises, a second-order problem \u2013 fragmentation \u2013 appears. Teams running five to ten agents in different tools face duplicate costs and inconsistency in security controls. From the perspective of IT governance, this situation can become unsustainable.\nEvidence from early Nexos adopters suggests consolidating agents on a enterprise-wide shared platform delivers faster deployment \u2013 in some cases twice as fast \u2013 and gives better oversight over spend and performance.\nGir\u0117nas says: &#8220;When teams are juggling multiple vendors and logins, usage drops. A single platform is what allows organisations to extract consistent value rather than paying for shelfware.&#8221;\nThe situation points to pattern familiar to enterprise technology veterans: AI agent systems follow the same trajectory of consolidation seen in collaboration, security, and analytics stacks.\nAI operations shifts to the business\nThe company&#8217;s findings suggest that the ownership of AI operations is moving from engineering teams and towards business leaders and discrete business functions. The function-specific deployment model means heads of HR, legal, finance, and sales are will expected to configure their own agents, a task that include prompt management. Thus, the ability to manage agents will become a core operational competency for individuals and business functions.\nThis places new requirements on agentic platforms, with the need for interfaces that are approachable by non-technical users, with the stack operating with minimal reliance on APIs or developer-style tooling. Team leads will need to be able to adjust instructions, test outputs from their adopted systems and find ways to scale successful configurations. Engineering support will be reserved for isolated problem-solving.\nDemand will outstrip delivery capacity\nNexos.ai&#8217;s final prediction is the appearance of a capacity challenge. It says that once teams can deploy their first few agents successfully, demand for similar systems will accelerate in the organisation. Marketing departments may look for workflow automation, finance pros will want compliance-checking agents, and customer success teams will explore the effects of support triage: Each department, seeing proven value elsewhere, will expect similar abilities and efficiencies.\nIndustry projections suggest that by the end of 2026, around 40% of enterprise software applications will incorporate task-specific AI agents, up from under 5% in 2024. Engineering capacity is unlikely to keep pace if every agent is built from scratch \u2013 thus the call for centralised capability.\n&#8220;The organisations that cope best will be those with agent libraries rather than bespoke builds,&#8221; Gir\u0117nas says. &#8220;Templates, playbooks, and pre-built agents are the only way to meet rising demand without overwhelming delivery teams.&#8221;\n(Image source: &#8220;Office Assistant&#8221; by LornaJane.net is licensed under CC BY-ND 2.0.)\n&nbsp;\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post 2026 to be the year of the agentic AI intern appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/agent-ai-as-the-intern-in-2026-prediction-by-nexos-ai/",
      "author": "AI News",
      "published": "2026-01-08T12:24:21",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "AI Market Trends",
        "Special Reports & Series",
        "agentic ai",
        "business processes",
        "centralised deployment"
      ],
      "summary": "Nexos.ai predicts 2026 will see enterprises shift from general chatbots to fleets of task-specific AI agents embedded in workflows. Organizations using multiple role-specific agents reportedly see higher adoption and clearer business impact.",
      "importance_score": 52.0,
      "reasoning": "Industry prediction about agentic AI adoption. Useful analysis but forward-looking speculation rather than concrete news.",
      "themes": [
        "Agentic AI",
        "Enterprise AI",
        "Predictions"
      ],
      "continuation": null
    },
    {
      "id": "ab06cfd7ac34",
      "title": "People Are Using AI to Falsely Identify the Federal Agent Who Shot Renee Good",
      "content": "Online detectives are inaccurately claiming to have identified the federal agent who shot and killed a 37-year-old woman in Minnesota based on AI-manipulated images.",
      "url": "https://www.wired.com/story/people-are-using-ai-to-falsely-identify-the-federal-agent-who-shot-renee-good/",
      "author": "David Gilbert",
      "published": "2026-01-08T16:33:56",
      "source": "Feed: Artificial Intelligence Latest",
      "source_type": "rss",
      "tags": [
        "Politics",
        "Politics / Disinformation",
        "artificial intelligence",
        "Social Media",
        "Crime",
        "Immigration and Customs Enforcement",
        "government",
        "AI detectives"
      ],
      "summary": "Online users are using AI to falsely identify the federal agent who shot Renee Good in Minnesota, based on AI-manipulated images. The case illustrates AI's role in spreading disinformation during high-profile incidents.",
      "importance_score": 50.0,
      "reasoning": "AI misuse case demonstrating real-world harms, but more about social media behavior than frontier AI development.",
      "themes": [
        "Disinformation",
        "AI Misuse",
        "Social Media"
      ],
      "continuation": null
    },
    {
      "id": "40c1668112b1",
      "title": "\u201cDr AI, am I healthy?\u201d 59% of Brits rely on AI for self-diagnosis",
      "content": "AI advancements are changing the way we look at health and deal with health-related issues. According to a new nationwide study by Confused.com Life Insurance, three in five Brits now use AI to self-diagnose health conditions. Through various searches, like side effects of medical conditions, treatment options, and symptom checks, as much as 11% of respondents claim AI has helped improve their conditions. More than a third (35%) are likely to use AI in this context in the future, moving away from traditional GP appointments \u2013 increasingly harder to get at short notice.\nIn the UK, the average GP appointment waiting time is currently 10 days, a period too long for many. Therefore, health related searches have significantly risen since January 2025, including &#8220;what is my illness?&#8221;, increasing by 85%, &#8220;what are the symptoms for?&#8221; (33%), and &#8220;side effects&#8221; (22%).\nMost common health-related queries with AI\nAccording to Confused.com, the most searched for health-related query is symptom checks, with 63% seeking advice from AI. Next are side effects at 50% and lifestyle and well-being techniques at 38%. 20% have also sought mental health support through therapy or recommended coping strategies, treating ChatGPT as their virtual therapist.\n35% of respondents over 65 are using AI to self diagnose, with 54% using the technology to check their symptoms. This pales in comparison to 18-24 year olds, with 85% using AI to search regularly for health issues.\nTom Vaughan, life insurance expert at Confused.com, commented on these latest findings, saying, &#8220;Advances in AI technology have created a new way for people to approach healthcare and self-diagnosis. More individuals are taking steps to support their own and their family&#8217;s well-being, getting ahead of health concerns and addressing situations as quickly as possible.&#8221;\nAI self diagnosis potential benefits\nWith current GP waiting times sometimes reaching a month, it is no surprise that 42% claimed AI is quicker than waiting for a doctor&#8217;s appointment. 50% of 25-34 year olds and 51% of 35-44 year olds said they are not comfortable taking any risks with timings, believing self-diagnosis provides a faster response than waiting for a GP.\nFamily well-being is also crucial, with 20% using AI to determine the best methods to support their loved one&#8217;s health. Not having to physically speak to a doctor is another reason many turned to AI. 24% said they feel more comfortable using AI than discussing their health face to face with a healthcare professional, rising to 39% for 18-24 year olds.\n17% are searching for alternative medical solutions and support via AI, increasing to 27% for those aged 25-34. Money is another key factor, as 20% feel self diagnosis through AI could save them substantial private healthcare fees.\nAI has also had a positive influence for non-binary individuals and those with an alternative identity. 75% said the technology&#8217;s diagnosis had helped them a &#8220;great deal&#8221;, compared to just 13% for men and 9% for women.\nOverall, AI seems to have a positive impact on users&#8217; health situations. For instance, 11% stated that AI has helped their health conditions &#8220;a great deal,&#8221; while 41% claimed it has helped &#8220;somewhat.&#8221; The hope is that this self-diagnosis, though not guaranteeing accuracy, will encourage people to visit their GP for a formal diagnosis.\nOnly a minority of respondents (9%) felt AI has not helped their health in any way, indicating traditional healthcare methods are more reliable.\nTom Vaughan emphasised the importance of GP consultations. &#8220;While AI can be useful for initial research and gaining an understanding of a condition, it&#8217;s clear that for the ultimate peace of mind people should consult a GP or pharmacist. GPs and other medical professionals are the only people who can accurately diagnose conditions, some of which may worsen or become long-term illnesses without the proper treatment.&#8221;\nOpenAI launches ChatGPT Health\nConfused.com&#8216;s insights into AI use for health concerns coincides with OpenAI&#8217;s launch of its new ChatGPT Health feature, part of the ChatGPT platform. This has been set up to meet the substantial number of health-related queries made on the site each day. Figures suggest over 230 million health-related inquiries are made weekly.\nChatGPT Health allows users to connect their personal medical records and wellness apps, like Apple Health, allowing the AI to provide tailored responses, rather than general knowledge surrounding certain health conditions.\nAlthough set up to help users find answers to their health questions, OpenAI has stressed the new feature is not a diagnostic tool or substitute for professional medical care. It has been designed to support medical care, like understanding lab results and track wellness, rather than replace it and give formal medical diagnoses or treatment plans.\nChatGPT Health has been developed with input from hundreds of physicians around the world, ensuring clarity and safety for its users. Despite not being a substitute for medical professionals and traditional GP appointments, the number of people turning to AI for health information and help to understand medical issues is expected to rise, raising important questions and potential repercussions for patient care and clinical trust.\n(Image source: &#8220;The Sick Classroom by Nge Lay&#8221; by Jnzl&#8217;s Photos is licensed under CC BY 2.0.)\n\nWant to learn more about AI and big data from industry leaders? Check out AI &amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.\nThe post &#8220;Dr AI, am I healthy?&#8221; 59% of Brits rely on AI for self-diagnosis appeared first on AI News.",
      "url": "https://www.artificialintelligence-news.com/news/dr-ai-am-i-healthy-59-of-brits-rely-on-ai-for-self-diagnosis/",
      "author": "David Thomas",
      "published": "2026-01-08T13:10:00",
      "source": "AI News",
      "source_type": "rss",
      "tags": [
        "Healthcare & Wellness AI",
        "consumer",
        "healthcare",
        "medical"
      ],
      "summary": "A UK survey found 59% of Brits use AI for health self-diagnosis, with 11% claiming AI improved their conditions. Rising GP wait times (averaging 10 days) are driving adoption of AI health tools.",
      "importance_score": 45.0,
      "reasoning": "Consumer behavior survey about AI health use. Interesting social trend but not a frontier AI development.",
      "themes": [
        "Healthcare AI",
        "Consumer Behavior",
        "UK"
      ],
      "continuation": null
    }
  ]
}