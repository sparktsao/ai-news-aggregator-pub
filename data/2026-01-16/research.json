{
  "category": "research",
  "date": "2026-01-16",
  "category_summary": "Today's research features significant advances in open VLMs, safety evaluation, and theoretical understanding of neural networks. **Molmo2** from AI2 [achieves state-of-the-art](/?date=2026-01-16&category=research#item-e865ed62da32) among open vision-language models with novel video understanding and point-driven grounding capabilities.\n\n- A comprehensive **safety report** [evaluates 7 frontier models](/?date=2026-01-16&category=research#item-6c69a8d17a75) including **GPT-5.2**, **Gemini 3 Pro**, and **Grok 4.1** across language, vision, and image generation modalities\n- **OpenRouter** analysis of **100+ trillion tokens** [reveals unprecedented insights](/?date=2026-01-16&category=research#item-4ccb874eae2d) into real-world LLM usage patterns across tasks and geographies\n- Mechanistic analysis [exposes surprising failure modes](/?date=2026-01-16&category=research#item-1d227e067b0f) in **Hierarchical Reasoning Models**, including failures on simple puzzles due to fixed-point property violations\n\nTheoretical contributions include [proving transformers operate](/?date=2026-01-16&category=research#item-8df66c147511) as **tropical polynomial circuits** in the max-plus algebra, and demonstrating that [neural scaling laws emerge](/?date=2026-01-16&category=research#item-e9ccb8b58436) from random graph structure independent of power-law distributions. **CaMeLs** [introduces Single-Shot Planning](/?date=2026-01-16&category=research#item-7a483d653b37) to resolve security vulnerabilities in computer-use agents, while **GRACE** [proposes neuro-symbolic containment](/?date=2026-01-16&category=research#item-99cdfec7e509) separating normative from instrumental reasoning. **ML-Master 2.0** [advances ultra-long-horizon](/?date=2026-01-16&category=research#item-188a6bb2adad) autonomous ML engineering through cognitive accumulation.",
  "category_summary_html": "<p>Today's research features significant advances in open VLMs, safety evaluation, and theoretical understanding of neural networks. <strong>Molmo2</strong> from AI2 <a href=\"/?date=2026-01-16&category=research#item-e865ed62da32\" class=\"internal-link\">achieves state-of-the-art</a> among open vision-language models with novel video understanding and point-driven grounding capabilities.</p>\n<ul>\n<li>A comprehensive <strong>safety report</strong> <a href=\"/?date=2026-01-16&category=research#item-6c69a8d17a75\" class=\"internal-link\">evaluates 7 frontier models</a> including <strong>GPT-5.2</strong>, <strong>Gemini 3 Pro</strong>, and <strong>Grok 4.1</strong> across language, vision, and image generation modalities</li>\n<li><strong>OpenRouter</strong> analysis of <strong>100+ trillion tokens</strong> <a href=\"/?date=2026-01-16&category=research#item-4ccb874eae2d\" class=\"internal-link\">reveals unprecedented insights</a> into real-world LLM usage patterns across tasks and geographies</li>\n<li>Mechanistic analysis <a href=\"/?date=2026-01-16&category=research#item-1d227e067b0f\" class=\"internal-link\">exposes surprising failure modes</a> in <strong>Hierarchical Reasoning Models</strong>, including failures on simple puzzles due to fixed-point property violations</li>\n</ul>\n<p>Theoretical contributions include <a href=\"/?date=2026-01-16&category=research#item-8df66c147511\" class=\"internal-link\">proving transformers operate</a> as <strong>tropical polynomial circuits</strong> in the max-plus algebra, and demonstrating that <a href=\"/?date=2026-01-16&category=research#item-e9ccb8b58436\" class=\"internal-link\">neural scaling laws emerge</a> from random graph structure independent of power-law distributions. <strong>CaMeLs</strong> <a href=\"/?date=2026-01-16&category=research#item-7a483d653b37\" class=\"internal-link\">introduces Single-Shot Planning</a> to resolve security vulnerabilities in computer-use agents, while <strong>GRACE</strong> <a href=\"/?date=2026-01-16&category=research#item-99cdfec7e509\" class=\"internal-link\">proposes neuro-symbolic containment</a> separating normative from instrumental reasoning. <strong>ML-Master 2.0</strong> <a href=\"/?date=2026-01-16&category=research#item-188a6bb2adad\" class=\"internal-link\">advances ultra-long-horizon</a> autonomous ML engineering through cognitive accumulation.</p>",
  "themes": [
    {
      "name": "AI Safety & Alignment",
      "description": "Research on making AI systems safe, secure, and aligned with human values, including jailbreak defense, safety evaluation, ethical reasoning, and containment architectures",
      "item_count": 30,
      "example_items": [],
      "importance": 88
    },
    {
      "name": "Language Models & Transformers",
      "description": "Research on transformer architectures, LLM capabilities, scaling laws, and theoretical understanding of attention mechanisms",
      "item_count": 15,
      "example_items": [],
      "importance": 85
    },
    {
      "name": "LLM Agents & Autonomy",
      "description": "Development of autonomous AI agents for long-horizon tasks, computer use, scientific research, and multi-agent coordination",
      "item_count": 12,
      "example_items": [],
      "importance": 82
    },
    {
      "name": "Reasoning & Chain-of-Thought",
      "description": "Improvements to LLM reasoning capabilities including test-time scaling, structured reasoning, and mechanistic analysis of reasoning models",
      "item_count": 10,
      "example_items": [],
      "importance": 80
    },
    {
      "name": "Reasoning & Knowledge Distillation",
      "description": "Improving reasoning capabilities through distillation, chain-of-thought optimization, and data-efficient training methods",
      "item_count": 6,
      "example_items": [],
      "importance": 78
    },
    {
      "name": "Interpretability & Mechanistic Analysis",
      "description": "Understanding how models work internally, diagnosing failures, and explaining model behavior",
      "item_count": 5,
      "example_items": [],
      "importance": 76
    },
    {
      "name": "Multi-Agent Systems",
      "description": "Frameworks for coordinating multiple AI agents, including communication protocols, orchestration, and collective behavior",
      "item_count": 9,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "AI Safety and Alignment",
      "description": "Safety preservation during fine-tuning, alignment pretraining effects, prompt injection defense, and cross-lingual moral alignment",
      "item_count": 8,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "Multimodal & Vision-Language",
      "description": "Vision-language models, cross-modal learning, and multimodal foundation models",
      "item_count": 6,
      "example_items": [],
      "importance": 75
    },
    {
      "name": "LLM Interpretability",
      "description": "Understanding internal representations, persona vectors, attention mechanisms, and confidence calibration in language models",
      "item_count": 9,
      "example_items": [],
      "importance": 75
    }
  ],
  "total_items": 30,
  "items": [
    {
      "id": "e865ed62da32",
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "content": "arXiv:2601.10611v1 Announce Type: cross  Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight...",
      "url": "http://arxiv.org/abs/2601.10611",
      "author": "Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Presents Molmo2, a state-of-the-art open-weight vision-language model family with video understanding and point-driven grounding capabilities. Notably releases both weights and full training data/recipe.",
      "importance_score": 88,
      "reasoning": "Major contribution from AI2 to open VLM research. State-of-the-art among open models with novel grounding capabilities. Full transparency on data and training enables reproducibility and further research.",
      "themes": [
        "Vision-Language Models",
        "Open Source",
        "Video Understanding",
        "Multimodal AI"
      ],
      "continuation": null,
      "summary_html": "<p>Presents Molmo2, a state-of-the-art open-weight vision-language model family with video understanding and point-driven grounding capabilities. Notably releases both weights and full training data/recipe.</p>",
      "content_html": "<p>arXiv:2601.10611v1 Announce Type: cross  Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight...</p>"
    },
    {
      "id": "6c69a8d17a75",
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "content": "arXiv:2601.10527v1 Announce Type: new  Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger...",
      "url": "http://arxiv.org/abs/2601.10527",
      "author": "Xingjun Ma, Yixu Wang, Hengyuan Xu, Yutao Wu, Yifan Ding, Yunhan Zhao, Zilong Wang, Jiabin Hua, Ming Wen, Jianan Liu, Ranjie Duan, Yifeng Gao, Yingshui Tan, Yunhao Chen, Hui Xue, Xin Wang, Wei Cheng, Jingjing Chen, Zuxuan Wu, Bo Li, Yu-Gang Jiang",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Comprehensive safety evaluation of 7 frontier models (GPT-5.2, Gemini 3 Pro, etc.) across language, vision-language, and image generation using unified protocol with adversarial and multilingual evaluation.",
      "importance_score": 85,
      "reasoning": "Critical safety benchmarking of frontier models. Integrated evaluation across modalities and threat models is valuable. Important timing as models become more capable. (Note: some model names appear unreleased)",
      "themes": [
        "AI Safety",
        "Model Evaluation",
        "Frontier Models",
        "Adversarial Robustness"
      ],
      "continuation": null,
      "summary_html": "<p>Comprehensive safety evaluation of 7 frontier models (GPT-5.2, Gemini 3 Pro, etc.) across language, vision-language, and image generation using unified protocol with adversarial and multilingual evaluation.</p>",
      "content_html": "<p>arXiv:2601.10527v1 Announce Type: new  Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger...</p>"
    },
    {
      "id": "1d227e067b0f",
      "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
      "content": "arXiv:2601.10679v1 Announce Type: new  Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning...",
      "url": "http://arxiv.org/abs/2601.10679",
      "author": "Zirui Ren, Ziming Liu",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Mechanistic analysis of Hierarchical Reasoning Models revealing surprising failure modes: failure on simple puzzles due to fixed-point property violation, 'grokking' dynamics in reasoning, and multiple fixed points leading to 'guessing'.",
      "importance_score": 83,
      "reasoning": "Important interpretability work revealing fundamental limitations of HRMs. Three key findings provide actionable insights for model improvement. Critical for understanding reasoning model reliability.",
      "themes": [
        "Interpretability",
        "Reasoning Models",
        "Mechanistic Analysis",
        "Model Failures"
      ],
      "continuation": null,
      "summary_html": "<p>Mechanistic analysis of Hierarchical Reasoning Models revealing surprising failure modes: failure on simple puzzles due to fixed-point property violation, 'grokking' dynamics in reasoning, and multiple fixed points leading to 'guessing'.</p>",
      "content_html": "<p>arXiv:2601.10679v1 Announce Type: new  Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning...</p>"
    },
    {
      "id": "7a483d653b37",
      "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
      "content": "arXiv:2601.09923v1 Announce Type: new  Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can...",
      "url": "http://arxiv.org/abs/2601.09923",
      "author": "Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina Nikoli\\'c, Florian Tram\\`er, Ilia Shumailov, Cheng Zhang, Yiren Zhao",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Addresses security vulnerabilities in Computer Use Agents (CUAs) by introducing Single-Shot Planning, which resolves the tension between continuous UI observation and architectural isolation needed to prevent prompt injection attacks.",
      "importance_score": 82,
      "reasoning": "Critical security research for increasingly deployed computer-use agents. Novel insight that UI workflows are structurally predictable enables secure single-shot planning. Addresses fundamental security challenge.",
      "themes": [
        "AI Safety",
        "Computer Use Agents",
        "Prompt Injection",
        "Security"
      ],
      "continuation": null,
      "summary_html": "<p>Addresses security vulnerabilities in Computer Use Agents (CUAs) by introducing Single-Shot Planning, which resolves the tension between continuous UI observation and architectural isolation needed to prevent prompt injection attacks.</p>",
      "content_html": "<p>arXiv:2601.09923v1 Announce Type: new  Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can...</p>"
    },
    {
      "id": "e9ccb8b58436",
      "title": "On the origin of neural scaling laws: from random graphs to natural language",
      "content": "arXiv:2601.10684v1 Announce Type: cross  Abstract: Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd\\\"os-Renyi and scale-free Barab\\'asi-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining...",
      "url": "http://arxiv.org/abs/2601.10684",
      "author": "Maissam Barkeshli, Alberto Alfarano, Andrey Gromov",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Studies neural scaling laws using transformers trained on random walks on graphs, demonstrating that scaling laws emerge even without power-law structure in data. Provides theoretical insights into the origin of scaling laws.",
      "importance_score": 82,
      "reasoning": "Important theoretical contribution to understanding scaling laws, a fundamental phenomenon in modern AI. Controlled experimental setup enables causal insights. Highly relevant to LLM research direction.",
      "themes": [
        "Scaling Laws",
        "Theory",
        "Language Models",
        "Transformers"
      ],
      "continuation": null,
      "summary_html": "<p>Studies neural scaling laws using transformers trained on random walks on graphs, demonstrating that scaling laws emerge even without power-law structure in data. Provides theoretical insights into the origin of scaling laws.</p>",
      "content_html": "<p>arXiv:2601.10684v1 Announce Type: cross  Abstract: Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd\\\"os-Renyi and scale-free Barab\\'asi-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining...</p>"
    },
    {
      "id": "99cdfec7e509",
      "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
      "content": "arXiv:2601.10520v1 Announce Type: new  Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
      "url": "http://arxiv.org/abs/2601.10520",
      "author": "Felix Jahn, Yannic Muskalla, Lisa Dargasz, Patrick Schramowski, Kevin Baum",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces GRACE, a neuro-symbolic containment architecture that separates normative reasoning (Moral Module) from instrumental decision-making, enabling containment of AI agents of any design.",
      "importance_score": 81,
      "reasoning": "Important safety architecture contribution. Principled separation of normative and instrumental reasoning addresses fundamental alignment challenge. Deontic logic grounding provides formal foundation.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Neuro-Symbolic AI",
        "Ethical AI"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces GRACE, a neuro-symbolic containment architecture that separates normative reasoning (Moral Module) from instrumental decision-making, enabling containment of AI agents of any design.</p>",
      "content_html": "<p>arXiv:2601.10520v1 Announce Type: new  Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.</p>"
    },
    {
      "id": "4ccb874eae2d",
      "title": "State of AI: An Empirical 100 Trillion Token Study with OpenRouter",
      "content": "arXiv:2601.10088v1 Announce Type: new  Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage...",
      "url": "http://arxiv.org/abs/2601.10088",
      "author": "Malika Aubakirova, Alex Atallah, Chris Clark, Justin Summerville, Anjney Midha",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Large-scale empirical analysis of 100+ trillion tokens of real LLM interactions via OpenRouter, examining usage patterns across tasks, geographies, and time, including the shift to reasoning models like o1.",
      "importance_score": 80,
      "reasoning": "Unprecedented scale of real-world LLM usage data analysis. Valuable industry insights on how models are actually used, timing (post-o1 era), and practical deployment patterns. Unique data access.",
      "themes": [
        "LLM Usage Analysis",
        "Industry Trends",
        "Reasoning Models",
        "Empirical AI Research"
      ],
      "continuation": null,
      "summary_html": "<p>Large-scale empirical analysis of 100+ trillion tokens of real LLM interactions via OpenRouter, examining usage patterns across tasks, geographies, and time, including the shift to reasoning models like o1.</p>",
      "content_html": "<p>arXiv:2601.10088v1 Announce Type: new  Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage...</p>"
    },
    {
      "id": "1d4308c3ac20",
      "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
      "content": "arXiv:2601.10543v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at:...",
      "url": "http://arxiv.org/abs/2601.10543",
      "author": "Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes using latent safety-related signals during LLM decoding to defend against jailbreaks, observing that models internally exhibit safety signals even when successfully jailbroken.",
      "importance_score": 80,
      "reasoning": "Novel and important observation about latent safety signals. Practical defense mechanism that leverages internal representations. Addresses real vulnerability of aligned models.",
      "themes": [
        "AI Safety",
        "Jailbreak Defense",
        "Interpretability",
        "LLM Security"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes using latent safety-related signals during LLM decoding to defend against jailbreaks, observing that models internally exhibit safety signals even when successfully jailbroken.</p>",
      "content_html": "<p>arXiv:2601.10543v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at:...</p>"
    },
    {
      "id": "8df66c147511",
      "title": "The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit",
      "content": "arXiv:2601.09775v1 Announce Type: new  Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($\\beta \\to \\infty$, where $\\beta$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.",
      "url": "http://arxiv.org/abs/2601.09775",
      "author": "Faruk Alpay, Bilge Senturk",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proves that transformer self-attention in high-confidence regime operates in the tropical semiring (max-plus algebra), revealing that forward pass executes Bellman-Ford path-finding on token similarity graphs. Provides geometric interpretation of chain-of-thought reasoning.",
      "importance_score": 80,
      "reasoning": "Novel theoretical insight connecting transformers to dynamic programming algorithms. Elegant mathematical framework with potential implications for understanding reasoning emergence.",
      "themes": [
        "Theory",
        "Transformers",
        "Chain-of-Thought",
        "Mathematical Foundations"
      ],
      "continuation": null,
      "summary_html": "<p>Proves that transformer self-attention in high-confidence regime operates in the tropical semiring (max-plus algebra), revealing that forward pass executes Bellman-Ford path-finding on token similarity graphs. Provides geometric interpretation of chain-of-thought reasoning.</p>",
      "content_html": "<p>arXiv:2601.09775v1 Announce Type: new  Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($\\beta \\to \\infty$, where $\\beta$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.</p>"
    },
    {
      "id": "188a6bb2adad",
      "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
      "content": "arXiv:2601.10402v1 Announce Type: new  Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable...",
      "url": "http://arxiv.org/abs/2601.10402",
      "author": "Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Presents ML-Master 2.0, an autonomous agent for ultra-long-horizon ML engineering that uses cognitive accumulation to maintain strategic coherence over days/weeks of experimental cycles.",
      "importance_score": 79,
      "reasoning": "Important advancement for autonomous scientific agents. Ultra-long-horizon autonomy is a key frontier. Cognitive accumulation addresses real context management challenge.",
      "themes": [
        "Autonomous Agents",
        "AI for Science",
        "Long-Horizon Planning",
        "ML Engineering"
      ],
      "continuation": null,
      "summary_html": "<p>Presents ML-Master 2.0, an autonomous agent for ultra-long-horizon ML engineering that uses cognitive accumulation to maintain strategic coherence over days/weeks of experimental cycles.</p>",
      "content_html": "<p>arXiv:2601.10402v1 Announce Type: new  Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable...</p>"
    },
    {
      "id": "402ae431f307",
      "title": "Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models",
      "content": "arXiv:2601.09855v1 Announce Type: new  Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.",
      "url": "http://arxiv.org/abs/2601.09855",
      "author": "Michael R. Metel, Yufei Cui, Boxing Chen, Prasanna Parthasarathi",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Introduces Min-Seek, a training-free test-time scaling method that stabilizes reasoning model accuracy across extended thinking lengths, addressing the problem where longer reasoning can degrade performance. Claims to eliminate need for reasoning length tuning.",
      "importance_score": 78,
      "reasoning": "Important practical contribution to test-time compute scaling. Addresses a real problem (accuracy degradation with longer reasoning) that limits current reasoning models. Training-free approach is valuable for deployment.",
      "themes": [
        "Test-Time Scaling",
        "Reasoning Models",
        "LLM Efficiency"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Min-Seek, a training-free test-time scaling method that stabilizes reasoning model accuracy across extended thinking lengths, addressing the problem where longer reasoning can degrade performance. Claims to eliminate need for reasoning length tuning.</p>",
      "content_html": "<p>arXiv:2601.09855v1 Announce Type: new  Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.</p>"
    },
    {
      "id": "ccee4d6880d2",
      "title": "Transition Matching Distillation for Fast Video Generation",
      "content": "arXiv:2601.09881v1 Announce Type: cross  Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models...",
      "url": "http://arxiv.org/abs/2601.09881",
      "author": "Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie, Arash Vahdat",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Transition Matching Distillation (TMD) distills video diffusion models into few-step generators by matching multi-step trajectories with lightweight conditional flows. Decomposes backbone for efficiency.",
      "importance_score": 78,
      "reasoning": "NVIDIA authors (Arash Vahdat, Weili Nie) with track record in diffusion. Novel distillation framework for practical video generation acceleration.",
      "themes": [
        "Video Generation",
        "Diffusion Models",
        "Model Distillation"
      ],
      "continuation": null,
      "summary_html": "<p>Transition Matching Distillation (TMD) distills video diffusion models into few-step generators by matching multi-step trajectories with lightweight conditional flows. Decomposes backbone for efficiency.</p>",
      "content_html": "<p>arXiv:2601.09881v1 Announce Type: cross  Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models...</p>"
    },
    {
      "id": "ad92ec3f545a",
      "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models",
      "content": "arXiv:2601.10460v1 Announce Type: cross  Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.   We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.   We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.   The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.",
      "url": "http://arxiv.org/abs/2601.10460",
      "author": "Abhinaba Basu, Pavan Chakraborty",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Demonstrates that LLM bias measurements shift dramatically with contextual framing (time, audience, setting) without adversarial prompting. Testing 13 models shows anchoring to 1990 vs 2030 significantly raises stereotype selection.",
      "importance_score": 78,
      "reasoning": "Important finding for AI safety showing that bias benchmarks may not predict deployment behavior. Rigorous methodology across 13 models with statistical significance. Challenges current bias evaluation practices.",
      "themes": [
        "AI Safety",
        "Bias Evaluation",
        "Language Models",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Demonstrates that LLM bias measurements shift dramatically with contextual framing (time, audience, setting) without adversarial prompting. Testing 13 models shows anchoring to 1990 vs 2030 significantly raises stereotype selection.</p>",
      "content_html": "<p>arXiv:2601.10460v1 Announce Type: cross  Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.   We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.   We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.   The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.</p>"
    },
    {
      "id": "345c206d1ac2",
      "title": "Discrete Feynman-Kac Correctors",
      "content": "arXiv:2601.10403v1 Announce Type: new  Abstract: Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.",
      "url": "http://arxiv.org/abs/2601.10403",
      "author": "Mohsin Hasan, Viktor Ohanesian, Artem Gazizov, Yoshua Bengio, Al\\'an Aspuru-Guzik, Roberto Bondesan, Marta Skreta, Kirill Neklyudov",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes Discrete Feynman-Kac Correctors for controlling distribution of discrete masked diffusion models at inference time, enabling temperature control and conditional sampling via SMC.",
      "importance_score": 78,
      "reasoning": "Novel framework for inference-time control of discrete diffusions. Addresses important gap in controllable generation. Mathematically grounded approach.",
      "themes": [
        "Diffusion Models",
        "Discrete Generation",
        "Inference-time Control",
        "Sequential Monte Carlo"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Discrete Feynman-Kac Correctors for controlling distribution of discrete masked diffusion models at inference time, enabling temperature control and conditional sampling via SMC.</p>",
      "content_html": "<p>arXiv:2601.10403v1 Announce Type: new  Abstract: Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.</p>"
    },
    {
      "id": "c25afc761321",
      "title": "Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay",
      "content": "arXiv:2601.10589v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach...",
      "url": "http://arxiv.org/abs/2601.10589",
      "author": "Hao Wang, Yanting Wang, Hao Li, Rui Li, Lei Sha",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CR"
      ],
      "summary": "Safety Self-Play (SSP) enables LLMs to be their own red teamers, using a single model as both attacker and defender with reflective experience replay for autonomous evolving safety alignment.",
      "importance_score": 78,
      "reasoning": "Novel and important approach to safety alignment. Addresses limitation of static red teaming with autonomous adaptation. Significant for AI safety.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Red Teaming",
        "Self-Play"
      ],
      "continuation": null,
      "summary_html": "<p>Safety Self-Play (SSP) enables LLMs to be their own red teamers, using a single model as both attacker and defender with reflective experience replay for autonomous evolving safety alignment.</p>",
      "content_html": "<p>arXiv:2601.10589v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach...</p>"
    },
    {
      "id": "0cf674e11b58",
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "content": "arXiv:2601.10553v1 Announce Type: new  Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
      "url": "http://arxiv.org/abs/2601.10553",
      "author": "Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall, Reyhane Askari-Hemmat, Xiaochuang Han, Nicolas Ballas, Michal Drozdzal, Adriana Romero-Soriano",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces WMReward, treating physics plausibility in video generation as an inference-time alignment problem by using VJEPA-2 as a latent world model reward to guide denoising trajectories, enabling test-time compute scaling.",
      "importance_score": 78,
      "reasoning": "Important contribution leveraging world models for video generation physics alignment. Uses VJEPA-2 (Meta's model) as reward signal. Novel inference-time approach could significantly improve video generation quality.",
      "themes": [
        "Video Generation",
        "World Models",
        "Physics Simulation",
        "Inference-time Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces WMReward, treating physics plausibility in video generation as an inference-time alignment problem by using VJEPA-2 as a latent world model reward to guide denoising trajectories, enabling test-time compute scaling.</p>",
      "content_html": "<p>arXiv:2601.10553v1 Announce Type: new  Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.</p>"
    },
    {
      "id": "d46dc901c311",
      "title": "TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks",
      "content": "arXiv:2601.10245v1 Announce Type: new  Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All...",
      "url": "http://arxiv.org/abs/2601.10245",
      "author": "Vansh Kapoor, Aman Gupta, Hao Chen, Anurag Beniwal, Jing Huang, Aviral Kumar",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes TRIM, which routes only critical reasoning steps (likely to cause cascading failures) to larger models while smaller models handle routine steps, achieving efficiency without sacrificing accuracy.",
      "importance_score": 77,
      "reasoning": "Novel and practical efficiency improvement for reasoning. Step-level routing is more granular than query-level routing. Addresses real cost concerns for reasoning models.",
      "themes": [
        "Model Routing",
        "Reasoning Efficiency",
        "Multi-Model Systems",
        "Test-Time Compute"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes TRIM, which routes only critical reasoning steps (likely to cause cascading failures) to larger models while smaller models handle routine steps, achieving efficiency without sacrificing accuracy.</p>",
      "content_html": "<p>arXiv:2601.10245v1 Announce Type: new  Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All...</p>"
    },
    {
      "id": "3a585fc9f804",
      "title": "Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning",
      "content": "arXiv:2601.10306v1 Announce Type: new  Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.",
      "url": "http://arxiv.org/abs/2601.10306",
      "author": "Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes EAPO (Evidence-Augmented Policy Optimization) for long-context reasoning, introducing Group-Relative Evidence Reward to provide dense supervision for evidence extraction, addressing sparse reward limitations.",
      "importance_score": 76,
      "reasoning": "Important contribution to RL for long-context reasoning. Evidence-augmented paradigm addresses critical bottleneck (needle-in-haystack retrieval). Novel reward design.",
      "themes": [
        "Reinforcement Learning",
        "Long-Context Reasoning",
        "Policy Optimization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes EAPO (Evidence-Augmented Policy Optimization) for long-context reasoning, introducing Group-Relative Evidence Reward to provide dense supervision for evidence extraction, addressing sparse reward limitations.</p>",
      "content_html": "<p>arXiv:2601.10306v1 Announce Type: new  Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.</p>"
    },
    {
      "id": "3d007893235a",
      "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines",
      "content": "arXiv:2601.09714v1 Announce Type: cross  Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.",
      "url": "http://arxiv.org/abs/2601.09714",
      "author": "Devesh Saraogi, Rohit Singhee, Dhruv Kumar",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Benchmarks five reasoning architectures (Reflection, Sakana AI v2, Google Co-Scientist, GPT Deep Research, Claude Opus 4) for research plan novelty, investigating whether agentic workflows overcome 'smart plagiarism'.",
      "importance_score": 76,
      "reasoning": "Timely and important evaluation of AI for science. Addresses critical question about AI-generated research originality. Comparison of major systems is valuable.",
      "themes": [
        "AI for Science",
        "Research Agents",
        "Novelty Evaluation",
        "Agentic Workflows"
      ],
      "continuation": null,
      "summary_html": "<p>Benchmarks five reasoning architectures (Reflection, Sakana AI v2, Google Co-Scientist, GPT Deep Research, Claude Opus 4) for research plan novelty, investigating whether agentic workflows overcome 'smart plagiarism'.</p>",
      "content_html": "<p>arXiv:2601.09714v1 Announce Type: cross  Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.</p>"
    },
    {
      "id": "2b4a641a9668",
      "title": "High-accuracy and dimension-free sampling with diffusions",
      "content": "arXiv:2601.10708v1 Announce Type: new  Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.   More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \\emph{polylogarithmically} in $1/\\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \\emph{effective radius} of the support of the target distribution only.",
      "url": "http://arxiv.org/abs/2601.10708",
      "author": "Khashayar Gatmiry, Sitan Chen, Adil Salim",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes new diffusion model solver achieving dimension-free sampling complexity using low-degree approximation and collocation methods, improving over polynomial dimension dependence.",
      "importance_score": 76,
      "reasoning": "Important theoretical and practical contribution to diffusion sampling. Dimension-free complexity is significant for high-dimensional applications.",
      "themes": [
        "Diffusion Models",
        "Sampling",
        "Theory",
        "Algorithms"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes new diffusion model solver achieving dimension-free sampling complexity using low-degree approximation and collocation methods, improving over polynomial dimension dependence.</p>",
      "content_html": "<p>arXiv:2601.10708v1 Announce Type: new  Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.   More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \\emph{polylogarithmically} in $1/\\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \\emph{effective radius} of the support of the target distribution only.</p>"
    },
    {
      "id": "73da185d0734",
      "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
      "content": "arXiv:2601.10156v1 Announce Type: new  Abstract: While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
      "url": "http://arxiv.org/abs/2601.10156",
      "author": "Yutao Mou, Zhangchi Xue, Lijun Li, Peiyang Liu, Shikun Zhang, Wei Ye, Jing Shao",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Computation and Language)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "ToolSafe provides step-level tool invocation safety detection for LLM agents, introducing TS-Bench benchmark and TS-Guard guardrail model using multi-task RL to proactively detect unsafe actions before execution.",
      "importance_score": 76,
      "reasoning": "Critical for safe LLM agent deployment. Novel proactive safety approach with both benchmark and detection model. Important for AI safety.",
      "themes": [
        "AI Safety",
        "LLM Agents",
        "Tool Use",
        "Guardrails"
      ],
      "continuation": null,
      "summary_html": "<p>ToolSafe provides step-level tool invocation safety detection for LLM agents, introducing TS-Bench benchmark and TS-Guard guardrail model using multi-task RL to proactively detect unsafe actions before execution.</p>",
      "content_html": "<p>arXiv:2601.10156v1 Announce Type: new  Abstract: While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.</p>"
    },
    {
      "id": "cae97f4f5a57",
      "title": "Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs",
      "content": "arXiv:2601.10114v1 Announce Type: new  Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.",
      "url": "http://arxiv.org/abs/2601.10114",
      "author": "Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Proposes Scheduled Checkpoint Distillation (SCD) with theoretical insight that students can outperform teachers if advantage on Student-Favored Subdomain exceeds deficit on Teacher-Favored Subdomain.",
      "importance_score": 75,
      "reasoning": "Novel theoretical contribution explaining when/how smaller models can exceed teacher performance. Practical implications for efficient model deployment. Well-motivated approach.",
      "themes": [
        "Knowledge Distillation",
        "Model Compression",
        "LLM Training"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Scheduled Checkpoint Distillation (SCD) with theoretical insight that students can outperform teachers if advantage on Student-Favored Subdomain exceeds deficit on Teacher-Favored Subdomain.</p>",
      "content_html": "<p>arXiv:2601.10114v1 Announce Type: new  Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.</p>"
    },
    {
      "id": "094b68e3a74b",
      "title": "Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions",
      "content": "arXiv:2601.09724v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with \"should not.\" We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical...",
      "url": "http://arxiv.org/abs/2601.09724",
      "author": "Katherine Elkins, Jon Chun",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces Syntactic Framing Fragility (SFF) framework with Logical Polarity Normalization to audit whether LLMs maintain consistent ethical judgments across syntactically equivalent prompts with different negation/conditional structures.",
      "importance_score": 75,
      "reasoning": "Important robustness audit methodology for AI ethics. SFF framework is novel and principled. Reveals concerning instabilities in ethical reasoning across 23 models.",
      "themes": [
        "AI Ethics",
        "Robustness",
        "LLM Evaluation",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Syntactic Framing Fragility (SFF) framework with Logical Polarity Normalization to audit whether LLMs maintain consistent ethical judgments across syntactically equivalent prompts with different negation/conditional structures.</p>",
      "content_html": "<p>arXiv:2601.09724v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with \"should not.\" We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical...</p>"
    },
    {
      "id": "c2fd2cbf55b2",
      "title": "Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment",
      "content": "arXiv:2601.10160v1 Announce Type: cross  Abstract: Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai",
      "url": "http://arxiv.org/abs/2601.10160",
      "author": "Cameron Tice, Puria Radmard, Samuel Ratnam, Andy Kim, David Africa, Kyle O'Brien",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Studies how AI discourse in pretraining data causally influences alignment. Finds upsampling misalignment discussion increases misaligned behavior; aligned discourse reduces misalignment by ~10 points.",
      "importance_score": 75,
      "reasoning": "First controlled study of self-fulfilling alignment. 6.9B parameter pretraining experiments provide strong evidence. Important implications for data curation.",
      "themes": [
        "AI Safety",
        "Alignment",
        "Pretraining"
      ],
      "continuation": null,
      "summary_html": "<p>Studies how AI discourse in pretraining data causally influences alignment. Finds upsampling misalignment discussion increases misaligned behavior; aligned discourse reduces misalignment by ~10 points.</p>",
      "content_html": "<p>arXiv:2601.10160v1 Announce Type: cross  Abstract: Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai</p>"
    },
    {
      "id": "5959dd6a1f75",
      "title": "Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning",
      "content": "arXiv:2601.10498v1 Announce Type: cross  Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.",
      "url": "http://arxiv.org/abs/2601.10498",
      "author": "Nilin Abrahamsen",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.LG"
      ],
      "summary": "Proposes PROMA, a proximal policy update method for LLM fine-tuning that accumulates gradients by projecting out sequence-wise components. Claims tighter KL control than GRPO without entropy collapse or reference policy.",
      "importance_score": 75,
      "reasoning": "Novel approach to policy optimization that addresses known issues with GRPO and PPO. Computationally efficient with no additional forward/backward passes. Relevant to LLM alignment.",
      "themes": [
        "Reinforcement Learning",
        "LLM Fine-tuning",
        "Policy Optimization",
        "Alignment"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes PROMA, a proximal policy update method for LLM fine-tuning that accumulates gradients by projecting out sequence-wise components. Claims tighter KL control than GRPO without entropy collapse or reference policy.</p>",
      "content_html": "<p>arXiv:2601.10498v1 Announce Type: cross  Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.</p>"
    },
    {
      "id": "c37c1da359b5",
      "title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure",
      "content": "arXiv:2601.10566v1 Announce Type: cross  Abstract: Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting...",
      "url": "http://arxiv.org/abs/2601.10566",
      "author": "Syed Naveed Mahmood, Md. Rezaur Rahman Bhuiyan, Tasfia Zaman, Jareen Tasneem Khondaker, Md. Sameer Sakib, Nazia Tasnim, Farig Sadeque",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Machine Learning)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Introduces Knowledge Immunization Framework (KIF) for genuine knowledge erasure from LLMs by targeting internal activation signatures rather than surface outputs. Achieves near-oracle erasure while preserving model utility for GDPR compliance.",
      "importance_score": 75,
      "reasoning": "Addresses critical problem of machine unlearning for privacy/safety. Novel representation-aware approach distinguishes true erasure from superficial suppression. Important for AI governance.",
      "themes": [
        "AI Safety",
        "Machine Unlearning",
        "Privacy",
        "LLM Interpretability"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Knowledge Immunization Framework (KIF) for genuine knowledge erasure from LLMs by targeting internal activation signatures rather than surface outputs. Achieves near-oracle erasure while preserving model utility for GDPR compliance.</p>",
      "content_html": "<p>arXiv:2601.10566v1 Announce Type: cross  Abstract: Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting...</p>"
    },
    {
      "id": "baee630fac46",
      "title": "Action100M: A Large-scale Video Action Dataset",
      "content": "arXiv:2601.10592v1 Announce Type: new  Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
      "url": "http://arxiv.org/abs/2601.10592",
      "author": "Delong Chen, Tejaswi Kasarla, Yejin Bang, Mustafa Shukor, Willy Chung, Jade Yu, Allen Bolourchi, Theo Moutakanni, Pascale Fung",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Computer Vision)",
      "source_type": "arxiv",
      "tags": [
        "cs.CV"
      ],
      "summary": "Introduces Action100M, a massive dataset of ~100 million temporally localized action segments from 1.2M instructional videos (14.6 years), generated using V-JEPA 2 embeddings and hierarchical segmentation with Tree-of-Captions.",
      "importance_score": 75,
      "reasoning": "Massive scale dataset contribution (100M segments). Leverages V-JEPA 2 from Meta. Fully automated pipeline could be influential for video understanding research. Open-vocabulary supervision is valuable.",
      "themes": [
        "Video Understanding",
        "Large-scale Datasets",
        "Action Recognition",
        "World Models"
      ],
      "continuation": null,
      "summary_html": "<p>Introduces Action100M, a massive dataset of ~100 million temporally localized action segments from 1.2M instructional videos (14.6 years), generated using V-JEPA 2 embeddings and hierarchical segmentation with Tree-of-Captions.</p>",
      "content_html": "<p>arXiv:2601.10592v1 Announce Type: new  Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.</p>"
    },
    {
      "id": "ae0d01a3f9c0",
      "title": "Continuum Memory Architectures for Long-Horizon LLM Agents",
      "content": "arXiv:2601.09913v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.",
      "url": "http://arxiv.org/abs/2601.09913",
      "author": "Joe Logan",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.AI"
      ],
      "summary": "Defines Continuum Memory Architecture (CMA) for LLM agents, specifying requirements for persistent storage, selective retention, associative routing, temporal chaining, and consolidationaddressing RAG's stateless limitations for long-horizon tasks.",
      "importance_score": 74,
      "reasoning": "Important architectural specification for long-horizon agents. Identifies real limitations of RAG (stateless, no temporal continuity) and proposes principled alternative, though implementation details limited.",
      "themes": [
        "LLM Agents",
        "Memory Architecture",
        "RAG",
        "Long-Horizon Planning"
      ],
      "continuation": null,
      "summary_html": "<p>Defines Continuum Memory Architecture (CMA) for LLM agents, specifying requirements for persistent storage, selective retention, associative routing, temporal chaining, and consolidationaddressing RAG's stateless limitations for long-horizon tasks.</p>",
      "content_html": "<p>arXiv:2601.09913v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.</p>"
    },
    {
      "id": "724ed597c31b",
      "title": "Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models",
      "content": "arXiv:2601.09719v1 Announce Type: cross  Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT",
      "url": "http://arxiv.org/abs/2601.09719",
      "author": "Hoyoon Byun, Youngjun Choi, Taero Kim, Sungrae Park, Kyungwoo Song",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes Bounded Hyperbolic Tanh (BHyT) as a drop-in replacement for Pre-Layer Normalization that couples tanh with explicit input bounding to prevent depth-related instability while improving efficiency.",
      "importance_score": 74,
      "reasoning": "Novel architectural component addressing real training stability issues. Drop-in replacement makes it practical. Addresses both efficiency and stability.",
      "themes": [
        "LLM Architecture",
        "Training Stability",
        "Normalization"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes Bounded Hyperbolic Tanh (BHyT) as a drop-in replacement for Pre-Layer Normalization that couples tanh with explicit input bounding to prevent depth-related instability while improving efficiency.</p>",
      "content_html": "<p>arXiv:2601.09719v1 Announce Type: cross  Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT</p>"
    },
    {
      "id": "5b72644b26d3",
      "title": "From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis",
      "content": "arXiv:2601.09734v1 Announce Type: cross  Abstract: Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary \"detection\" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from \"detection\" to \"diagnosis\". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance...",
      "url": "http://arxiv.org/abs/2601.09734",
      "author": "Yanyi Liu, Qingwen Yang, Tiezheng Guo, Feiyu Qu, Jun Liu, Yingyou Wen",
      "published": "2026-01-16T00:00:00-05:00",
      "source": "arXiv (Artificial Intelligence)",
      "source_type": "arxiv",
      "tags": [
        "cs.CL"
      ],
      "summary": "Proposes shifting hallucination research from detection to diagnosis, introducing task requiring error localization, causal explanation, and correction, with automated data synthesis methodology.",
      "importance_score": 74,
      "reasoning": "Important reframing of hallucination research toward actionable diagnosis. Comprehensive task definition is valuable. Automated synthesis enables scaling.",
      "themes": [
        "Hallucination",
        "Error Diagnosis",
        "LLM Reliability"
      ],
      "continuation": null,
      "summary_html": "<p>Proposes shifting hallucination research from detection to diagnosis, introducing task requiring error localization, causal explanation, and correction, with automated data synthesis methodology.</p>",
      "content_html": "<p>arXiv:2601.09734v1 Announce Type: cross  Abstract: Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary \"detection\" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from \"detection\" to \"diagnosis\". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance...</p>"
    }
  ]
}