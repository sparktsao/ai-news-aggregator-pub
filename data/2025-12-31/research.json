{
  "category": "research",
  "date": "2025-12-31",
  "category_summary": "A light batch for core AI research, with the strongest content in **AI alignment education** and **biotechnology methodology**.\n\n- **Advanced Intro to AI Alignment** [presents a structured 'thinking loop' framework](/?date=2025-12-31&category=research#item-4c5c0901c602) (search, predict, evaluate, iterate) for understanding goal-directed reasoning in AI systems\n- **Chromosome identification methods** [proposes novel techniques](/?date=2025-12-31&category=research#item-62ba0607fb53) for genome assembly from multiple source cells, enabling 'chromosome selection'\n- Meta-commentary on **Mechanize Work's Unfalsifiable Doom** essay [engages with epistemological debates](/?date=2025-12-31&category=research#item-50fa483389e8) around AI risk arguments\n- Compiler optimization analysis [offers useful framing](/?date=2025-12-31&category=research#item-dfd0bc9968f1) for human-AI collaboration dynamics relevant to AI coding assistants\n\nRemaining items cover **CFAR** [organizational updates](/?date=2025-12-31&category=research#item-aef929f278cd), rationality workshop curricula, education policy proposals for gifted children, and science history writing. No traditional ML papers or benchmark results in this batch.",
  "category_summary_html": "<p>A light batch for core AI research, with the strongest content in <strong>AI alignment education</strong> and <strong>biotechnology methodology</strong>.</p>\n<ul>\n<li><strong>Advanced Intro to AI Alignment</strong> <a href=\"/?date=2025-12-31&category=research#item-4c5c0901c602\" class=\"internal-link\">presents a structured 'thinking loop' framework</a> (search, predict, evaluate, iterate) for understanding goal-directed reasoning in AI systems</li>\n<li><strong>Chromosome identification methods</strong> <a href=\"/?date=2025-12-31&category=research#item-62ba0607fb53\" class=\"internal-link\">proposes novel techniques</a> for genome assembly from multiple source cells, enabling 'chromosome selection'</li>\n<li>Meta-commentary on <strong>Mechanize Work's Unfalsifiable Doom</strong> essay <a href=\"/?date=2025-12-31&category=research#item-50fa483389e8\" class=\"internal-link\">engages with epistemological debates</a> around AI risk arguments</li>\n<li>Compiler optimization analysis <a href=\"/?date=2025-12-31&category=research#item-dfd0bc9968f1\" class=\"internal-link\">offers useful framing</a> for human-AI collaboration dynamics relevant to AI coding assistants</li>\n</ul>\n<p>Remaining items cover <strong>CFAR</strong> <a href=\"/?date=2025-12-31&category=research#item-aef929f278cd\" class=\"internal-link\">organizational updates</a>, rationality workshop curricula, education policy proposals for gifted children, and science history writing. No traditional ML papers or benchmark results in this batch.</p>",
  "themes": [
    {
      "name": "AI Safety & Alignment",
      "description": "Content related to AI alignment research, goal-directed reasoning, and debates about AI existential risk arguments",
      "item_count": 2,
      "example_items": [],
      "importance": 50
    },
    {
      "name": "Biotechnology",
      "description": "Technical proposals in genomics and reproductive technology",
      "item_count": 2,
      "example_items": [],
      "importance": 40
    },
    {
      "name": "Programming & Human-AI Collaboration",
      "description": "Analysis of compiler optimization and parallels to AI coding tools",
      "item_count": 1,
      "example_items": [],
      "importance": 35
    },
    {
      "name": "CFAR & Rationality Community",
      "description": "Updates from CFAR organization, rationality training workshops, and community events",
      "item_count": 5,
      "example_items": [],
      "importance": 20
    },
    {
      "name": "Personal Finance & Philanthropy",
      "description": "Tax optimization and donation strategy advice for effective giving",
      "item_count": 3,
      "example_items": [],
      "importance": 15
    }
  ],
  "total_items": 14,
  "items": [
    {
      "id": "4c5c0901c602",
      "title": "[Advanced Intro to AI Alignment] 1. Goal-Directed Reasoning and Why It Matters",
      "content": "1.1 Summary and Table of ContentsWhy would an AI \"want\" anything? This post answers that question by examining a key part of the structure of intelligent cognition.When you solve a novel problem, your mind searches for plans, predicts their outcomes, evaluates whether those outcomes achieve what you want, and iterates. I call this the \"thinking loop\". We will build some intuition for why any AI capable of solving difficult real-world problems will need something structurally similar.This framework maps onto model-based reinforcement learning, where separate components predict outcomes (the model) and evaluate them (the critic). We'll use model-based RL as an important lens for analyzing alignment in this sequence—not because AGI will necessarily be built this way, but because analogous structure will be present in any very capable AI, and model-based RL provides a cleaner frame for examining difficulties and approaches.The post is organized as follows:Section 1.2 distinguishes goal-directed reasoning from habitual/heuristic reasoning, and explains how these interact. It also builds intuition for the thinking loop through everyday examples, and explains why it is necessary for solving novel problems.Section 1.3 connects this to model-based reinforcement learning - the main framework we will be using for analyzing alignment approaches in future posts.Section 1.4 explains that some behaviors are not easily compatible with goal-directed reasoning, including some we find intuitive and desirable.Section 1.5 argues that for most value functions, keeping humans alive isn't optimal. It considers why we might survive - the AI caring about us, successful trade, exotic possibilities - but concludes we mostly need to figure out how to point the values of an AI.Section 1.6 is a brief conclusion.1.2 Thinking ahead is useful1.2.1 Goal-Directed and Habitual ReasoningSuppose you want to drive to your doctor’s office. To get there, you need to get to your doctor’s town, for which you ...",
      "url": "https://www.lesswrong.com/posts/hDZm59uuzLwrG8gQq/advanced-intro-to-ai-alignment-1-goal-directed-reasoning-and",
      "author": "Towards_Keeperhood",
      "published": "2025-12-30T10:48:39.469000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "An educational introduction to AI alignment examining goal-directed reasoning through the 'thinking loop' framework (search, predict, evaluate, iterate). Connects these concepts to model-based reinforcement learning as a lens for understanding alignment challenges.",
      "importance_score": 52,
      "reasoning": "Substantive AI alignment educational content providing a structured introduction to goal-directed reasoning. Good pedagogical value but introductory rather than novel research. Part of a sequence suggesting more depth to come.",
      "themes": [
        "AI Alignment",
        "Reinforcement Learning",
        "AI Safety",
        "Goal-Directed AI"
      ],
      "continuation": null
    },
    {
      "id": "62ba0607fb53",
      "title": "Chromosome identification methods",
      "content": "PDF version. berkeleygenomics.org. x.com. bluesky. This is a linkpost for \"Chromosome identification methods\"; a few of the initial sections are reproduced here. Abstract Chromosome selection is a hypothetical technology that assembles the genome of a new living cell out of whole chromosomes taken from multiple source cells. To do chromosome selection, you need a method for chromosome identification—distinguishing between chromosomes by number, and ideally also by allele content. This article investigates methods for chromosome identification. It seems that existing methods are subject to a tradeoff where they either destroy or damage the chromosomes they measure, or else they fail to confidently identify chromosomes. A paradigm for non-destructive high-confidence chromosome identification is proposed, based on the idea of complementary identification. The idea is to isolate a single chromosome taken from a single cell, destructively identify all the remaining chromosomes from that cell, and thus infer the identity of the preserved chromosome. The overall aim is to eventually develop a non-destructive, low-cost, accurate way to identify single chromosomes, to apply as part of a chromosome selection protocol. Context Reprogenetics is biotechnology to empower parents to make genomic choices on behalf of their future children. One key operation that's needed for reprogenetics is genomic vectoring: creating a cell with a genome that's been modified in some specific direction. Chromosome selection is one possible genomic vectoring method. It could be fairly powerful if applied to sperm chromosomes or applied to multiple donors. The basic idea is to take several starting cells, select one or more chromosomes from each of those cells, and then put all those chromosomes together into one new cell: There are three fundamental elements needed to perform chromosome selection: Transmission and Exclusion. Get some chromosomes into the final cell, while excluding some other chrom...",
      "url": "https://www.lesswrong.com/posts/PneJmhC769dygaM8D/chromosome-identification-methods",
      "author": "TsviBT",
      "published": "2025-12-30T01:02:28.446000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Technical proposal for chromosome identification methods to enable 'chromosome selection' - assembling genomes from chromosomes of multiple source cells. Proposes 'complementary identification' paradigm: destructively identify all chromosomes except one to infer the preserved one's identity.",
      "importance_score": 45,
      "reasoning": "Novel technical proposal in genomics/biotech with clear methodology. Substantive research content, though not AI-related. Could have significant implications for reproductive technology.",
      "themes": [
        "Biotechnology",
        "Genomics",
        "Reproductive Technology"
      ],
      "continuation": null
    },
    {
      "id": "50fa483389e8",
      "title": "Mechanize Work's essay on Unfalsifiable Doom",
      "content": "Like&nbsp;Daniel Kokotajlo's coverage of Vitalik's response to AI-2027, I've copied the author's text. However, I would like to comment upon potential errors right in the text, since it would be clearer.Our critics tell us that our work will destroy the world.We want to engage with these critics, but there is no standard argument to respond to, no single text that unifies the AI safety community. Nonetheless, while this community lacks a central unifying argument, it does have a central figure: Eliezer Yudkowsky.Moreover, Yudkowsky, along with his colleague Nate Soares (hereafter Y&amp;S), have recently published&nbsp;a book. This new book comes closer than anything else to a canonical case for AI doom. It is titled “If Anyone Builds It, Everyone Dies”.Given the title, one would expect the book to be filled with evidence for why, if we build it, everyone will die. But it is not. To prove their case, Y&amp;S rely instead on vague theoretical arguments, illustrated through lengthy parables and analogies. Nearly every chapter either opens with an allegory or is itself a fictional story, with one of the book’s three parts consisting entirely of a story about a fictional AI named “Sable”.S.K.'s comment: these arguments are arguably easy to concentrate in a few phrases. Chapter 1 explains why mankind's special power is the ability to develop intelligence.&nbsp;Chapter 2 is supposed to convey the message that mankind's interpretability techniques usable for understanding the AI's mind are far from enough to understand why the mind does some action and not some other. Chapter 3 explains that the machines can optimize the world towards a state even more efficiently than the humans despite having no human-like parts. Chapter 4 explains that the AI's actual goals correlate, at best, with what was reinforced during training and not with the objective that the humans tried to instill. For example, the reward function of AIs like&nbsp;GPT-4o-sycophant was biased towards flatterin...",
      "url": "https://www.lesswrong.com/posts/i6sBAT4SPCJnBPKPJ/mechanize-work-s-essay-on-unfalsifiable-doom",
      "author": "StanislavKrym",
      "published": "2025-12-30T17:57:17.377000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Commentary and critique of Mechanize Work's essay responding to Yudkowsky & Soares' book 'If Anyone Builds It, Everyone Dies', arguing that AI doom arguments are unfalsifiable and rely on vague theoretical arguments rather than concrete evidence. The post embeds the original text with inline commentary on potential errors.",
      "importance_score": 42,
      "reasoning": "Meta-commentary on AI safety discourse rather than original research. Engages with important debates about falsifiability of doom arguments but is primarily a response piece without novel technical contributions.",
      "themes": [
        "AI Safety",
        "AI Doom Arguments",
        "Rationalist Discourse"
      ],
      "continuation": null
    },
    {
      "id": "dfd0bc9968f1",
      "title": "Many can write faster asm than the compiler, yet don't. Why?",
      "content": "There's a take I've seen going around, which goes approximately like this:It used to be the case that you had to write assembly to make computers do things, but then compilers came along. Now we have optimizing compilers, and those optimizing compilers can write assembly better than pretty much any human. Because of that, basically nobody writes assembly anymore. The same is about to be true of regular programming.I 85% agree with this take.However, I think there's one important inaccuracy: even today, finding places where your optimizing compiler failed to produce optimal code is often pretty straightforward, and once you've identified those places 10x+ speedups for that specific program on that specific hardware is often possible[1]. The reason nobody writes assembly anymore is the difficulty of mixing hand-written assembly with machine-generated assembly.The issue is that it's easy to have the compiler write all of the assembly in your project, and it's easy from a build perspective to have the compiler write none of the assembly in your project, but having the compiler write most but not all of the assembly in your project is hard. As with many things in proramming, having two sources of truth leads to sadness. You have many choices for what to do if you spot an optimization the compiler missed, and all of them are bad:Hope there's a pragma or compiler flag. If one exists, great! Add it and pray that your codebase doesn't change such that your pragma now hurts perf.Inline assembly. Now you're maintaining two mental models: the C semantics the rest of your code assumes, and the register/memory state your asm block manipulates. The compiler can't optimize across inline asm boundaries. Lots of other pitfalls as well - using inline asm feels to me like a knife except the handle has been replaced by a second blade so you can have twice as much knife per knife.Factor the hot path into a separate .s file, write an ABI-compliant assembly function and link it in. It work...",
      "url": "https://www.lesswrong.com/posts/LwzSqz3CAmNkWawe8/many-can-write-faster-asm-than-the-compiler-yet-don-t-why",
      "author": "faul_sname",
      "published": "2025-12-30T03:40:51.960000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Argues that while humans can often write faster assembly than compilers for specific cases, the real barrier is the difficulty of mixing hand-written and machine-generated assembly in practical projects. Draws implicit parallels to AI coding assistants.",
      "importance_score": 35,
      "reasoning": "Thought-provoking analysis relevant to AI coding tools debate. Offers useful framing about human-AI collaboration barriers, though more commentary than research.",
      "themes": [
        "Programming",
        "Human-AI Collaboration",
        "Software Engineering"
      ],
      "continuation": null
    },
    {
      "id": "45c0a6ae5de3",
      "title": "Exceptionally Gifted Children",
      "content": "I gave a talk on exceptionally gifted children at the Reproductive Frontiers Summit at Lighthaven this June. &nbsp;I believe the subject matter is highly relevant to the experience of many rationalists (e.g. one of Scott's surveys has put the average IQ of his readers at 137, and although that's not as extreme as 160+, I think many of the observations generalize to the merely highly gifted). &nbsp;The talk is on YouTube:&nbsp;I also adapted the talk into an article for the Center for Educational Progress. &nbsp;It has now been published: https://www.educationprogress.org/p/exceptionally-gifted-childrenI'd say the talk is more fun and more rationalist-focused, while the article is a bit more serious and meant for a wider audience. &nbsp;But mostly just pick whichever format you prefer.The central policy proposal is that schools should allow students to progress through each subject at whatever rate fits them, and the cheapest implementation is to let them take placement tests and move up or down grade levels as appropriate (so a child might be taking 3rd grade math, 5th grade English, 4th grade history, etc. at once). &nbsp;I think this would benefit children of all ability levels, and have some systemic benefits as well; but obviously it makes the largest difference at the extremes.",
      "url": "https://www.lesswrong.com/posts/uujobB6mbSZk7SJg9/exceptionally-gifted-children",
      "author": "John Boyle",
      "published": "2025-12-30T01:28:31.681000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Talk and article about exceptionally gifted children (IQ 160+), proposing that schools allow subject-by-subject grade placement based on ability. Notes relevance to rationalist community given high average reader IQ.",
      "importance_score": 25,
      "reasoning": "Education policy proposal with some community relevance but no AI research content. Interesting to rationalist audience but limited technical contribution.",
      "themes": [
        "Education",
        "Gifted Children",
        "Policy"
      ],
      "continuation": null
    },
    {
      "id": "a0000142199f",
      "title": "The origin of rot",
      "content": "Note: I spent my holidays writing a bunch of biology-adjacent, nontechnical pieces. I’ll intermittently mix them between whatever technical thing I send out, much like how a farmer may mix sawdust into feed, or a compounding pharmacist, butter into bathtub-created semaglutide. This one is about history!The book ‘Death with Interruptions’ is a 2005 speculative fiction novel written by Portuguese author José Saramago. It is about how, mysteriously, on January 1st of an unnamed year in an unnamed country, death ceases to occur. Everyone, save the Catholic church, is initially very delighted with this. But as expected, the natural order collapses, and several Big Problems rear their ugly heads. I recommend reading it in full, but the synopsis is all I need to mention.The situation described by José is obviously impossible. Cells undergo apoptosis to keep tissues healthy; immune systems kill off infected or malfunctioning cells; predators and prey form a food chain that only works because things end.But what you may find interesting is that what exactly happens after death has not always been so clear-cut. Not the religious aspect, but the so-called thanatomicrobiome—the community of microbes that colonize and decompose a body after death—is not necessarily a given. And there is some evidence that, for a very, very long time, it simply did not exist at all. Perhaps for much of the planets lifetime, the earth was a graveyard of pristine corpses, forests of bodies, oceans of carcasses, a world littered with the indigestible dead.Implausible, yes, but there is some evidence for it: the writings of a young apprentice scribe, aged fifteen, named Ninsikila-Enlil who was born in 1326 BCE and lived at a temple in ancient Babylon. Ninsikila-Enlil kept a diary, inscribed in tight, spiraling cuneiform on long clay tablets. In these tablets is his daily life, which primarily consisted of performing religious rituals for what has been loosely translated as the ‘Pit of Eternal Rest’. ...",
      "url": "https://www.lesswrong.com/posts/k6brDHrGtpoo4XsDa/the-origin-of-rot",
      "author": "Abhishaike Mahajan",
      "published": "2025-12-30T12:51:40.323000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A history-focused essay exploring the scientific understanding of decomposition and the thanatomicrobiome (microbial communities after death), using Saramago's novel 'Death with Interruptions' as a framing device.",
      "importance_score": 22,
      "reasoning": "Interesting science history writing but not AI-related. Non-technical piece with no research novelty, though well-written.",
      "themes": [
        "Biology",
        "Science History",
        "Microbiology"
      ],
      "continuation": null
    },
    {
      "id": "aef929f278cd",
      "title": "CFAR’s todo list re: our workshops",
      "content": "(This post is part of a sequence of year-end efforts to invite real conversation about CFAR; you’ll find more about our workshops, as well as our fundraiser, at What’s going on at CFAR? Updates and Fundraiser and at More details on CFAR's new workshops)In part of that post, we discuss the main thing that bothered me about our past workshop and why I think it is probably fixed now (though we’re still keeping an eye out). Here, I list the biggest remaining known troubles with our workshops and our other major workshop-related todo items.Your thoughts as to what’s really up with these and how to potentially address them (or what cheap investigations might get us useful info) are most welcome.Ambiguous impact on health&nbsp;(Current status: ?)In the 2012-2020 workshops, our “CFAR techniques” seemed to help people do 5-minute-timer or insight-based things, but seemed to some of us to make it harder, or at least not easier, to eg:Get physical exerciseLearn slow and unglamorous things from textbooks across an extended time[1]Be happy and hard-working at a day job in a slow and stable wayThis seems unfortunate.I’m mildly hopeful the changes we’ve made to our new workshops will also help with this. My reasons for hope:Our old workshop had a hyper/agitated/ungrounded energy running through it: “do X and you can be cool and rational like HPMOR!Harry”; “do X and you can maybe help with whether we’ll all die.” Our current workshop is quieter (“now that you’ve thought of X, does it feel inside-view-hopeful to try X some? Now that you’ve tried it some, does it feel inside-view-hopeful to do more?”). I’m hoping the quieter thing is less likely to drown out other useful work-and-morale-flows.We now aim to teach skills for tuning into the (surprisingly detailed) processes that make possible the cool structures around us, including those that one has already been running without explicitly knowing why.However, I won’t be surprised if this is still a problem. If so, we’ll need to fix i...",
      "url": "https://www.lesswrong.com/posts/77L6wyZqibxdGKd6e/cfar-s-todo-list-re-our-workshops",
      "author": "AnnaSalamon",
      "published": "2025-12-30T00:16:35.143000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "CFAR organizational update listing known issues with their rationality workshops, including concerns that techniques may help with quick tasks but not sustained health behaviors like exercise or textbook learning.",
      "importance_score": 20,
      "reasoning": "Organizational transparency and self-critique from CFAR. Relevant to rationality community but not research content.",
      "themes": [
        "CFAR",
        "Rationality Training",
        "Community Organizations"
      ],
      "continuation": null
    },
    {
      "id": "ba3cd602e125",
      "title": "What’s going on at CFAR? (Updates and Fundraiser)",
      "content": "This post is the main part of a sequence of year-end efforts to invite real conversation about CFAR, published to coincide with our fundraiser.&nbsp;Introduction / What’s up with this postMy main aim with this post is to have a real conversation about aCFAR[1]&nbsp;that helps us be situated within a community that (after this conversation) knows us. My idea for how to do this is to show you guys a bunch of pieces of how we’re approaching things, in enough detail to let you kibitz.[2]My secondary aim, which I also care about, is to see if some of you wish to donate, once you understand who we are and what we’re doing. (Some of you may wish to skip to the donation section.)Since this post is aimed partly at letting you kibitz on our process, it’s long.[3]&nbsp;Compared to most fundraiser posts, it’s also a bit unusually structured. Please feel free to skip around, and to participate in the comment thread after reading only whatever (maybe tiny) pieces interest you.I’d like CFAR to live in a communityI’d like CFAR to live in a community where:People can see aCFARWe can see you guys seeing usFolks are sharing what they’re seeing, not what their theory says they should seeInterested folks within LessWrong, and within the CFAR alumni community, can benefit from the experience we gather as we try things and collide with reality. Our failures and fizzles aren’t opaque (they have moving parts), and our successes can be built on by othersYou guys can tell us what we’re missing and help us do cooler experimentsWe are all aware in common knowledge that aCFAR is one group among many. We all know together that other groups already have norms and customs and their own local territories. Both we and you guys can track where we are having good or bad impacts on the spaces around us; it’s easier to be a good neighborIn the past, CFAR didn't know how to live in a community in this way (partly because I was often in charge, and I didn’t know how to do it). But I think CFAR and I now ha...",
      "url": "https://www.lesswrong.com/posts/4W8ZbcRr47x9bNEf6/what-s-going-on-at-cfar-updates-and-fundraiser",
      "author": "AnnaSalamon",
      "published": "2025-12-30T00:00:49.036000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Comprehensive CFAR update and fundraiser post aiming for transparent community engagement, describing organizational approach and inviting feedback. Discusses how CFAR wants to be 'situated within a community that knows us.'",
      "importance_score": 20,
      "reasoning": "Organizational fundraiser with emphasis on transparency. Community-relevant but not research content.",
      "themes": [
        "CFAR",
        "Community Organizations",
        "Fundraising"
      ],
      "continuation": null
    },
    {
      "id": "6f56506c21e6",
      "title": "The 7 Types Of Advice (And 3 Common Failure Modes)",
      "content": "Reposting my Inkhaven post on ontology of advice here.&nbsp;Are you interested in learning a new field, whether it’s programming, writing, or how to win Paper Mario games? Have you searched for lots of advice and couldn’t choose which advice to follow? Worse, have you tried to follow other people’s Wise Sounding Advice and ended up worse than where you started?Alternatively, have you tried to give useful advice distilling your hard-earning learnings and only to realize it fell on deaf ears? Or perhaps you’ve given advice that filled a much-needed hole that you now regret giving?If so, this post is for you!While this post is far from exhaustive, I hope reading it can help you a) identify the type of advice you want to give and receive and b) recognize and try to avoid common failure modes!7 Categories of Good Advice&nbsp;Source: https://englishlive.ef.com/en/blog/english-in-the-real-world/5-simple-ways-give-advice-english/Here are 7 semi-distinct categories of good advice. Some good advice mixes and matches between the categories, whereas others are more “purist” and just tries to do one style well.I. The Master KeyThis is where someone who deeply understands a field tries to impart the central tenets/frames of a field so newbies can decide whether the model is a good fit for what they want to do. And the rest of the article/book/lecture will be a combination of explaining the model and why they believe it’s true, and examples to get the learner to deeply understand the model. Eg “Focus on the user” as the central dictum in tech startups, or understanding and groking the Popperian framework for science.2My previous post, How to Win New Board Games, is an unusually pure example, where I spend 2000 words hammering different variations and instantiations of a single idea (“Understand the win condition, and play to win”).In writing advice, Clear and Simple as the Truth (by Thomas and Turner, review + extension here) works in this way as well, doing their best to model ho...",
      "url": "https://www.lesswrong.com/posts/gcCXPMLaxsWJb8hLJ/the-7-types-of-advice-and-3-common-failure-modes",
      "author": "Linch",
      "published": "2025-12-30T16:55:05.393000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "A taxonomy of seven categories of advice (Master Key, shortcuts, guardrails, etc.) and three common failure modes when giving or receiving advice. Aimed at helping people identify what type of advice they need and avoid mismatches.",
      "importance_score": 18,
      "reasoning": "General rationality/self-help content with no AI research relevance. Useful framework but not substantive technical content.",
      "themes": [
        "Rationality",
        "Self-Improvement",
        "Communication"
      ],
      "continuation": null
    },
    {
      "id": "6c478658e164",
      "title": "More details on CFAR’s new workshops",
      "content": "(This post is part of a sequence of year-end efforts to invite real conversation about CFAR; you’ll find more about our workshops, as well as our fundraiser, at What’s going on at CFAR? Updates and Fundraiser.)If you’d like to know more about CFAR’s current workshops (either because you’re thinking of attending / sending a friend, or because you’re just interested), this post is for you. Our focus in this post is on the new parts of our content. Kibitzing on content is welcome and appreciated regardless of whether or not you’re interested in the workshop.The core workshop format is unchanged:4.5 days of immersion with roughly 8 hours of class per dayClasses still aim partly to prime people to have great conversations during meals/eveningsAlmost everyone stays in a shared venueRoughly 25[1]&nbsp;participants, and 12-ish staff and volunteersMostly small classes“Honoring Who-ness”We added a new thread to our curriculum on working well with one’s own and other peoples’ “who-ness” (alternately: pride, ego, spark, self-ness, authorship).What, you might ask, is “who-ness?”Alas, we do not (yet?) have a technical concept near “who-ness.”[2]&nbsp;However, we want to make room at the workshop for discussing some obvious phenomena that are hard to model if your map of humans is just “humans have beliefs and goals” and easier once we try talking about “who-ness.” These phenomena include:a) Many of us humans feel good when someone notices a telling detail of a project we worked hard on -- especially if the detail is one we cared about, and double especially if they seem to see the generator behind that detail. After being affirmed in this way, we often have more energy (and especially energy for that particular generator).b) We seem similarly nourished by working alongside competent others to accomplish difficult tasks that use our skills fully and deeply.c) Much useful work seems to be bottlenecked more by psychological energy than by time and/or money.d) When I (or most people)...",
      "url": "https://www.lesswrong.com/posts/Ww6Mv8bkMCMcdKDRm/more-details-on-cfar-s-new-workshops",
      "author": "AnnaSalamon",
      "published": "2025-12-30T00:12:18.765000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Detailed description of CFAR's updated workshop format and new curriculum thread on 'who-ness' (ego, pride, authorship). Workshop maintains 4.5-day immersive format with ~25 participants.",
      "importance_score": 18,
      "reasoning": "Organizational/program update with no research content. Primarily relevant to those considering attending workshops.",
      "themes": [
        "CFAR",
        "Rationality Training",
        "Personal Development"
      ],
      "continuation": null
    },
    {
      "id": "269ad6c139bc",
      "title": "Don't Sell Stock to Donate",
      "content": "When you sell stock [1] you pay capital gains tax, but there's no tax if you donate the stock directly. Under a bunch of assumptions, someone donating $10k could likely increase their donations by ~$1k by donating stock. This applies to all 501(c) organizations, such as regular 501(c)3 non-profits, but also 501(c)4s such as advocacy groups. In the US, when something becomes more valuable and you sell it you need to pay tax proportional to the gains. [2] This gets complicated based on how much other income you have (which determines your tax bracket for marginal income), how long you've held it (which determines whether this is long-term vs short-term capital gains), and where you live (many states and some municipalities add additional tax). Some example cases: A single person in Boston with other income of $100k who had $10k in long-term capital gains would pay $2,000 (20%). This is 15% in federal tax and 5% in MA tax. A couple in SF with other income of $200k who had $10k in long-term capital gains would pay $2,810 (28%). This is 15% in federal tax, 3.8% for the NIIT surcharge, and 9.3% in CA taxes. A single person in NYC with other income of $600k who had $10k in short-term capital gains would pay $4,953 (50%). This is 35% in federal tax, 3.8% for the NIIT surcharge, 6.9% in NY taxes, and 3.9% in NYC taxes. When you donate stock to a 501(c), however, you don't pay this tax. This lets you potentially donate a lot more! Some things to pay attention to: Donations to political campaigns are treated as if you sold the stock and donated the money. If you've held the stock over a year and are donating to a 501(c)3 (or a few other less common ones like a 501(c)13 or a 501(c)19) then you can take a tax deduction of the full fair market value of the stock. This is bizarre to me (why can you deduct as if you had sold it and donated the money, when if you had gone that route you'd have needed to pay tax on the gains) but since it exists it's great to take advantage of. This ...",
      "url": "https://www.lesswrong.com/posts/2wn2k4gsCPjYQpTGZ/don-t-sell-stock-to-donate",
      "author": "jefftk",
      "published": "2025-12-30T14:50:25.098000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Tax optimization advice explaining that donating appreciated stock directly to 501(c) organizations avoids capital gains tax, potentially increasing effective donation value by ~10%. Provides specific examples for different income levels and locations.",
      "importance_score": 15,
      "reasoning": "Practical personal finance advice for donors, not research content. Useful for EA community but no technical or AI relevance.",
      "themes": [
        "Personal Finance",
        "Effective Altruism",
        "Philanthropy"
      ],
      "continuation": null
    },
    {
      "id": "38b554303a0f",
      "title": "Dating Roundup #9: Signals and Selection",
      "content": "Ultimately, it comes down to one question. Are you in? For you, and for them. You’re Single Because They Got The Ick The Ick, the ultimate red flag, makes perfect sense and is all about likelihood ratios. Koenfucius: The ‘ick’ is a colloquial term for a feeling of disgust triggered by a specific—typically trivial—behaviour from a romantic partner, often leading to the relationship’s demise. New research explores why some are more prone to getting it than others. Robin Hanson: “Women also experienced the ick more frequently, with 75% having had the ick compared to 57% of men … Those with a higher tendency for disgust … [&amp;] grandiose narcissism was linked to stronger ick reactions, as was holding partners to exceptionally high standards.” Paul Graham: About 30% of Seinfeld episodes were about this. One gets The Ick because a small act is evidence of one’s general nature. The right type of person would never do [X], ideally never want to do [X], and at minimum would have learned not to do [X]. Often this is because they would know this is indicative of attribute [Y]. Indeed, if they should be aware that [X] is indicative of [Y], then their failure to do [X] is indicative not only of a lack of [Y], but also of a lack of desire or ability to even fake or signal [Y], especially in a romantic context. They don’t respect [Y]. Thus, this is extremely strong evidence. Thus, The Ick. The person is not consciously thinking through this, but that’s the point. That doesn’t mean that The Ick is always valid. Quite the contrary. Mistakes are made. It’s fun to look at this list of Icks. There are very clear categories involved – status markers, stupidity and Your Mom are the big three. In general, it’s something that ‘looks bad’ and the fact that the man should know it looks bad and therefore not do it. To what extent is The Ick a win-win? The majority of the time, I think it’s win-win, because them caring so much about this little thing, combined with you not caring, means it w...",
      "url": "https://www.lesswrong.com/posts/6hhPbovwRWem4fvwr/dating-roundup-9-signals-and-selection",
      "author": "Zvi",
      "published": "2025-12-30T07:40:30.842000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Analysis of dating signals and selection effects, particularly examining 'the ick' as a Bayesian likelihood ratio update - small behaviors serving as evidence of deeper traits. Draws on research about disgust sensitivity and narcissism.",
      "importance_score": 15,
      "reasoning": "Social/dating analysis using rationalist frameworks. Not AI research, limited research relevance despite applying analytical thinking.",
      "themes": [
        "Social Dynamics",
        "Signaling",
        "Rationality"
      ],
      "continuation": null
    },
    {
      "id": "bc1e11607d67",
      "title": "End-of year donation taxes 101",
      "content": "Tl;drIf you’re taking the standard deduction (ie donating &lt;~$15k), ignore all this–there are basically no tax implications for youConsider how much money you want to donate to c3s specifically (as opposed to c4s, political stuff, random individuals, some foreign organizations, etc.). For money you definitely want to give to c3s, you can put it in a DAF to count them as a donation this year, then figure out where to direct it later. For non-c3 money, it doesn’t really matter when you give itA surprisingly large number of my friends are scrambling to make donations before the end of the year, or wondering whether or not they should be scrambling to make donations before the end of the year, and feeling vaguely bad that they don't understand their tax implications.I will quickly break down the tax implications[1]&nbsp;and lay out how to buy yourself way more time and to decide on everything except how much you donate and how much of your donation will go to 501(c)3s vs other opportunities.Note this post is greatly simplified. Your tax situation will depend on the state you live in and your income and maybe a bunch of other stuff. &nbsp;If you are considering donating more than $100k, I would strongly recommend talking to a tax professional and reaching out to people you trust to get donation advice. If you don’t know who to talk to DM me or schedule a chat with a person I know and trust who has thought carefully about these things (but isn’t a professional) here.Why the end-of-year rush?You pay taxes each year. The amount you pay in taxes increases with your income. But any money you donate to a 501(c)3 in a given year is deducted from your income (unless you’re taking the standard deduction, i.e. if you’re giving less than ~$15k[2]&nbsp;just ignore all this). So if I make $100k but I donate $10k, I’m only taxed on $90k of income. If you’re in a high tax bracket, that means donations to 501(c)3s are effectively ~37-50% (depending on the state you’re in and your tax ...",
      "url": "https://www.lesswrong.com/posts/xCw3PauTi9CWkJ5Wq/end-of-year-donation-taxes-101",
      "author": "GradientDissenter",
      "published": "2025-12-29T21:16:53.177000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Quick guide to year-end charitable donation tax implications, explaining when donations matter for taxes, difference between 501(c)3 vs other organizations, and how DAFs can buy time for donation decisions.",
      "importance_score": 15,
      "reasoning": "Practical tax advice for donors. Useful community service post but no research content.",
      "themes": [
        "Personal Finance",
        "Philanthropy",
        "Tax Planning"
      ],
      "continuation": null
    },
    {
      "id": "562f1d5b935a",
      "title": "Boston Solstice 2025 Retrospective",
      "content": "I like writing retrospectives for things I'm involved in, especially if I'm likely to be involved in them in the future: it's a good place to set thoughts down so I can find them again, link materials I'm likely to want, and collect feedback from others (but also: fill out the feedback survey!). As a bonus, they can be useful to other people who are doing similar things. I've written ones band tours, failed attempts to limit covid spread, and dance weekends; Saturday night I ran the music for the 2025 Boston Secular Solstice, so here's another one! This was the tenth Boston Secular Solstice in the Ray Arnold tradition. These go back a bit over ten years, and have been an opportunity for the rationality / lesswrong community (with some effective altruism representation) to gather, sing, and consider hard truths. We were in in Connexion for the second time, and I continue to be very glad to no longer be trying to squeeze this many people into our house! They didn't charge us, because this is the kind of event they'd like to encourage, but you could consider sending them some money to help them maintain the space. Over time the singing at these has been trending in the direction of being mostly my family: this year Alex sang three (and was great!) but all of the others were me, Julia, and Lily. While it certainly is convenient for practicing to be working with people who live in my house (or, in my case, are myself) I'd love to have a wider range of singers here. Let me know if you'd like to be on the list of people I reach out to in ~October to ask if they might be interested in leading any songs this year. Same goes for music: Max accompanied three of these on guitar, and I'd love to have more volunteers if this sounds like it might be fun! If we had enough people we could do things like play the melody of the tune, or add harmonies. In past years I've had a somewhat elaborate setup, including footdrums and breath controller overlays. This year I decided to try doing...",
      "url": "https://www.lesswrong.com/posts/3c5G77x68h4d79wb7/boston-solstice-2025-retrospective",
      "author": "jefftk",
      "published": "2025-12-29T20:10:06.143000",
      "source": "LessWrong",
      "source_type": "research_blog",
      "tags": [],
      "summary": "Retrospective on the 10th Boston Secular Solstice event, documenting logistics, song selection, and lessons learned. Notes the trend toward more participatory singing and community gathering aspects.",
      "importance_score": 12,
      "reasoning": "Event documentation with no research relevance. Useful for future organizers but pure community content.",
      "themes": [
        "Community Events",
        "Rationalist Community",
        "Event Planning"
      ],
      "continuation": null
    }
  ]
}