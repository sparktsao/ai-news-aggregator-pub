{
  "date": "2025-12-27",
  "coverage_date": "2025-12-26",
  "coverage_start": "2025-12-26T00:00:00",
  "coverage_end": "2025-12-26T23:59:59.999999",
  "executive_summary": "#### Top Story\nReports emerged that **GPT-5** [autonomously solved an open math problem](/?date=2025-12-27&category=reddit#item-69dbaace9677) in enumerative geometry, while **Andrej Karpathy's** viral post (16M+ views) declaring he's [\"never felt this much behind\"](/?date=2025-12-27&category=social#item-669a6053df59) as a programmer sparked industry-wide discussion about AI transforming software engineering.\n\n#### Key Developments\n- **Andrej Karpathy**: Warned that even 30 days away from AI developments leaves practitioners with a [\"deprecated world view\"](/?date=2025-12-27&category=social#item-36dc0ce60d80), describing current AI as \"alien tech\"\n- **NVIDIA (Jim Fan)**: Argued the industry is shifting from \"AI as copilot\" to [\"humans as copilot\"](/?date=2025-12-27&category=social#item-2818563e655b), representing a fundamental inversion of technical workflows\n- **Boris Cherny**: Demonstrated the shift by reporting he went a [full month without opening an IDE](/?date=2025-12-27&category=social#item-2d1d7d954f12) while **Claude** handled coding tasks including debugging memory leaks from heap dumps\n- **NeurIPS 2025**: Best Paper Runner-up [challenges whether RL improves](/?date=2025-12-27&category=social#item-7b2c227de988) LLM reasoning beyond base model capabilities\n- **ChatGPT**: Market analysis indicates [~**20x** popularity advantage](/?date=2025-12-27&category=social#item-c82eb0139a3e) over **Gemini**, creating a compounding data flywheel\n\n#### Safety & Regulation\n- **Ryan Greenblatt** [published research measuring](/?date=2025-12-27&category=research#item-157eeda19a06) models' ability to solve math without chain-of-thought—a method for detecting potentially dangerous hidden reasoning capabilities\n- **MATS** [mentorship program offering](/?date=2025-12-27&category=research#item-d806cd72e43d) alignment training with alumni placements at **Anthropic** and other labs\n- Community [guidance addressing burnout](/?date=2025-12-27&category=research#item-ec8c0454bd02) among safety researchers, attributing challenges to hopelessness over existential risks rather than overwork\n\n#### Research Highlights\n- **NVIDIA + Stanford's NitroGen**: [Generalist game-playing AI](/?date=2025-12-27&category=reddit#item-23cbf3c05b50) trained across **1,000+** games signals progress toward general-purpose agents\n- **S2ID**: [Generates **1024x1024** images](/?date=2025-12-27&category=reddit#item-c5e33520f1bc) with only **6.1M** parameters\n- **Liquid AI**: **2.6B** parameter pure-RL model claims to outperform larger competitors\n- Research shows video models [develop emergent 3D understanding](/?date=2025-12-27&category=reddit#item-5c4c8caad761) from 2D training alone\n\n#### Looking Ahead\nWatch for verification standards around AI mathematical claims—the reported [**Erdős problem** solution](/?date=2025-12-27&category=reddit#item-29ee1a598674) without **Lean** formal verification raises questions about how the field will validate AI breakthroughs on open problems.",
  "executive_summary_html": "<h4>Top Story</h4>\n<p>Reports emerged that <strong>GPT-5</strong> <a href=\"/?date=2025-12-27&category=reddit#item-69dbaace9677\" class=\"internal-link\">autonomously solved an open math problem</a> in enumerative geometry, while <strong>Andrej Karpathy's</strong> viral post (16M+ views) declaring he's <a href=\"/?date=2025-12-27&category=social#item-669a6053df59\" class=\"internal-link\">\"never felt this much behind\"</a> as a programmer sparked industry-wide discussion about AI transforming software engineering.</p>\n<h4>Key Developments</h4>\n<ul>\n<li><strong>Andrej Karpathy</strong>: Warned that even 30 days away from AI developments leaves practitioners with a <a href=\"/?date=2025-12-27&category=social#item-36dc0ce60d80\" class=\"internal-link\">\"deprecated world view\"</a>, describing current AI as \"alien tech\"</li>\n<li><strong>NVIDIA (Jim Fan)</strong>: Argued the industry is shifting from \"AI as copilot\" to <a href=\"/?date=2025-12-27&category=social#item-2818563e655b\" class=\"internal-link\">\"humans as copilot\"</a>, representing a fundamental inversion of technical workflows</li>\n<li><strong>Boris Cherny</strong>: Demonstrated the shift by reporting he went a <a href=\"/?date=2025-12-27&category=social#item-2d1d7d954f12\" class=\"internal-link\">full month without opening an IDE</a> while <strong>Claude</strong> handled coding tasks including debugging memory leaks from heap dumps</li>\n<li><strong>NeurIPS 2025</strong>: Best Paper Runner-up <a href=\"/?date=2025-12-27&category=social#item-7b2c227de988\" class=\"internal-link\">challenges whether RL improves</a> LLM reasoning beyond base model capabilities</li>\n<li><strong>ChatGPT</strong>: Market analysis indicates <a href=\"/?date=2025-12-27&category=social#item-c82eb0139a3e\" class=\"internal-link\">~<strong>20x</strong> popularity advantage</a> over <strong>Gemini</strong>, creating a compounding data flywheel</li>\n</ul>\n<h4>Safety & Regulation</h4>\n<ul>\n<li><strong>Ryan Greenblatt</strong> <a href=\"/?date=2025-12-27&category=research#item-157eeda19a06\" class=\"internal-link\">published research measuring</a> models' ability to solve math without chain-of-thought—a method for detecting potentially dangerous hidden reasoning capabilities</li>\n<li><strong>MATS</strong> <a href=\"/?date=2025-12-27&category=research#item-d806cd72e43d\" class=\"internal-link\">mentorship program offering</a> alignment training with alumni placements at <strong>Anthropic</strong> and other labs</li>\n<li>Community <a href=\"/?date=2025-12-27&category=research#item-ec8c0454bd02\" class=\"internal-link\">guidance addressing burnout</a> among safety researchers, attributing challenges to hopelessness over existential risks rather than overwork</li>\n</ul>\n<h4>Research Highlights</h4>\n<ul>\n<li><strong>NVIDIA + Stanford's NitroGen</strong>: <a href=\"/?date=2025-12-27&category=reddit#item-23cbf3c05b50\" class=\"internal-link\">Generalist game-playing AI</a> trained across <strong>1,000+</strong> games signals progress toward general-purpose agents</li>\n<li><strong>S2ID</strong>: <a href=\"/?date=2025-12-27&category=reddit#item-c5e33520f1bc\" class=\"internal-link\">Generates <strong>1024x1024</strong> images</a> with only <strong>6.1M</strong> parameters</li>\n<li><strong>Liquid AI</strong>: <strong>2.6B</strong> parameter pure-RL model claims to outperform larger competitors</li>\n<li>Research shows video models <a href=\"/?date=2025-12-27&category=reddit#item-5c4c8caad761\" class=\"internal-link\">develop emergent 3D understanding</a> from 2D training alone</li>\n</ul>\n<h4>Looking Ahead</h4>\n<p>Watch for verification standards around AI mathematical claims—the reported <a href=\"/?date=2025-12-27&category=reddit#item-29ee1a598674\" class=\"internal-link\"><strong>Erdős problem</strong> solution</a> without <strong>Lean</strong> formal verification raises questions about how the field will validate AI breakthroughs on open problems.</p>",
  "top_topics": [
    {
      "name": "AI Reshaping Software Development",
      "description": "Andrej Karpathy's [viral post declaring](/?date=2025-12-27&category=social#item-669a6053df59) he's 'never felt this much behind as a programmer' sparked widespread discussion about AI transforming software engineering. Boris Cherny [corroborated with experiences](/?date=2025-12-27&category=social#item-2d1d7d954f12) of Claude debugging memory leaks from heap dumps, while Andriy Burkov [offered pushback](/?date=2025-12-27&category=social#item-25aa4a9b951a) arguing deep system expertise remains valuable. On Reddit, related discussions of [self-improving software agents](/?date=2025-12-27&category=reddit#item-6e930a92f6ac) and Karpathy's ['alien tech' warning](/?date=2025-12-27&category=reddit#item-5137fae5bd60) added to concerns about fundamental changes in programming workflows.",
      "description_html": "Andrej Karpathy's <a href=\"/?date=2025-12-27&category=social#item-669a6053df59\" class=\"internal-link\">viral post declaring</a> he's 'never felt this much behind as a programmer' sparked widespread discussion about AI transforming software engineering. Boris Cherny <a href=\"/?date=2025-12-27&category=social#item-2d1d7d954f12\" class=\"internal-link\">corroborated with experiences</a> of Claude debugging memory leaks from heap dumps, while Andriy Burkov <a href=\"/?date=2025-12-27&category=social#item-25aa4a9b951a\" class=\"internal-link\">offered pushback</a> arguing deep system expertise remains valuable. On Reddit, related discussions of <a href=\"/?date=2025-12-27&category=reddit#item-6e930a92f6ac\" class=\"internal-link\">self-improving software agents</a> and Karpathy's <a href=\"/?date=2025-12-27&category=reddit#item-5137fae5bd60\" class=\"internal-link\">'alien tech' warning</a> added to concerns about fundamental changes in programming workflows.",
      "category_breakdown": {
        "news": 0,
        "research": 0,
        "social": 6,
        "reddit": 3
      },
      "representative_items": [],
      "importance": 92
    },
    {
      "name": "AI Mathematical Reasoning Milestones",
      "description": "Reports emerged on Reddit of GPT-5 [autonomously solving](/?date=2025-12-27&category=reddit#item-69dbaace9677) an open enumerative geometry problem and a separate AI system [tackling an Erdős problem](/?date=2025-12-27&category=reddit#item-29ee1a598674) without formal verification tools like Lean. Ryan Greenblatt's LessWrong [research measuring](/?date=2025-12-27&category=research#item-157eeda19a06) models' ability to solve math without chain-of-thought reasoning provides methodological grounding for evaluating such claims. A NeurIPS 2025 Best Paper Runner-up shared by Burkov [questions whether RL](/?date=2025-12-27&category=social#item-7b2c227de988) actually improves LLM reasoning beyond base models.",
      "description_html": "Reports emerged on Reddit of GPT-5 <a href=\"/?date=2025-12-27&category=reddit#item-69dbaace9677\" class=\"internal-link\">autonomously solving</a> an open enumerative geometry problem and a separate AI system <a href=\"/?date=2025-12-27&category=reddit#item-29ee1a598674\" class=\"internal-link\">tackling an Erdős problem</a> without formal verification tools like Lean. Ryan Greenblatt's LessWrong <a href=\"/?date=2025-12-27&category=research#item-157eeda19a06\" class=\"internal-link\">research measuring</a> models' ability to solve math without chain-of-thought reasoning provides methodological grounding for evaluating such claims. A NeurIPS 2025 Best Paper Runner-up shared by Burkov <a href=\"/?date=2025-12-27&category=social#item-7b2c227de988\" class=\"internal-link\">questions whether RL</a> actually improves LLM reasoning beyond base models.",
      "category_breakdown": {
        "news": 0,
        "research": 1,
        "social": 1,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 88
    },
    {
      "name": "AI Progress Velocity Concerns",
      "description": "Karpathy [stated that even 30 days away](/?date=2025-12-27&category=social#item-36dc0ce60d80) from AI developments leaves practitioners with a 'deprecated world view,' highlighting the unprecedented pace of advancement. Ethan Mollick [observed the pattern](/?date=2025-12-27&category=social#item-21d0a05ece5f) of goalpost-moving with benchmarks, noting the Turing Test seemed insurmountable until AI passed it. On Reddit, Peter Gostev's [26 probability-weighted predictions](/?date=2025-12-27&category=reddit#item-94415860b3ce) for AI in 2026 and widespread [discussion of Karpathy's warnings](/?date=2025-12-27&category=reddit#item-5137fae5bd60) reinforced urgency around keeping pace.",
      "description_html": "Karpathy <a href=\"/?date=2025-12-27&category=social#item-36dc0ce60d80\" class=\"internal-link\">stated that even 30 days away</a> from AI developments leaves practitioners with a 'deprecated world view,' highlighting the unprecedented pace of advancement. Ethan Mollick <a href=\"/?date=2025-12-27&category=social#item-21d0a05ece5f\" class=\"internal-link\">observed the pattern</a> of goalpost-moving with benchmarks, noting the Turing Test seemed insurmountable until AI passed it. On Reddit, Peter Gostev's <a href=\"/?date=2025-12-27&category=reddit#item-94415860b3ce\" class=\"internal-link\">26 probability-weighted predictions</a> for AI in 2026 and widespread <a href=\"/?date=2025-12-27&category=reddit#item-5137fae5bd60\" class=\"internal-link\">discussion of Karpathy's warnings</a> reinforced urgency around keeping pace.",
      "category_breakdown": {
        "news": 0,
        "research": 0,
        "social": 3,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 85
    },
    {
      "name": "Human-AI Role Inversion",
      "description": "Jim Fan at NVIDIA [argued we're shifting](/?date=2025-12-27&category=social#item-2818563e655b) from 'AI as copilot' to 'humans as copilot,' representing a fundamental inversion of the human-AI relationship in technical workflows. Boris Cherny illustrated this with his experience of [not opening an IDE](/?date=2025-12-27&category=social#item-2d1d7d954f12) for a full month while AI handled coding tasks. Reddit discussions of [self-improving software agents](/?date=2025-12-27&category=reddit#item-6e930a92f6ac) operating without human-labeled data and NVIDIA-Stanford's [NitroGen generalist game-playing AI](/?date=2025-12-27&category=reddit#item-23cbf3c05b50) further highlight expanding AI autonomy.",
      "description_html": "Jim Fan at NVIDIA <a href=\"/?date=2025-12-27&category=social#item-2818563e655b\" class=\"internal-link\">argued we're shifting</a> from 'AI as copilot' to 'humans as copilot,' representing a fundamental inversion of the human-AI relationship in technical workflows. Boris Cherny illustrated this with his experience of <a href=\"/?date=2025-12-27&category=social#item-2d1d7d954f12\" class=\"internal-link\">not opening an IDE</a> for a full month while AI handled coding tasks. Reddit discussions of <a href=\"/?date=2025-12-27&category=reddit#item-6e930a92f6ac\" class=\"internal-link\">self-improving software agents</a> operating without human-labeled data and NVIDIA-Stanford's <a href=\"/?date=2025-12-27&category=reddit#item-23cbf3c05b50\" class=\"internal-link\">NitroGen generalist game-playing AI</a> further highlight expanding AI autonomy.",
      "category_breakdown": {
        "news": 0,
        "research": 0,
        "social": 3,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 80
    },
    {
      "name": "AI Evaluation Challenges",
      "description": "Ethan Mollick's [observation](/?date=2025-12-27&category=social#item-21d0a05ece5f) about Turing Test goalpost-moving reflects persistent difficulties in meaningful AI evaluation. Ryan Greenblatt's research [measuring no-CoT math reasoning](/?date=2025-12-27&category=research#item-157eeda19a06) provides methodology for assessing potentially dangerous hidden reasoning capabilities in future systems. Reddit [discussions](/?date=2025-12-27&category=reddit#item-69dbaace9677) of AI mathematical claims raised questions about verification standards when systems tackle open problems without formal proof systems like Lean.",
      "description_html": "Ethan Mollick's <a href=\"/?date=2025-12-27&category=social#item-21d0a05ece5f\" class=\"internal-link\">observation</a> about Turing Test goalpost-moving reflects persistent difficulties in meaningful AI evaluation. Ryan Greenblatt's research <a href=\"/?date=2025-12-27&category=research#item-157eeda19a06\" class=\"internal-link\">measuring no-CoT math reasoning</a> provides methodology for assessing potentially dangerous hidden reasoning capabilities in future systems. Reddit <a href=\"/?date=2025-12-27&category=reddit#item-69dbaace9677\" class=\"internal-link\">discussions</a> of AI mathematical claims raised questions about verification standards when systems tackle open problems without formal proof systems like Lean.",
      "category_breakdown": {
        "news": 0,
        "research": 1,
        "social": 2,
        "reddit": 2
      },
      "representative_items": [],
      "importance": 75
    },
    {
      "name": "AI Safety Community Health",
      "description": "LessWrong featured multiple posts addressing AI safety community concerns: Holly Elmore's [interview discussed existential risks](/?date=2025-12-27&category=research#item-c1e05aa68eda) and industry criticism, while the MATS mentorship program from TurnTrout and Alex Cloud [offers alignment training](/?date=2025-12-27&category=research#item-d806cd72e43d) with successful alumni placements at Anthropic and other labs. A practical guide [addressing burnout and mental health](/?date=2025-12-27&category=research#item-ec8c0454bd02) argues these challenges stem from hopelessness rather than overwork, reflecting the psychological burden of working on existential risks.",
      "description_html": "LessWrong featured multiple posts addressing AI safety community concerns: Holly Elmore's <a href=\"/?date=2025-12-27&category=research#item-c1e05aa68eda\" class=\"internal-link\">interview discussed existential risks</a> and industry criticism, while the MATS mentorship program from TurnTrout and Alex Cloud <a href=\"/?date=2025-12-27&category=research#item-d806cd72e43d\" class=\"internal-link\">offers alignment training</a> with successful alumni placements at Anthropic and other labs. A practical guide <a href=\"/?date=2025-12-27&category=research#item-ec8c0454bd02\" class=\"internal-link\">addressing burnout and mental health</a> argues these challenges stem from hopelessness rather than overwork, reflecting the psychological burden of working on existential risks.",
      "category_breakdown": {
        "news": 0,
        "research": 4,
        "social": 1,
        "reddit": 0
      },
      "representative_items": [],
      "importance": 72
    }
  ],
  "total_items_collected": 432,
  "total_items_analyzed": 432,
  "collection_status": {
    "overall": "success",
    "sources": [
      {
        "name": "news",
        "display_name": "News",
        "status": "success",
        "count": 0,
        "error": null
      },
      {
        "name": "research",
        "display_name": "Research",
        "status": "success",
        "count": 8,
        "error": null
      },
      {
        "name": "social",
        "display_name": "Social",
        "status": "success",
        "count": 327,
        "error": null
      },
      {
        "name": "reddit",
        "display_name": "Reddit",
        "status": "success",
        "count": 97,
        "error": null
      }
    ],
    "social_platforms": [
      {
        "name": "twitter",
        "display_name": "Twitter",
        "status": "success",
        "count": 320,
        "error": null
      },
      {
        "name": "bluesky",
        "display_name": "Bluesky",
        "status": "success",
        "count": 0,
        "error": null
      },
      {
        "name": "mastodon",
        "display_name": "Mastodon",
        "status": "success",
        "count": 7,
        "error": null
      }
    ],
    "warnings": []
  },
  "hero_image_url": "/data/2025-12-27/hero.webp?v=1768083437",
  "hero_image_prompt": "You are generating a daily hero image for an AI news aggregator website.\n\n## Your Goal\nCreate a playful, colorful editorial illustration that visually represents today's top AI news stories. The scene should immediately convey the themes of the day's news to readers.\n\n## The Mascot (CRITICAL)\nThe attached image shows our skunk mascot. You MUST:\n- Keep the EXACT circuit board pattern on the skunk's body and tail - this is a core part of the brand identity\n- Maintain the skunk's white and black coloring with the tech circuit pattern visible\n- The skunk must be ACTIVELY DOING SOMETHING related to the topics - typing on a keyboard, reading papers, adjusting equipment, pointing at a screen, holding tools, etc. NOT just standing and smiling at the camera!\n- Position the skunk in the lower-left or lower-right portion, engaged with the scene\n\n## Today's Stories\n\n**Topic 1: AI Reshaping Software Development**\nAndrej Karpathy's viral post declaring he's 'never felt this much behind as a programmer' sparked widespread discussion about AI transforming software engineering. Boris Cherny corroborated with experiences of Claude debugging memory leaks from heap dumps, while Andriy Burkov offered pushback arguing deep system expertise remains valuable. On Reddit, related discussions of self-improving software agents and Karpathy's 'alien tech' warning added to concerns about fundamental changes in programming workflows.\n**Topic 2: AI Mathematical Reasoning Milestones**\nReports emerged on Reddit of GPT-5 autonomously solving an open enumerative geometry problem and a separate AI system tackling an Erdős problem without formal verification tools like Lean. Ryan Greenblatt's LessWrong research measuring models' ability to solve math without chain-of-thought reasoning provides methodological grounding for evaluating such claims. A NeurIPS 2025 Best Paper Runner-up shared by Burkov questions whether RL actually improves LLM reasoning beyond base models.\n**Topic 3: AI Progress Velocity Concerns**\nKarpathy stated that even 30 days away from AI developments leaves practitioners with a 'deprecated world view,' highlighting the unprecedented pace of advancement. Ethan Mollick observed the pattern of goalpost-moving with benchmarks, noting the Turing Test seemed insurmountable until AI passed it. On Reddit, Peter Gostev's 26 probability-weighted predictions for AI in 2026 and widespread discussion of Karpathy's warnings reinforced urgency around keeping pace.\n**Topic 4: Human-AI Role Inversion**\nJim Fan at NVIDIA argued we're shifting from 'AI as copilot' to 'humans as copilot,' representing a fundamental inversion of the human-AI relationship in technical workflows. Boris Cherny illustrated this with his experience of not opening an IDE for a full month while AI handled coding tasks. Reddit discussions of self-improving software agents operating without human-labeled data and NVIDIA-Stanford's NitroGen generalist game-playing AI further highlight expanding AI autonomy.\n**Topic 5: AI Evaluation Challenges**\nEthan Mollick's observation about Turing Test goalpost-moving reflects persistent difficulties in meaningful AI evaluation. Ryan Greenblatt's research measuring no-CoT math reasoning provides methodology for assessing potentially dangerous hidden reasoning capabilities in future systems. Reddit discussions of AI mathematical claims raised questions about verification standards when systems tackle open problems without formal proof systems like Lean.\n**Topic 6: AI Safety Community Health**\nLessWrong featured multiple posts addressing AI safety community concerns: Holly Elmore's interview discussed existential risks and industry criticism, while the MATS mentorship program from TurnTrout and Alex Cloud offers alignment training with successful alumni placements at Anthropic and other labs. A practical guide addressing burnout and mental health argues these challenges stem from hopelessness rather than overwork, reflecting the psychological burden of working on existential risks.\n\n## Visual Direction\nCreate a scene that represents these stories. You must include Topic 1 (the top story), then pick 2-3 others that would make the best scene together. Consider:\n- What visual metaphors could represent these themes?\n- How can the skunk mascot interact with or observe these elements?\n- Suggested scene elements: thought bubbles, chain of logic, decision trees, shield icons, protective barriers, guardrails\n\n## Style Requirements\n- Playful cartoon illustration, tech editorial art style\n- Vibrant colors with Trend Red (#E63946) accents\n- Energetic, forward-looking, tech-optimistic mood\n- No Trend Micro logos or watermarks - but other company logos (OpenAI, Anthropic, Google, etc.) are encouraged when relevant to the stories",
  "generated_at": "2026-01-10T17:17:17.078706",
  "categories": {
    "news": {
      "count": 0,
      "category_summary": "No items to analyze.",
      "category_summary_html": "<p>No items to analyze.</p>",
      "themes": [],
      "top_items": []
    },
    "research": {
      "count": 8,
      "category_summary": "Today's most significant work centers on **measuring opaque AI reasoning capabilities**. Ryan Greenblatt's [empirical research quantifies](/?date=2025-12-27&category=research#item-157eeda19a06) models' ability to solve math problems without chain-of-thought—a key proxy for detecting potentially dangerous hidden reasoning in future systems.\n\n- **Whole Brain Emulation** [proposed as an anchor point](/?date=2025-12-27&category=research#item-58d6d852cbd6) for AI welfare considerations, offering a principled framework where moral status claims are least controversial\n- **Regression by Composition (RBC)** [introduces novel statistical methodology](/?date=2025-12-27&category=research#item-7146cf24edb2) using group action composition, accepted at **JRSS-B**\n- Community resources include alignment [mentorship opportunities](/?date=2025-12-27&category=research#item-d806cd72e43d) from **MATS** (TurnTrout/Alex Cloud) and practical [mental health strategies](/?date=2025-12-27&category=research#item-ec8c0454bd02) for safety researchers\n\nNote: Limited research volume today—only 8 items available, with top work focused on AI safety measurement and ethics rather than capabilities advances.",
      "category_summary_html": "<p>Today's most significant work centers on <strong>measuring opaque AI reasoning capabilities</strong>. Ryan Greenblatt's <a href=\"/?date=2025-12-27&category=research#item-157eeda19a06\" class=\"internal-link\">empirical research quantifies</a> models' ability to solve math problems without chain-of-thought—a key proxy for detecting potentially dangerous hidden reasoning in future systems.</p>\n<ul>\n<li><strong>Whole Brain Emulation</strong> <a href=\"/?date=2025-12-27&category=research#item-58d6d852cbd6\" class=\"internal-link\">proposed as an anchor point</a> for AI welfare considerations, offering a principled framework where moral status claims are least controversial</li>\n<li><strong>Regression by Composition (RBC)</strong> <a href=\"/?date=2025-12-27&category=research#item-7146cf24edb2\" class=\"internal-link\">introduces novel statistical methodology</a> using group action composition, accepted at <strong>JRSS-B</strong></li>\n<li>Community resources include alignment <a href=\"/?date=2025-12-27&category=research#item-d806cd72e43d\" class=\"internal-link\">mentorship opportunities</a> from <strong>MATS</strong> (TurnTrout/Alex Cloud) and practical <a href=\"/?date=2025-12-27&category=research#item-ec8c0454bd02\" class=\"internal-link\">mental health strategies</a> for safety researchers</li>\n</ul>\n<p>Note: Limited research volume today—only 8 items available, with top work focused on AI safety measurement and ethics rather than capabilities advances.</p>",
      "themes": [
        {
          "name": "AI Capabilities",
          "description": "Measurement and tracking of AI model capabilities, particularly opaque reasoning without explicit chain-of-thought",
          "item_count": 1,
          "example_items": [],
          "importance": 72
        },
        {
          "name": "AI Safety",
          "description": "Research and discussion on preventing harmful AI outcomes, including scheming risks, alignment, and existential concerns",
          "item_count": 5,
          "example_items": [],
          "importance": 65
        },
        {
          "name": "Mechanistic Interpretability",
          "description": "Understanding internal representations and computations within neural networks",
          "item_count": 2,
          "example_items": [],
          "importance": 55
        },
        {
          "name": "AI Welfare/Ethics",
          "description": "Philosophical and empirical approaches to determining moral status of AI systems",
          "item_count": 1,
          "example_items": [],
          "importance": 45
        }
      ],
      "top_items": [
        {
          "id": "157eeda19a06",
          "title": "Measuring no CoT math time horizon (single forward pass)",
          "content": "A key risk factor for scheming (and misalignment more generally) is opaque reasoning ability. One proxy for this is how good AIs are at solving math problems immediately without any chain-of-thought (CoT) (as in, in a single forward pass). I've measured this on a dataset of easy math problems and used this to estimate 50% reliability no-CoT time horizon using the same methodology introduced in Measuring AI Ability to Complete Long Tasks (the METR time horizon paper). Important caveat: To get human completion times, I ask Opus 4.5 (with thinking) to estimate how long it would take the median AIME participant to complete a given problem. These times seem roughly reasonable to me, but getting some actual human baselines and using these to correct Opus 4.5's estimates would be better. Here are the 50% reliability time horizon results: I find that Opus 4.5 has a no-CoT 50% reliability time horizon of 3.5 minutes and that time horizon has been doubling every 9 months. In an earlier post (Recent LLMs can use filler tokens or problem repeats to improve (no-CoT) math performance), I found that repeating the problem substantially boosts performance. In the above plot, if repeating the problem 5 times in the prompt helps some model, I use the score with repeats (because I think this somewhat more conservative measurement is plausibly more representative of the concerning type of cognition). The fit appears very clean, but note that I've opinionatedly just done the fit on frontier Anthropic models (specifically, I exclude DeepSeek-v3 which was barely SOTA at the time of release). Also note that if you don't allow for repeats, frontier Anthropic models and GPT-4 no longer appear to be on the same nice trend (frontier Anthropic models still have their own very clean exponential fit, it just no longer back-predicts GPT-4 as well). I compare results with repeats to without repeats: Some details about the time horizon fit and data For the relatively more capable models I evaluate, I...",
          "url": "https://www.lesswrong.com/posts/Ty5Bmg7P6Tciy2uj2/measuring-no-cot-math-time-horizon-single-forward-pass",
          "author": "ryan_greenblatt",
          "published": "2025-12-26T11:37:52.424000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Ryan Greenblatt measures AI models' ability to solve math problems without chain-of-thought reasoning as a proxy for opaque reasoning capability—a key risk factor for scheming. Finds Opus 4.5 has a 3.5-minute no-CoT time horizon and that this capability has been doubling approximately every 9 months.",
          "importance_score": 72,
          "reasoning": "Original empirical research directly relevant to AI safety from a credible researcher. Provides quantifiable metrics for tracking opaque reasoning capabilities, which is crucial for monitoring scheming risks. Novel methodology applying METR time horizon approach to no-CoT performance. The 9-month doubling rate is a significant finding for forecasting.",
          "themes": [
            "AI Safety",
            "AI Capabilities",
            "Alignment",
            "Scheming Risk",
            "Evaluation"
          ],
          "continuation": null
        },
        {
          "id": "58d6d852cbd6",
          "title": "Whole Brain Emulation as an Anchor for AI Welfare",
          "content": "Epistemic status: Fairly confident in the framework, uncertain about object-level claims. Keen to receive pushback on the thought experiments.TL;DR: I argue that Whole Brain Emulations (WBEs) would clearly have moral patienthood, and that the relevant features are computational, not biological. Recent Mechanistic Interpretability (MI) work shows Large Language Models (LLMs) have emotional representations with geometric structure matching human affect. This doesn't prove LLMs deserve moral consideration, but it establishes a necessary condition, and we should take it seriously.Acknowledgements: Thanks to Boyd Kane, Anna Soligo, and Isha Gupta for providing feedback on early drafts.In this post I’ll be arguing for the following claim: we can make empirical progress on AI welfare without solving consciousness.The key move is using Whole Brain Emulation as an anchor point. WBEs would clearly deserve moral consideration (under functionalism), and they're non-biological, so whatever grounds their moral status must be computational. This gives us something concrete to look for in LLMs.In this post I'll:Argue that WBEs establish a moral precedent that rules out biological prerequisitesShow that LLMs have human-like geometric structure in their emotional representationsExamine (and mostly dismiss) other candidate features that might be necessary for moral patienthoodThe WBE Anchor: Why Substrate Doesn't MatterDiscussions of whether LLMs deserve moral patienthood often get stuck on whether they have experiences. A useful intuition comes from considering Whole Brain Emulation: a computational simulation of a human brain.I claim WBEs have a strong&nbsp;basis for moral patienthood. This requires accepting functionalism (which asserts that computational structure matters more than physical substrate).&nbsp;Functionalism is a key crux for this argument. If you reject functionalism, the rest of the post won't be compelling. (Similarly, if you accept&nbsp;illusionism about conscious...",
          "url": "https://www.lesswrong.com/posts/ooCiXHFzoob8FiFDm/whole-brain-emulation-as-an-anchor-for-ai-welfare",
          "author": "sturb",
          "published": "2025-12-26T09:45:51.847000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Argues that Whole Brain Emulations can serve as an anchor point for AI welfare considerations since they would clearly deserve moral status under functionalism while being non-biological. Connects this framework to recent mechanistic interpretability findings showing LLMs have emotional representations with geometric structures matching human affect.",
          "importance_score": 45,
          "reasoning": "Novel philosophical framework connecting MI research to AI welfare questions. Makes interesting theoretical contribution but relies heavily on speculative claims. The connection to concrete MI findings (emotional representations) adds some empirical grounding. Not peer-reviewed technical research but thoughtful conceptual work.",
          "themes": [
            "AI Welfare",
            "AI Ethics",
            "Mechanistic Interpretability",
            "Consciousness Studies"
          ],
          "continuation": null
        },
        {
          "id": "7146cf24edb2",
          "title": "Regression by Composition",
          "content": "This is a linkpost for the preprint “Regression by Composition”, by Daniel Farewell, Rhian Daniel, Mats Stensrud, and myself.The paper introduces Regression by Composition (RBC): a new, modular framework for regression modelling built around the composition of group actions. The manuscript has been accepted as a discussion paper in JRSS-B and will be read to the Royal Statistical Society in London on March 24th, 2026.Background and motivationIn earlier posts on LessWrong, I have argued that an effect parameter I call the Switch Relative Risk (SRR) is often the most appropriate scale for extrapolating causal effects from one population to another—for example, from a randomized trial population to patients seen in routine clinical practice.That position has been debated extensively elsewhere, including on statistical discourse forums. One common objection is that the odds ratio has a privileged status because it corresponds to the canonical link function in generalized linear models (GLMs), whereas the SRR does not admit a natural GLM formulation.This objection was one of the original motivations for developing a new regression framework. Regression by Composition allows models that are closely related to the SRR, without forcing them into the GLM mould.But the SRR—and binary outcomes more generally—are only a small part of why we think RBC is important.What Regression by Composition doesAt a high level, RBC reframes regression models in terms of group actions and invariance, rather than link functions and linear predictors. This shift has several consequences:Unification: RBC subsumes almost all standard regression models as special cases.Expansion: It substantially enlarges the class of allowable models beyond what GLMs and other preexisting frameworks permit.Modularity: Features that traditionally belong to different model classes can be combined within a single coherent model.Conceptual clarity: Statistical properties of effect parameters (such as collapsibility) ...",
          "url": "https://www.lesswrong.com/posts/uF3E6n6e6ftac7Bk5/regression-by-composition",
          "author": "Anders_H",
          "published": "2025-12-26T07:18:43.912000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "This is a linkpost for the preprint “Regression by Composition”, by Daniel Farewell, Rhian Daniel, Mats Stensrud, and myself.The paper introduces Regression by Composition (RBC): a new, modular framew...",
          "importance_score": 30,
          "reasoning": "Not analyzed (batch processing)",
          "themes": [],
          "continuation": null
        },
        {
          "id": "c1e05aa68eda",
          "title": "The moral critic of the AI industry—a Q&A with Holly Elmore",
          "content": "Since AI was first conceived of as a serious technology, some people wondered whether it might bring about the end of humanity. For some, this concern was simply logical. Human individuals have caused catastrophes throughout history, and powerful AI, which would not be bounded in the same way, might therefore pose even worse dangers.In recent times, as the capabilities of AI have grown larger, one might have thought that its existential risks would also have become more obvious in nature. And in some ways, they have. It is increasingly easy to see how AI could pose severe risks now that it is being endowed with agency, for example, or being put in control of military weaponry.On the other hand, the existential risks of AI have become more murky. Corporations increasingly sell powerful AI as just another consumer technology. They talk blandly about giving it the capability to improve itself, without setting any boundaries. They perform safety research, even while racing to increase performance. And, while they might acknowledge existential risks of AI, in some cases, they tend to disregard serious problems with other, closely related technologies.&nbsp;The rising ambiguity of the AI issue has led to introspection and self-questioning in the AI safety community, chiefly concerned about existential risks for humanity. Consider what happened in November, when a prominent researcher named Joe Carlsmith, who had worked at the grantmaking organization called Open Philanthropy (recently renamed as Coefficient Giving), announced that he would be joining the leading generative AI company, Anthropic.&nbsp;There was one community member on Twitter/X, named Holly Elmore, who provided a typically critical commentary: \"Sellout,\" she wrote, succinctly.Prior to seeing Elmore's post, I had felt that Carlsmith probably deserved, if not sympathy—making a decision that would presumably be highly lucrative for himself—at least a measure of understanding. He had provided, in a long post, ...",
          "url": "https://www.lesswrong.com/posts/pLKZybncxqrdaEbXw/the-moral-critic-of-the-ai-industry-a-q-and-a-with-holly",
          "author": "Mordechai Rorvig",
          "published": "2025-12-26T12:49:24.887000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "An interview with Holly Elmore discussing AI existential risks and her role as a critic of the AI industry. Explores the tension between corporations marketing AI as consumer technology while acknowledging existential risks and the growing ambiguity around AI safety concerns.",
          "importance_score": 32,
          "reasoning": "Interview/profile piece providing perspective on AI safety advocacy but lacking original research or technical contributions. Offers useful context on current discourse but no novel findings or methodologies.",
          "themes": [
            "AI Safety",
            "AI Governance",
            "Existential Risk"
          ],
          "continuation": null
        },
        {
          "id": "d806cd72e43d",
          "title": "Apply for Alignment Mentorship from TurnTrout and Alex Cloud",
          "content": "Through the MATS program, we (Alex Turner and Alex Cloud[1]) help alignment researchers grow from seeds into majestic trees. We have fun, consistently make real alignment progress, and help scholars tap into their latent abilities.MATS summer '26 applications are open until January 18th!Team Shard in MATS 6.0 during the summer of '24. From left: Evžen Wyitbul, Jacob Goldman-Wetzler, Alex Turner, Alex Cloud, and Joseph Miller.Many mentees now fill impactful roles.Lisa Thiergart (MATS 3.0) moved on to being a research lead at MIRI and is now a senior director at the SL5 task force.Alex Cloud (MATS 6.0) went from mentee to co-mentor in one round and also secured a job at Anthropic. Lead author on the Subliminal Learning paper.Jacob Goldman-Wetzler (MATS 6.0) also accepted an offer from Anthropic!Luke Marks accepted work with Redwood Research after MATS 8.0.And several mentees have gone on to the Anthropic Fellows program.We likewise have a strong track record in research outputs, includingPioneering steering vectors for use in LLMs (Steering GPT-2-XL by Adding an Activation Vector, Steering LLAMA-2 With Contrastive Activation Additions),Masking Gradients to Localize Computation in Neural Networks,Distillation Robustifies Unlearning (NeurIPS 2025 spotlight!), andOutput Supervision Can Obfuscate the Chain of Thought.Former scholar from Team Shard&nbsp;I really appreciate the calmness Alex [Turner] brings. He creates a stress-free environment where it feels easy and low-risk to have lots of ideas, pivot frequently, and generally be mentally nimble.Compared to other MATS streams, Team Shard has some of the best team culture and the highest mentor investment. With us, you aren't looking at a half-hour call with a remote mentor once a week. TurnTrout and Cloud each hold a ~45 minute weekly meeting with each scholar, in addition to a weekly in-person team lunch.&nbsp;Our team culture is tight-knit and fun, extending beyond the research itself. For example, in the summer of 20...",
          "url": "https://www.lesswrong.com/posts/hgoj2WAwLwn3qWLuc/apply-for-alignment-mentorship-from-turntrout-and-alex-cloud",
          "author": "TurnTrout",
          "published": "2025-12-26T12:20:14.755000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Recruitment announcement for the MATS alignment mentorship program led by Alex Turner and Alex Cloud. Highlights successful alumni placements at Anthropic, MIRI, and Redwood Research, and past research outputs including pioneering work on steering vectors.",
          "importance_score": 25,
          "reasoning": "Primarily a program advertisement rather than research content. References important past work (steering vectors, subliminal learning) but doesn't present new findings. Useful signal about talent pipeline but limited research value.",
          "themes": [
            "AI Safety",
            "Alignment",
            "Career Development"
          ],
          "continuation": null
        },
        {
          "id": "ec8c0454bd02",
          "title": "Burnout, depression, and AI safety: some concrete mental health strategies",
          "content": "Cross-posted from my SubstackBurnout and depression in AI safety usually don’t happen because of overwork.From what I've seen, it usually comes from a lack of hope.Working on something you don’t think will work and if it doesn’t work, you’ll die? That's a recipe for misery.How do you fix AI safety hopelessness?&nbsp;First off, rationally assess the likelihood of your work actually helping with AI safety.If you rationally believe that it’s too unlikely to actually help with AI safety, well, then, stop working on it!In this case, your feelings are an important signal to listen to.Now, if you think your work is high expected value but the likelihood of payoff is low, it’s quite common to find this dispiriting.Humans were not evolved to be motivated by expected value calculations.Here are some different ways to get different parts of your brain on board with a high risk high reward strategy:Pick a strategy that you enjoy the process of. Then, even if it doesn’t work. . . at least you had a good time!If you’re only working for the outcome and the outcome looks bleak, that’s very demotivating. If you’re working for the outcome and the process, then even if the outcome looks bad, you can still happily work.Understanding the cause of your unhappiness can help you avoid certain strategies that are unlikely to workFor example, a lot of people think that the solution to them feeling bad is to take some time off.I can't tell you how many people I know who have then taken a few weeks or months off and then continue to have exactly the same problem.Because the problem was not overwork.The problem was something else and the problem was still there when they came back.If your problem is hopelessness, then exercise, meditation, or meds are unlikely to fix it either. Even though all of those techniques are good for a lot of different causes of depression or burnout.Of course, hopelessness is also a symptom of depression/burnout, so it’s important to try to do some differential diagno...",
          "url": "https://www.lesswrong.com/posts/Atgb5wcHQT7fHzbS6/burnout-depression-and-ai-safety-some-concrete-mental-health",
          "author": "KatWoods",
          "published": "2025-12-26T14:52:50.483000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "A practical guide addressing mental health challenges in the AI safety community, arguing that burnout typically stems from hopelessness rather than overwork. Offers strategies for staying motivated when working on low-probability, high-stakes problems by focusing on process enjoyment and reframing expectations.",
          "importance_score": 20,
          "reasoning": "Community wellness content rather than technical research. While valuable for AI safety workers, it contains no novel research findings, methodology, or technical insights. Limited broader applicability outside the specific community context.",
          "themes": [
            "AI Safety",
            "Community Health",
            "Career Advice"
          ],
          "continuation": null
        },
        {
          "id": "2a1400f00410",
          "title": "Childhood and Education #16: Letting Kids Be Kids",
          "content": "The Revolution of Rising Requirements has many elements. The most onerous are the supervisory requirements on children. They have become, as Kelsey Piper recently documented, completely, utterly insane, to the point where: A third of people, both parents and non-parents, responded in a survey that it is not appropriate to leave a 13 year old at home for an hour or two, as opposed to when we used to be 11 year olds babysitting for other neighborhood kids. A third of people said in that same survey that if a 10-year-old is allowed to play alone in the park, there needs to be an investigation by CPS. Whereas I think that if you don’t allow your 10-year-old to play alone in a park, that is a much better (although still quite bad) potential reason for a CPS investigation. This is not an idle threat, per the common statistic that around 35% of American families get investigated by CPS. Even if you are confident that will ultimately turn out fine, and given the vagaries and insanities one can never fully be sure, the process is already the punishment. As Kelsey Piper says, we don’t want a lot of 14-year-olds being breadwinners for their families. But this is so bad in the other direction it might be even worse than that, even discounting the kids that this causes to never be born at all. Kids need to be kids. We don’t let them. It’s a big problem, both greatly raising the dollar, time and lifestyle costs of having kids and also destroying their childhoods. This post is about various ways of seeing exactly how bad things have gotten. We Don’t Let Kids Be Kids Some dire statistics from the Harris poll. Harris Poll: More than half of the kids surveyed have not experienced many real-life experiences on their own. According to the kids surveyed aged 8 to 12 years old: 45% have not walked in a different aisle than their parents at a store 56% have not talked with a neighbor without their parents 61% have not made plans with friends without adults helping them 62% have not walked...",
          "url": "https://www.lesswrong.com/posts/dkbE54ESRBW69P3cY/childhood-and-education-16-letting-kids-be-kids",
          "author": "Zvi",
          "published": "2025-12-26T08:50:39.242000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "Introduces Regression by Composition (RBC), a new statistical framework for regression modeling using composition of group actions. The paper has been accepted as a discussion paper in JRSS-B and addresses limitations of standard GLM approaches for causal effect extrapolation.",
          "importance_score": 18,
          "reasoning": "Legitimate academic statistics research accepted at a prestigious venue (JRSS-B discussion paper). However, it's statistical methodology work not directly related to AI/ML research. Limited relevance to AI research analysis despite methodological quality.",
          "themes": [
            "Statistical Methods",
            "Causal Inference"
          ],
          "continuation": null
        },
        {
          "id": "1a9d9aedab91",
          "title": "How hard should I prioritize having kids?",
          "content": "I am really not sure how hard I should prioritize having kids in my life. I am posting this because I mostly would like to hear perspectives from other people who have kids. The current worlds that feel possible / close to me right now:not have kidshave kids with someone I like but don't love[1][2]have kids w a known donor and be a single parentmy ideas right now are:get good&nbsp;could try harder to meet people.[3]&nbsp;could change my own visibility so people meet me.could try to actually build the rationalist orphanage.[4]could get a really demanding job and not think about this.&nbsp;could attempt to fall off and try to find peace in a beach town in Australia in between getting knocked up and forgetting everything I know about what is possible in this strange lovely world.for more context:&nbsp;am currently 29.have 36 eggs frozen with plans to freeze more.&nbsp;have only ever seriously wanted to have kids with one person.am only medium on the have kids thing but my biology yells at me it is important.(more context but a bit more meta):Kids seems to be a thing people recommend hard (which makes me curious and suspicious but mostly curious)I think I am configured in a way where I would be fine / good at having kids.I think I have some degree of arrogance here and predict I would be humbled, but I think kids wouldn't be too hard if I can solve for things like sensory issues (noise).&nbsp;I don't have hard time finding people who want to date me but do have harder time finding people I want to date.&nbsp;&nbsp;I am open to hearing better ideas. I am open to hearing that I am thinking about this wrong (and alternative ways to think about). I am open to dms&nbsp;^The word love here is loosely defined in my head in two ways.&nbsp;One is the warm, gooey, nice, alive feeling most could prob consider romantic love.&nbsp;Two is a deeper level of respect, trust, care, taste and support. I think you can find this in friends too.&nbsp;For me to do creatures with someone I ant...",
          "url": "https://www.lesswrong.com/posts/4BTxdof8nnuCvRxK6/how-hard-should-i-prioritize-having-kids",
          "author": "Recurrented",
          "published": "2025-12-26T14:29:30.689000",
          "source": "LessWrong",
          "source_type": "research_blog",
          "tags": [],
          "summary": "A personal post seeking advice about family planning priorities, weighing various options including single parenthood and relationship choices. Discusses the author's personal circumstances and solicits perspectives from parents in the rationalist community.",
          "importance_score": 5,
          "reasoning": "Personal life advice request with no connection to AI research, technical content, or safety work. Not relevant to AI research analysis.",
          "themes": [
            "Personal",
            "Community Discussion"
          ],
          "continuation": null
        }
      ]
    },
    "social": {
      "count": 327,
      "category_summary": "The programming profession is experiencing an existential moment. **Andrej Karpathy's** [landmark post](/?date=2025-12-27&category=social#item-669a6053df59) (16M+ views) declaring he's never felt 'this much behind as a programmer' ignited massive discussion about AI's transformation of software engineering.\n\n- **Boris Cherny** [corroborated with concrete examples](/?date=2025-12-27&category=social#item-2d1d7d954f12): Claude debugging memory leaks from heap dumps, going a full month without opening an IDE\n- **Karpathy** [followed up](/?date=2025-12-27&category=social#item-36dc0ce60d80) noting that even 30 days away from AI developments leaves you with a 'deprecated world view'\n- **Jim Fan** (NVIDIA) [argues we're shifting](/?date=2025-12-27&category=social#item-2818563e655b) from 'AI as copilot' to 'humans as copilot'\n- **Andriy Burkov** [offered contrarian pushback](/?date=2025-12-27&category=social#item-25aa4a9b951a): 'shitcoders might disappear' but deep system expertise remains valuable\n\n**Ethan Mollick** [observed the goalpost-moving pattern](/?date=2025-12-27&category=social#item-21d0a05ece5f) with benchmarks—the Turing Test seemed insurmountable until AI passed it. Meanwhile, a **NeurIPS 2025** Best Paper Runner-up [challenges whether RL](/?date=2025-12-27&category=social#item-7b2c227de988) actually improves LLM reasoning. [Market analysis](/?date=2025-12-27&category=social#item-c82eb0139a3e) suggests **ChatGPT's** ~20x popularity advantage over **Gemini** creates a compounding data flywheel for dominance.",
      "category_summary_html": "<p>The programming profession is experiencing an existential moment. <strong>Andrej Karpathy's</strong> <a href=\"/?date=2025-12-27&category=social#item-669a6053df59\" class=\"internal-link\">landmark post</a> (16M+ views) declaring he's never felt 'this much behind as a programmer' ignited massive discussion about AI's transformation of software engineering.</p>\n<ul>\n<li><strong>Boris Cherny</strong> <a href=\"/?date=2025-12-27&category=social#item-2d1d7d954f12\" class=\"internal-link\">corroborated with concrete examples</a>: Claude debugging memory leaks from heap dumps, going a full month without opening an IDE</li>\n<li><strong>Karpathy</strong> <a href=\"/?date=2025-12-27&category=social#item-36dc0ce60d80\" class=\"internal-link\">followed up</a> noting that even 30 days away from AI developments leaves you with a 'deprecated world view'</li>\n<li><strong>Jim Fan</strong> (NVIDIA) <a href=\"/?date=2025-12-27&category=social#item-2818563e655b\" class=\"internal-link\">argues we're shifting</a> from 'AI as copilot' to 'humans as copilot'</li>\n<li><strong>Andriy Burkov</strong> <a href=\"/?date=2025-12-27&category=social#item-25aa4a9b951a\" class=\"internal-link\">offered contrarian pushback</a>: 'shitcoders might disappear' but deep system expertise remains valuable</li>\n</ul>\n<p><strong>Ethan Mollick</strong> <a href=\"/?date=2025-12-27&category=social#item-21d0a05ece5f\" class=\"internal-link\">observed the goalpost-moving pattern</a> with benchmarks—the Turing Test seemed insurmountable until AI passed it. Meanwhile, a <strong>NeurIPS 2025</strong> Best Paper Runner-up <a href=\"/?date=2025-12-27&category=social#item-7b2c227de988\" class=\"internal-link\">challenges whether RL</a> actually improves LLM reasoning. <a href=\"/?date=2025-12-27&category=social#item-c82eb0139a3e\" class=\"internal-link\">Market analysis</a> suggests <strong>ChatGPT's</strong> ~20x popularity advantage over <strong>Gemini</strong> creates a compounding data flywheel for dominance.</p>",
      "themes": [
        {
          "name": "Software Engineering Transformation",
          "description": "Major discussion about how AI tools are fundamentally changing programming, from being 10X more powerful to not opening IDEs at all. Key tension between 'programming is dying' vs 'skilled programmers will thrive'",
          "item_count": 10,
          "example_items": [],
          "importance": 95
        },
        {
          "name": "AI Progress Velocity",
          "description": "Observations about the unprecedented pace of AI capability improvements, with worldviews becoming 'deprecated' in 30 days",
          "item_count": 3,
          "example_items": [],
          "importance": 88
        },
        {
          "name": "AI Safety Movement Critique",
          "description": "Timnit Gebru's detailed criticism of Future of Life Institute, questioning their ideology, funding sources, and actual priorities versus stated AI safety concerns",
          "item_count": 7,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "AI Model Releases",
          "description": "New AI model availability announcements, specifically MiniMax-M2.1 on Hugging Face",
          "item_count": 1,
          "example_items": [],
          "importance": 78
        },
        {
          "name": "Human-AI Collaboration",
          "description": "Evolving dynamics of human-AI workflows, with humans becoming 'copilots' and need to adapt to AI-driven processes",
          "item_count": 4,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "AI Agents & Agentic AI",
          "description": "Discussion of AI agent capabilities, architectures, tools and distinctions between AI agents and agentic AI systems",
          "item_count": 8,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "Autonomous Vehicles & Sensor Fusion",
          "description": "Technical discussions of Tesla FSD, voxel mapping, localization systems, multi-sensor fusion, and handling edge cases like snow/sensor failures",
          "item_count": 14,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "TESCREAL Ideology",
          "description": "Discussion of tech elite ideological framework combining transhumanism, extropianism, singularitarianism, cosmicism, rationalism, effective altruism, and longtermism",
          "item_count": 4,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "LLM Architecture & Evolution",
          "description": "Technical developments in LLM systems including Meta's GEM architecture, Karpathy's predictions about ghost intelligence and ambient programming",
          "item_count": 4,
          "example_items": [],
          "importance": 72
        },
        {
          "name": "AI Benchmarks & Evaluation",
          "description": "Discussion of Turing Test, ARC-AGI, and the pattern of goalpost-moving when AI passes benchmarks; questions about measuring intelligence",
          "item_count": 5,
          "example_items": [],
          "importance": 70
        }
      ],
      "top_items": [
        {
          "id": "669a6053df59",
          "title": "I've never felt this much behind as a programmer. The profession is being dramatically refactored as...",
          "content": "I've never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become available over the last ~year and a failure to claim the boost feels decidedly like skill issue. There's a new programmable layer of abstraction to master (in addition to the usual layers below) involving agents, subagents, their prompts, contexts, memory, modes, permissions, tools, plugins, skills, hooks, MCP, LSP, slash commands, workflows, IDE integrations, and a need to build an all-encompassing mental model for strengths and pitfalls of fundamentally stochastic, fallible, unintelligible and changing entities suddenly intermingled with what used to be good old fashioned engineering. Clearly some powerful alien tool was handed around except it comes with no manual and everyone has to figure out how to hold it and operate it, while the resulting magnitude 9 earthquake is rocking the profession. Roll up your sleeves to not fall behind.",
          "url": "https://twitter.com/karpathy/status/2004607146781278521",
          "author": "@karpathy",
          "published": "2025-12-26T17:36:02",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Karpathy's landmark post on programming transformation: discusses agents, prompts, MCP, new abstraction layers, and being potentially 10X more powerful while feeling behind",
          "importance_score": 98,
          "reasoning": "Exceptional engagement (16M+ views, 55K likes), seminal statement from AI's most respected voice on fundamental shift in software engineering",
          "themes": [
            "Software engineering transformation",
            "AI agents",
            "Developer tools",
            "Skill adaptation"
          ],
          "continuation": null
        },
        {
          "id": "2d1d7d954f12",
          "title": "@karpathy I feel this way most weeks tbh. Sometimes I start approaching a problem manually, and have...",
          "content": "@karpathy I feel this way most weeks tbh. Sometimes I start approaching a problem manually, and have to remind myself “claude can probably do this”. Recently we were debugging a memory leak in Claude Code, and I started approaching it the old fashioned way: connecting a profiler, using the app, pausing the profiler, manually looking through heap allocations. My coworker was looking at the same issue, and just asked Claude to make a heap dump, then read the dump to look for retained objects that probably shouldn’t be there; Claude 1-shotted it and put up a PR. The same thing happens most weeks.\n\nIn a way, newer coworkers and even new grads that don’t make all sorts of assumptions about what the model can and can’t do — legacy memories formed when using old models — are able to use the model most effectively. It takes significant mental work to re-adjust to what the model can do every month or two, as models continue to become better and better at coding and engineering.\n\nThe last month was my first month as an engineer that I didn’t open an IDE at all. Opus 4.5 wrote around 200 PRs, every single line. Software engineering is radically changing, and the hardest part even for early adopters and practitioners like us is to continue to re-adjust our expectations. And this is *still* just the beginning.",
          "url": "https://twitter.com/bcherny/status/2004626064187031831",
          "author": "@bcherny",
          "published": "2025-12-26T18:51:12",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Boris Cherny describes transformative experience: Claude debugged memory leak by reading heap dump and 1-shotted fix; first month not opening IDE; Opus 4.5 wrote 200 PRs",
          "importance_score": 92,
          "reasoning": "Massive engagement (1.7M views), detailed firsthand account corroborating Karpathy's post about fundamental shift in engineering",
          "themes": [
            "Software engineering transformation",
            "Claude Code",
            "AI-assisted development",
            "Developer experience"
          ],
          "continuation": null
        },
        {
          "id": "36dc0ce60d80",
          "title": "@ibab It’s very good. People who aren’t keeping up even over the last 30 days already have a depreca...",
          "content": "@ibab It’s very good. People who aren’t keeping up even over the last 30 days already have a deprecated world view on this topic.",
          "url": "https://twitter.com/karpathy/status/2004621825180139522",
          "author": "@karpathy",
          "published": "2025-12-26T18:34:21",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Karpathy states AI progress is so rapid that not keeping up for even 30 days leaves you with 'deprecated world view'",
          "importance_score": 88,
          "reasoning": "Massive engagement (870K views), powerful statement about acceleration of AI capabilities from top researcher",
          "themes": [
            "AI progress velocity",
            "Industry dynamics"
          ],
          "continuation": null
        },
        {
          "id": "2be13842953b",
          "title": "@bcherny I have similar experiences. You point the thing around and it shoots pellets or sometimes e...",
          "content": "@bcherny I have similar experiences. You point the thing around and it shoots pellets or sometimes even misfires and then once in a while when you hold it just right a powerful beam of laser erupts and melts your problem.",
          "url": "https://twitter.com/karpathy/status/2004628491862696070",
          "author": "@karpathy",
          "published": "2025-12-26T19:00:51",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Karpathy uses metaphor comparing AI coding tools to an inconsistent weapon - sometimes misfires, sometimes produces powerful results when 'held just right'",
          "importance_score": 82,
          "reasoning": "Extremely high engagement (476K views), evocative metaphor capturing current AI tool unpredictability from top AI researcher",
          "themes": [
            "AI tool reliability",
            "Software engineering transformation"
          ],
          "continuation": null
        },
        {
          "id": "2818563e655b",
          "title": "2024: AI is the copilot\n2025+: humans are the copilot\nCopilot is the new engineering skill. It’s not...",
          "content": "2024: AI is the copilot\n2025+: humans are the copilot\nCopilot is the new engineering skill. It’s not easy to leave the driver seat - we must learn to think the AI way and adapt to the alien workflows. Help AI help ourselves.",
          "url": "https://twitter.com/DrJimFan/status/2004633997662716397",
          "author": "@DrJimFan",
          "published": "2025-12-26T19:22:43",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Jim Fan argues the paradigm is shifting from 'AI as copilot' to 'humans as copilot', emphasizing need to adapt to AI-driven workflows",
          "importance_score": 78,
          "reasoning": "High engagement (245K views), thought-provoking insight on human-AI collaboration dynamics from NVIDIA senior researcher",
          "themes": [
            "Future of work",
            "Human-AI collaboration",
            "AI workflows"
          ],
          "continuation": null
        },
        {
          "id": "25aa4a9b951a",
          "title": "Wrong. Shitcoders might disappear. Folks capable of writing code capable of finding a proprietary do...",
          "content": "Wrong. Shitcoders might disappear. Folks capable of writing code capable of finding a proprietary document in a billion-size collection in a matter of milliseconds, developing a driver for cutting-edge hardware, or developing a new niche OS will not disappear.\n\nBasically, the work will again belong to those who actually understand computers and know how to write optimal code.\n\nWant to know what it looked like before shitcoders came? Watch how Jordan Mechner made Prince of Persia in 1989: https://t.co/jNFis3LF6i",
          "url": "https://twitter.com/burkov/status/2004395343530594369",
          "author": "@burkov",
          "published": "2025-12-26T03:34:24",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Burkov argues 'shitcoders' may disappear but skilled programmers who understand systems deeply will remain valuable",
          "importance_score": 70,
          "reasoning": "High engagement (172K views), contrarian counterpoint to Karpathy's post about programming transformation",
          "themes": [
            "Software engineering transformation",
            "Programming skills",
            "Future of work"
          ],
          "continuation": null
        },
        {
          "id": "21d0a05ece5f",
          "title": "Its funny how much the Turing Test seemed like a giant insurmountable achievement for AI and then su...",
          "content": "Its funny how much the Turing Test seemed like a giant insurmountable achievement for AI and then suddenly AI passes it &amp; is now only worth concentrating on the Test’s many (real) flaws as a measure of thinking, Same thing with Theory of Mind, etc. Will happen with ARC-AGI, too",
          "url": "https://twitter.com/emollick/status/2004355265802670409",
          "author": "@emollick",
          "published": "2025-12-26T00:55:09",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Mollick observes pattern: Turing Test seemed insurmountable, AI passes it, then we focus on test's flaws. Predicts same for ARC-AGI",
          "importance_score": 72,
          "reasoning": "High engagement (73K views), insightful meta-observation about AI benchmark goalpost-moving",
          "themes": [
            "AI benchmarks",
            "ARC-AGI",
            "Turing Test",
            "Goalpost moving"
          ],
          "continuation": null
        },
        {
          "id": "7b2c227de988",
          "title": "Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model?\n\nA ...",
          "content": "Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model?\n\nA NeurIPS 2025 Best Paper Runner-up.\n\nChapterPal: https://t.co/hQhrldgejM\n\nPDF: https://t.co/K1gCNHWq2i https://t.co/pp16qfE6HY",
          "url": "https://twitter.com/burkov/status/2004352449398952447",
          "author": "@burkov",
          "published": "2025-12-26T00:43:57",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Burkov shares NeurIPS 2025 Best Paper Runner-up questioning if RL actually improves LLM reasoning beyond base model",
          "importance_score": 72,
          "reasoning": "Important research finding challenging assumptions about RL for LLM reasoning improvement",
          "themes": [
            "LLM research",
            "Reinforcement learning",
            "Reasoning capabilities",
            "Research findings"
          ],
          "continuation": null
        },
        {
          "id": "5840db125840",
          "title": "@krislipman I've been thinking more about that. There are actually 20 companies working with Niantic...",
          "content": "@krislipman I've been thinking more about that. There are actually 20 companies working with Niantic to build a world model or map with one-voxel-per-millimeter resolution. It is incredibly fine-grained. Once you map a street into voxels, as soon as the camera can see enough of a pattern in those voxels, it knows exactly where it is. This technology will be in phones and cars by next year.\n\nTesla currently has the best voxel map of the world—better than Niantic's—and it is more up-to-date because they have so many cars driving around, creating a real-time map.\n\nThis is crucial for difficult conditions:\n1. If a car is in a snowstorm and loses its main sensor, it can still use other inputs. Even if the rear camera is covered by snow (as it was in my case), you still have four side cameras and an interior camera.\n2. Even the interior camera can sometimes see landmarks, like a skyscraper through the back window, allowing the car to calculate its exact position in 3D space as it identifies more objects.\n3. On a clear day, this is easy, but in a heavy blizzard, visibility is obviously limited.\n\nYour idea of watching other cars is a great way to keep the system safer. It would reduce error rates over time for the IMU (Inertial Measurement Unit), which tracks directionality, bumps, and heading. There are also sensors on each wheel to monitor for slip.\n\nWhile a microphone won't help much in a snowstorm, the main goals are:\n* Seeing the red lights of the truck in front of you\n* Tracking the movement direction of other vehicles\n* Finding any evidence of lane markings via the side cameras\n\nIt will be interesting to see how they fully solve this. It seems straightforward in a straight line, but a curvy canyon road in a snowstorm is more complicated. Still, the same idea applies: if you can see a car in front of you and where they are on the road, you can tell if they are entering a turn. Other cars are definitely a vital data point—especially if one wrecks, which tells the system it needs to slow down and stop. It's a lot of fun to think about.",
          "url": "https://twitter.com/Scobleizer/status/2004469692682100907",
          "author": "@Scobleizer",
          "published": "2025-12-26T08:29:50",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Detailed technical explanation of voxel mapping for autonomous vehicles - comparing Tesla's real-time mapping vs Niantic's approach, discussing millimeter-resolution world models, multi-sensor fusion for difficult conditions including snowstorms",
          "importance_score": 72,
          "reasoning": "Exceptional technical depth on AV localization and mapping; explains voxel resolution, sensor redundancy, IMU integration; first-hand industry knowledge; highly informative",
          "themes": [
            "autonomous vehicles",
            "computer vision",
            "sensor fusion",
            "voxel mapping",
            "Tesla",
            "Niantic"
          ],
          "continuation": null
        },
        {
          "id": "c82eb0139a3e",
          "title": "ChatGPT is still nearly 20x more popular than Gemini.\n\nOnly it is worse. Most of those are defacto e...",
          "content": "ChatGPT is still nearly 20x more popular than Gemini.\n\nOnly it is worse. Most of those are defacto early adopters.\n\nEarly adopters give better data about the real world than late adopters. \n\nSo ChatGPT, I predict, will be far better when the glasses come because the bigger user base of early adopters will make them better quickly.\n\nOur data improves systems.",
          "url": "https://twitter.com/Scobleizer/status/2004450189399179266",
          "author": "@Scobleizer",
          "published": "2025-12-26T07:12:20",
          "source": "Twitter",
          "source_type": "twitter",
          "tags": [],
          "summary": "Analysis arguing ChatGPT's ~20x popularity advantage over Gemini means better training data from early adopters, predicting ChatGPT will dominate when AR glasses arrive",
          "importance_score": 68,
          "reasoning": "Original market analysis with strategic insight; connects user base advantage to model improvement; high engagement (148 likes, 33K views); forward-looking prediction about glasses",
          "themes": [
            "ChatGPT",
            "Gemini",
            "market analysis",
            "AI competition",
            "AR glasses",
            "data flywheel"
          ],
          "continuation": null
        }
      ]
    },
    "reddit": {
      "count": 97,
      "category_summary": "**Autonomous mathematical reasoning** dominated today's most significant discussions, with reports of **GPT-5** [solving an open problem](/?date=2025-12-27&category=reddit#item-69dbaace9677) in enumerative geometry and a separate AI system [tackling an **Erdős problem**](/?date=2025-12-27&category=reddit#item-29ee1a598674) without Lean verification. These milestones sparked debate about formal proof requirements versus raw problem-solving capability.\n\n- **Andrej Karpathy's** ['alien tech' warning](/?date=2025-12-27&category=reddit#item-5137fae5bd60) drew massive engagement, urging practitioners not to fall behind on AI adoption\n- **NVIDIA + Stanford's NitroGen** [generalist game-playing AI](/?date=2025-12-27&category=reddit#item-23cbf3c05b50) trained across 1,000+ games signals progress toward general-purpose agents\n- **Self-improving software agents** research [showed capability gains](/?date=2025-12-27&category=reddit#item-6e930a92f6ac) without human-labeled data, raising recursive improvement questions\n\n**r/MachineLearning** featured impressive efficiency work: **S2ID** [generates 1024x1024 images](/?date=2025-12-27&category=reddit#item-c5e33520f1bc) at 6.1M parameters, while **NOMA** [compiles autodiff to LLVM IR](/?date=2025-12-27&category=reddit#item-701d03bad831). **Liquid AI's 2.6B pure-RL model** claims to outperform larger competitors, and research shows **video models** [develop emergent 3D understanding](/?date=2025-12-27&category=reddit#item-5c4c8caad761) from 2D training alone.",
      "category_summary_html": "<p><strong>Autonomous mathematical reasoning</strong> dominated today's most significant discussions, with reports of <strong>GPT-5</strong> <a href=\"/?date=2025-12-27&category=reddit#item-69dbaace9677\" class=\"internal-link\">solving an open problem</a> in enumerative geometry and a separate AI system <a href=\"/?date=2025-12-27&category=reddit#item-29ee1a598674\" class=\"internal-link\">tackling an <strong>Erdős problem</strong></a> without Lean verification. These milestones sparked debate about formal proof requirements versus raw problem-solving capability.</p>\n<ul>\n<li><strong>Andrej Karpathy's</strong> <a href=\"/?date=2025-12-27&category=reddit#item-5137fae5bd60\" class=\"internal-link\">'alien tech' warning</a> drew massive engagement, urging practitioners not to fall behind on AI adoption</li>\n<li><strong>NVIDIA + Stanford's NitroGen</strong> <a href=\"/?date=2025-12-27&category=reddit#item-23cbf3c05b50\" class=\"internal-link\">generalist game-playing AI</a> trained across 1,000+ games signals progress toward general-purpose agents</li>\n<li><strong>Self-improving software agents</strong> research <a href=\"/?date=2025-12-27&category=reddit#item-6e930a92f6ac\" class=\"internal-link\">showed capability gains</a> without human-labeled data, raising recursive improvement questions</li>\n</ul>\n<p><strong>r/MachineLearning</strong> featured impressive efficiency work: <strong>S2ID</strong> <a href=\"/?date=2025-12-27&category=reddit#item-c5e33520f1bc\" class=\"internal-link\">generates 1024x1024 images</a> at 6.1M parameters, while <strong>NOMA</strong> <a href=\"/?date=2025-12-27&category=reddit#item-701d03bad831\" class=\"internal-link\">compiles autodiff to LLVM IR</a>. <strong>Liquid AI's 2.6B pure-RL model</strong> claims to outperform larger competitors, and research shows <strong>video models</strong> <a href=\"/?date=2025-12-27&category=reddit#item-5c4c8caad761\" class=\"internal-link\">develop emergent 3D understanding</a> from 2D training alone.</p>",
      "themes": [
        {
          "name": "AI Capability Milestones",
          "description": "Major achievements including autonomous mathematical problem solving (GPT-5, Erdős problem), self-improving agents, and emergent 3D understanding",
          "item_count": 6,
          "example_items": [],
          "importance": 85
        },
        {
          "name": "Expert Industry Perspectives",
          "description": "Influential voices including Karpathy, Jensen Huang, and Peter Gostev providing forecasts and guidance",
          "item_count": 4,
          "example_items": [],
          "importance": 75
        },
        {
          "name": "ML Infrastructure & Tools",
          "description": "Novel compilers (NOMA), interpretability toolkits, and systems-level approaches to neural networks",
          "item_count": 5,
          "example_items": [],
          "importance": 70
        },
        {
          "name": "Efficient Small Models",
          "description": "Scale-invariant diffusion at 6.1M params, Liquid AI's 2.6B RL model outperforming larger models",
          "item_count": 3,
          "example_items": [],
          "importance": 68
        },
        {
          "name": "Humanoid Robotics Scale-up",
          "description": "Manufacturing milestones (UBTECH 1000 units), industry overviews, and predictions about robot-driven economy",
          "item_count": 7,
          "example_items": [],
          "importance": 65
        },
        {
          "name": "Production ML & Model Comparison",
          "description": "Real-world deployment experiences, GPT-4 vs Claude comparisons, and gap between benchmarks and production utility",
          "item_count": 5,
          "example_items": [],
          "importance": 60
        },
        {
          "name": "Model Architecture & Research",
          "description": "Technical exploration of model architectures including hybrid models, JEPA evaluation, and experimental SLMs",
          "item_count": 3,
          "example_items": [],
          "importance": 60
        },
        {
          "name": "AI Safety & Security",
          "description": "Prompt injection limitations, LLM interpretability, jailbreak detection",
          "item_count": 4,
          "example_items": [],
          "importance": 55
        },
        {
          "name": "AI Product Ecosystem & Competition",
          "description": "Comparative analysis of AI platforms, custom GPTs as competitive advantage, and ecosystem lock-in",
          "item_count": 2,
          "example_items": [],
          "importance": 55
        },
        {
          "name": "AI-Assisted Research Workflows",
          "description": "Using AI tools in scientific research including physics simulations and computational linguistics",
          "item_count": 2,
          "example_items": [],
          "importance": 50
        }
      ],
      "top_items": [
        {
          "id": "69dbaace9677",
          "title": "For the first time, an AI model (GPT-5) autonomously solved an open math problem in enumerative geometry",
          "content": "Paper: [https://arxiv.org/abs/2512.14575](https://arxiv.org/abs/2512.14575)",
          "url": "https://reddit.com/r/OpenAI/comments/1pw7sl3/for_the_first_time_an_ai_model_gpt5_autonomously/",
          "author": "u/MetaKnowing",
          "published": "2025-12-26T11:07:16",
          "source": "r/OpenAI",
          "source_type": "reddit",
          "tags": [
            "News"
          ],
          "summary": "Report that GPT-5 autonomously solved an open mathematical problem in enumerative geometry without human assistance",
          "importance_score": 85,
          "reasoning": "Major capability milestone demonstrating frontier AI solving previously unsolved math problems, high engagement with academic paper reference",
          "themes": [
            "AI capabilities",
            "mathematical reasoning",
            "research milestones"
          ],
          "continuation": null
        },
        {
          "id": "29ee1a598674",
          "title": "First AI system to solve an erdos problem without any human input and without lean",
          "content": "[https://archivara.org/pdf/df04f023-6ef0-4c52-bd12-18cdaa8f0741](https://archivara.org/pdf/df04f023-6ef0-4c52-bd12-18cdaa8f0741)",
          "url": "https://reddit.com/r/accelerate/comments/1pw5kui/first_ai_system_to_solve_an_erdos_problem_without/",
          "author": "u/gbomb13",
          "published": "2025-12-26T09:29:44",
          "source": "r/accelerate",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Report that an AI system solved an Erdős problem without human input and without formal verification tools like Lean",
          "importance_score": 80,
          "reasoning": "Major mathematical reasoning milestone demonstrating autonomous problem-solving capability",
          "themes": [
            "AI capabilities",
            "mathematical reasoning",
            "research milestones"
          ],
          "continuation": null
        },
        {
          "id": "5137fae5bd60",
          "title": "Andrej Karpathy: Powerful Alien Tech Is Here---Do Not Fall Behind",
          "content": "",
          "url": "https://reddit.com/r/singularity/comments/1pwhgre/andrej_karpathy_powerful_alien_tech_is_heredo_not/",
          "author": "u/Neurogence",
          "published": "2025-12-26T17:50:35",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Andrej Karpathy's statement that 'powerful alien tech is here' and warning about falling behind in AI adoption",
          "importance_score": 82,
          "reasoning": "High-profile expert opinion from former Tesla AI director with massive engagement, influential industry perspective",
          "themes": [
            "industry leadership",
            "AI adoption",
            "expert opinions"
          ],
          "continuation": null
        },
        {
          "id": "6e930a92f6ac",
          "title": "Software Agents Self Improve without Human Labeled Data",
          "content": "[Tweet](https://x.com/YuxiangWei9/status/2003541373853524347?s=20)\n\n[Paper](https://arxiv.org/abs/2512.18552)",
          "url": "https://reddit.com/r/singularity/comments/1pw795e/software_agents_self_improve_without_human/",
          "author": "u/SrafeZ",
          "published": "2025-12-26T10:44:19",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Research paper showing software agents can self-improve without human-labeled data",
          "importance_score": 78,
          "reasoning": "Significant research direction for autonomous AI improvement, high engagement with paper link",
          "themes": [
            "self-improving AI",
            "research papers",
            "AI agents"
          ],
          "continuation": null
        },
        {
          "id": "23cbf3c05b50",
          "title": "NVIDIA + Stanford just dropped NitroGen, \"plays-any-game\" AI trained on 40,000 hours of gameplay across 1,000+ games.",
          "content": "",
          "url": "https://reddit.com/r/accelerate/comments/1pvwqvv/nvidia_stanford_just_dropped_nitrogen/",
          "author": "u/Status-Platform7120",
          "published": "2025-12-26T00:39:26",
          "source": "r/accelerate",
          "source_type": "reddit",
          "tags": [
            "News"
          ],
          "summary": "NVIDIA and Stanford released NitroGen, an AI trained on 40,000 hours of gameplay across 1,000+ games to play any game",
          "importance_score": 70,
          "reasoning": "Significant research from major institutions on generalist game-playing AI",
          "themes": [
            "game AI",
            "research papers",
            "generalist agents"
          ],
          "continuation": null
        },
        {
          "id": "c5e33520f1bc",
          "title": "[P] S2ID: Scale Invariant Image Diffuser - trained on standard MNIST, generates 1024x1024 digits and at arbitrary aspect ratios with almost no artifacts at 6.1M parameters (Drastic code change and architectural improvement)",
          "content": "^(This is an update to the previous post which can be found) [^(here)](https://www.reddit.com/r/MachineLearning/comments/1puosfp/p_siid_a_scale_invariant_pixelspace_diffusion/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)^(. Take a look if you want the full context, but it's not necessary as a fair amount of things have been changed and improved. The GitHub repository can be found) [^(here)](https://github.com/Yegor-men/scale-invariant-image-diffuser)^(. Once again, forgive for the minimal readme, unclean train/inference code, as well as the usage of .pt and not .safetensors modelfiles. The focus of this post is on the architecture of the model.)\n\n# Preface\n\nHello everyone.\n\nOver the past couple weeks/months, annoyed by a couple pitfalls in classic diffusion architecures, I've been working on my own architecture, aptly named **S2ID: Scale Invariant Image Diffuser**. S2ID aims to avoid the major pitfalls found in standard diffusion architectures. Namely:\n\n* UNet style models heavily rely on convolution kernels, and convolution kernels train to a certain pixel density. If you change your pixel density, by upscaling the image for example, the feature detectors residing in the kernels no longer work, as they are now of a different size. This is why for models like SDXL, changing the resolution at which the model generates can easily create doubling artifacts.\n* DiT style models would treat the new pixels produced by upscaling as if they were actually new and appended to the edges of the image. RoPE helps generalize, but is there really a guarantee that the model knows how to \"compress\" context length back down to the actual size?\n\nFundamentally, it boils down to this: **Tokens in LLMs are atomic, pixels are not, the resolution (pixel density) doesn't affect the amount of information present, it simply changes the quality of the information**. Think of it this way: when your phone takes a 12mp photo, or a 48mp photo, or a 108mp photo, does the actual composition change? Do you treat the higher resolution image as if it had suddenly gained much more information? Not really. Same here: resolution is not a key factor. Resolution, or more importantly, pixel density, doesn't change the underlying function of the data. Hence, the goal of S2ID is to learn the underlying function, ignoring the \"view\" we have of it that's defined by the aspect ratio and image size. Thus, S2ID is tested to generalize on **varying image sizes and varying aspect ratios**. In the current iteration, **no image augmentation** was used during training, making the results all the more impressive.\n\nAs a side \"challenge\", S2ID is trained locally on my RTX 5080 and I do not intend to move to a server GPU unless absolutely necessary. The current iteration was trained in 20 epochs, batch size 25, AdamW with cosine scheduler with 2400 warmup steps going from 0 to 1e-3, and then decaying down to 1e-5. S2ID is an EMA model with 0.9995 decay (dummy text encoder is not EMA). VRAM consumption was at 12.5 GiB throughout training, total training took about 3 hours, although each epoch was trained in about 6m20s, the rest of the time was spent on intermediate diffusion and the test dataset. As said in the title, the parameter count is 6.1M, although I'm certain that it can be reduced more as in (super duper) early testing months ago I was able to get barely recognizable digits at around 2M, although that was very challenging.\n\n# Model showcase\n\nFor the sake of the showcase, it's critical to understand that **the model was trained on standard 28x28 MNIST images** ***without any augmentations***. The only augmentation used is the coordinate jitter (explained further later on), but I would argue that this is a core component of the architecture itself, not an augmentation to the images, as it is the backbone behind what allows the model to scale well beyond the training data and why it is that it learns the continuous function in the first place and doesn't just memorize coordinates like it did last time.\n\nLet us start with the elephant in the room, and that is the 1MP SDXL-esque generation. 1024 by 1024 images of the digits. Unfortunately, with the current implementation (issues and solutions described later) I'm hitting OOM for batch size of 10, so I'm forced to render one at a time and crappily combine them together in google sheets:\n\n[Grid of numbers, each one diffused at 1024x1024](https://preview.redd.it/65ssouxyrl9g1.png?width=665&amp;format=png&amp;auto=webp&amp;s=3348324b64bf7b830fd87db998a27b66c2007629)\n\nAs you can see, very clean digits. In fact, the model actually seems to have an easier time diffusing at larger resolutions, there's less artifacts, although admittedly the digits are much more uniform to each other. I'll test how this holds up when I change training from MNIST to CelebA.\n\nNow, let's take a look at the other results, namely for 1:1 trained, 2:3, 3:4, 4:5 and the dreaded 9:16 from last time and see how S2ID holds up this time:\n\n[1:1 - 28x28](https://preview.redd.it/hfg133rxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=b03a59cc5ee0b05803de88b5d1efe9e6d0a88c87)\n\n[2:3 - 27x18](https://preview.redd.it/7fvas2rxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=22ed0489be43b605510b5acccd22c362ba182cff)\n\n[3:2 - 18x27](https://preview.redd.it/mrtj64rxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=3b686da105f38671102690749311a47c0422d868)\n\n[3:4 - 32x24](https://preview.redd.it/k5wrsfrxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=6bc502fc1a97082edc7eca94c2e750c5601262a8)\n\n[4:3 - 24x32](https://preview.redd.it/e069jwtxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=11780e75214927390820b023c02997aefa816ea6)\n\n[4:5 - 30x24](https://preview.redd.it/kth3tltxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=a0177338e5184969d4a80392f76ad5de5ebf0391)\n\n[5:4 - 24x30](https://preview.redd.it/3drv5grxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=ddc1292978202dea98506d79c59380f869b14dac)\n\n[9:16 - 32x18](https://preview.redd.it/gbcvvfrxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=e8fbbdb37c02561574ce170b2ce3fde010ec649a)\n\n[16:9 - 18x32](https://preview.redd.it/ay3hpfrxol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=58037c6283c592d2fe49f6576a568fe3347ebce9)\n\nLike with the 1024x1024, the results are significantly better than in the last iteration. A lot less artifacts, even when we're really testing the limits with the 16:9 aspect ratio, as the coordinates become quite ugly in that scenario with the way the coordinate system works. Nevertheless, S2ID successfully seems to generalize: it applies a combination of a squish + crop whenever it has to, such that the key element of the image: the digit, doesn't actually change that much. Considering the fact that the model was trained on unaugmented data and still yields these results indicates great potential.\n\nAs last time, a quick look at double and quadruple the trained resolution. But unlike the last time, you'll see that this time around the results are far better cleaner and more accurate, at the expense of variety:\n\n[Double Resolution - 56x56](https://preview.redd.it/288wgozpol9g1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=742f9401d68305b59131deb93cbd3ac633975a08)\n\n[Quarduple resolution - 128x128](https://preview.redd.it/d0p44qzpol9g1.png?width=800&amp;format=png&amp;auto=webp&amp;s=06411795c0525c9eb5058d0ecff7ed3e42251689)\n\nFor completion, here is the t-scrape loss. It's a little noisy, which suggest to me that I should use the very same gaussian noisifying coordinate jitter technique used for the positioning, but that's for the next iteration:\n\n[T scrape loss, noisy but better than last time](https://preview.redd.it/wvkjc1ftol9g1.png?width=640&amp;format=png&amp;auto=webp&amp;s=fd2d587da70cae59b6dd2163e4d977553921f92f)\n\n# How does S2ID work?\n\nThe previous post/explanation was a bit of an infodump, I'll try to explain it a bit clearer this time, especially considering that some redundant parts were removed/replaced, the architecture is a bit simpler now.\n\nIn short, as the goal of S2ID is to be a scale invariant model, it treats the data accordingly. The images, when fed into the model, are a fixed grid that represent a much more elegant underlying function that doesn't care about the grid nature. So our goal is to approach the data as such. First, each pixel's coordinates is calculated as an exact value from -0.5 to 0.5 along the x and y axis. Two values are obtained: the coordinate relative to the image, and the coordinate relative to the composition. The way that the coordinate relative to the composition works is that we inscribe the image and whatever aspect ratio it is into a 1:1 square, and then project the pixels of the image on to the square. This allows the model to learn composition, and not stretch it as the distance between the pixels is uniform. The second coordinate system, the one relative to the image, simply assigns all the image edges the respective +- 0.5, and then have a linspace assign the values along there. The gap between pixels varies, but the model now knows how far the pixel is from the edge. If we only used the first system of coordinates, the model would ace composition, but would simply crop out the subject if the aspect ratio changed. If we used only the second system of coordinates, the model would never crop, but then at the same time it would always just squish and squeeze the subject. It is with these two systems together that the model generalizes. Next up is probably the most important part of it all: and that is turning the image from pixel space into more or less a function. We do not use FFT or anything like that. Instead, we add gaussian noise to the coordinates with a dynamic standard deviation such that the model learns that the pixels isn't the data, it's just one of the many views of the data, and the model is being trained on other, alternative views that the data could have been. We effectively treat it like this: \"If our coordinates are \\[0.0, 0.1, 0.2, ...\\], then what we really mean to say is that 0.1 is just the most likely coordinate of that pixel, but it could have been anything\". Applying gaussian noise does exactly this: jitters around the pixel's coordinates, but not their values, as an alternative, valid view of the data. Afterwards, we calculate the position vector via RoPE, but we use increasing instead of decreasing frequencies. From there, we simply use transformer blocks with axial but without cross attention so that the model understands the composition, then transformer blocks with axial and cross attention so that the model can attend to the prompt, and then we de-compress this back to the number of color channels and predicts the epsilon noise. As a workflow, it looks like this:\n\n1. Calculate the relative positioning coordinates for the each pixel in the image\n2. Add random jitter to each positioning system\n3. Turn the jittered coordinates into a per-pixel vector via fourier series, akin to RoPE, but we use ever-increasing frequencies instead\n4. Concatenate the coordinate vector with the pixel's color values and pass though a single 1x1 convolution kernel to expand to d\\_channels\n5. Pass the latent through a series of encoder blocks: it's transformers on axial attention, but no cross attention so that the model understands composition first\n6. Pass the attended latent though the decoder blocks that have axial and cross attention\n7. Pass the fully attended latent through a 1x1 convolution kernel to create and predict the epsilon noise\n\nThis is obviously a simplification, and you can read the full code on the repository linked above if you want (although like I said before, forgive for the messy code, I'd like to get the architecture to a stable state first, and then do one massive refactor to clean everything up). The current architecture also heavily employs FiLM time modulation, dropouts, residual and skip connections, and the encoder/decoder block (just the names I picked) make it so that the model should in theory work like FLUX Kontext as well, as the model understands composition before the actual text conditioning implementation.\n\n# What changed from the previous version?\n\nIn the [previous post](https://www.reddit.com/r/MachineLearning/comments/1puosfp/p_siid_a_scale_invariant_pixelspace_diffusion/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button), I asked for suggestions and improvements. One that stood out was by u/BigMrWeeb and u/cwkx to look into infinity diffusion. The core concept there is to model the underlying data as a function, and diffuse on the function, not the pixels. I read the paper, and while I can't say that I agree with the approach as compressing an image down to a certain fixed number of functions is not much different to learning it at a fixed resolution and then downscaling/upscaling accordingly; I must say that it has helped me understand/formalize the approach better, and it has helped me solve the key issue of artifacts. Namely:\n\n* In the previous iteration, during training, each pixel got a fixed coordinate that would be then used for the positioning system. However, the coordinates are a continuous system, not discrete. So when training, the model didn't have any incentive to learn the continuous distribution. This time around, in order to force the model to understand the continuity, each pixel's coordinates are jittered. During training, to the true coordinate is added a random value, a sample from a gaussian distribution with a mean of 0 and a standard deviation of half the distance between the pixel and the adjacent pixel. The idea here being that now, the model is generalizing to a smooth interpolation between the pixels. A gaussian distribution was chosen after a quick test with a uniform, since gaussian naturally better represents the \"uncertainty\" of the value of each pixel, while uniform is effectively a nearest-exact. The sum of all the gaussian distributions is pretty close to 1, with light wobble, but I don't think that this should be a major issue. Point being, the model now learns the coordinate system as smooth and continuous rather than discrete, allowing it to generalize into aspect ratios/resolutions well beyond trained.\n* With the coordinate system now being noisy, this means that we are no longer bound by the frequency count. Previously, I had to restrict the number of powers I could use, since beyond a certain point frequencies are indistinguishable from noise. However, this problem only makes sense when we're taking samples at fixed intervals. But with the added noise, we are not, we now have theoretical infinite accuracy. Thus, the new iteration was trained on a frequency well beyond what's usable for the simple 28x28 size. The highest frequency period is 128pi, and yet the model does not suffer. With the \"gaussian blur\" of the coordinates, the model is able to generalize and learn what those high frequencies mean, even though they don't actually exist in the training data. This also helps the model to diffuse at higher resolutions and make use of those higher frequencies to understand local details.\n* In the previous iteration, I used pixel unshuffle to compress the height and width into color channels. I experienced artifacts as early as 9:16 aspect ratio where the latent height/width was double what was trained. I was able to pinpoint the culprit of this error, and that was the pixel unshuffle. The pixel unshuffle is not scale invariant, and thus it was removed, the model is working on the pixel space directly.\n* With the pixel unshuffle removed, each token is now smaller by channel count, which is what allowed to decrease the parameter count down to 6.1M parameters. Furthermore, no new information is added by bicubic upscaling the image to 64x64, thus the model trains on 28x28 directly, and the gaussian coordinate jittering allows the model to generalize this data to a general function, the number of pixels you show to the image is only the amount of data you have, the accuracy of the function, nothing more.\n* With everything changed, the model is now more friendly with CFG and eta, doesn't need it to be as high, although I couldn't be bothered experimenting around.\n\n# Further improvements and suggestions\n\nAs mentioned, S2ID now diffuses in raw pixel space. This is both good and bad. From the good side, it's now truly scale invariant and the outputs are far cleaner. From the bad side, it takes longer to train. However, there are ways to mitigate it that I suppose are worth testing out:\n\n* Using S2ID as a latent diffusion model, use a VAE to compress the height and width down. The FLUX/SDXL vae compresses the height and width by 8x, resulting in a latent size of 128 for 1024 size images. A sequence length of 128 is already far more manageable than the 1024 by 1024 images that I've showcased here since this current iteration is working in pixel space. VAEs aren't exactly scale invariant, but oh well, sacrifices must be made I suppose.\n* Randomly drop out pixels/train in a smaller resolution. As mentioned before, the way that the gaussian noise is used, it forces the model to learn a general distribution and function to the data, not to just memorize coordinates. The fact that it learnt 28x28 data but has learnt to render good images at massive resolutions, or even at double resolutions, seems to suggest that you can simply feed in a lower resolution verison of the image and still get decent data. I will test this theory out by training on 14x14 MNIST. However, this won't speed up inference time like VAE will, but I suppose that both of these approaches can be used. As I say this now, talking about training on 14x14, this reminds me of how you can de-blur a pixelated video as long as the camera is moving. Same thing here? Just blur the digits properly instead of blindly downscaling, i.e. upscale via bicubic, jitter, downscale, and then feed that into the model. Seems reasonable.\n* Replace the MHA attention with FlashAttention or Linear Transformers. Honestly I don't know what I think about this, feels like a patch rather than an improvement, but it certainly is an option.\n* Words cannot describe how unfathomably slow it is to diffuse big resolutions, this is like the number 1 priority now. On the bright side, they require SIGNIFICANTLY less diffusion steps. Less than 10 is enough.\n\nNow with that being said, I'm open to critique, suggestions and questions. Like I said before, please forgive the messy state of the code, I hope you can understand my disinterest in cleaning it up when the architecture is not yet finalized. Frankly I would not recommend running the current ugly code anyway as I'm likely to make a bunch of changes and improvements in the near future; although I do understand how this looks more shady. I hope you can understand my standpoint.\n\nKind regards.",
          "url": "https://reddit.com/r/MachineLearning/comments/1pwd8su/p_s2id_scale_invariant_image_diffuser_trained_on/",
          "author": "u/Tripel_Meow",
          "published": "2025-12-26T14:51:31",
          "source": "r/MachineLearning",
          "source_type": "reddit",
          "tags": [
            "Project"
          ],
          "summary": "Project showcase of S2ID, a scale-invariant image diffusion model trained on standard MNIST that generates 1024x1024 images at arbitrary aspect ratios with only 6.1M parameters",
          "importance_score": 75,
          "reasoning": "Novel technical contribution with efficient architecture, demonstrates impressive capability-to-parameter ratio, good community engagement",
          "themes": [
            "diffusion models",
            "research projects",
            "efficient ML"
          ],
          "continuation": null
        },
        {
          "id": "701d03bad831",
          "title": "[P] NOMA: Neural networks that realloc themselves during training (compile-time autodiff to LLVM IR)",
          "content": "I’m the author of NOMA (Neural-Oriented Machine Architecture), an experimental systems language + compiler where reverse-mode autodiff is implemented as a compiler pass (Rust → LLVM IR). The goal is to make gradient-based training feel like a systems primitive, producing standalone native binaries.\n\nRepo: [https://github.com/pierridotite/Noma](https://github.com/pierridotite/Noma)\n\n# What’s different (vs typical Python frameworks)\n\nIn PyTorch/TensorFlow, a neural network is effectively an object hierarchy. If you want to change topology mid-training (dynamic capacity, grow/prune, neuroevolution-style experiments), you typically end up doing: stop the loop → rebuild objects → copy weights → rebuild optimizer state → resume.\n\nIn NOMA, a network is treated as a managed memory buffer. Growing capacity is a language primitive:\n\n* alloc / realloc / free are explicit\n* the compiler’s AD pass remaps gradients to the new layout\n* the intent is to preserve optimizer state across growth events (e.g., momentum/Adam moments) by mapping previous slots into the expanded buffer\n\n# [XOR Demo Loss](https://github.com/pierridotite/NOMA/tree/main/demo_self_growing_xor)\n\nThis benchmark evaluates the performance of a self-growing neural network that:\n\n1. Starts with 2 hidden neurons\n2. Trains on XOR until a fixed step (growth trigger)\n3. Expands to 16 hidden neurons\n4. Continues training until convergence (loss &lt; 0.002)\n\nAll implementations share identical initial weights and hyperparameters to ensure fair comparison.\n\n[XOR Training with NOMA language](https://preview.redd.it/zs8u6ux16z9g1.png?width=2083&amp;format=png&amp;auto=webp&amp;s=a797c3e0ada9cfcf8ce58375bd3b0f32c23bb35e)\n\n# Current status (alpha)\n\nImplemented:\n\n* Reverse-mode autodiff as a compiler pass\n* LLVM IR codegen → native compilation\n* Optimizers: SGD, Adam, RMSprop\n* Tensor ops (incl. broadcasting), user-defined functions\n* Dynamic memory: alloc/realloc/free\n* Batch training\n* File I/O: CSV + safetensors\n* Interpreter mode for rapid iteration\n* VS Code extension (syntax highlighting/snippets)\n\nKnown limitations / not done yet:\n\n* Single numeric type (f64) only\n* Single-file programs (no module system/imports yet)\n* Control flow is limited (loops currently handled via unrolling; true runtime CFG/phi nodes not implemented)\n* Minimal debugging/tooling\n\n# What I’m looking for (feedback + contributors)\n\nIf you’re into compilers / LLVM / ML systems, I’d appreciate feedback (or PRs) in these areas:\n\n* **LLVM backend**: true control flow (phi nodes) instead of loop unrolling\n* **GPU backend**: expand PTX/CUDA kernel generation beyond the current stub\n* **Stdlib**: higher-level layers (Conv2D, LSTM), more ops, better numerics\n* **Tooling**: error messages, debugging, multi-file projects/imports\n\n# Questions for the community\n\n1. What’s the cleanest design for AD + true runtime control flow (branches/loops) while keeping gradients correct and efficient in LLVM IR?\n2. For the realloc growth primitive: what semantics would you recommend for optimizer-state remapping when tensors expand (esp. Adam moments)?\n3. Any prior art I should study that is closest to “compiler-first autodiff + explicit memory/topology semantics”?\n\nRepo again: [https://github.com/pierridotite/Noma](https://github.com/pierridotite/Noma)",
          "url": "https://reddit.com/r/MachineLearning/comments/1pw4jco/p_noma_neural_networks_that_realloc_themselves/",
          "author": "u/Cylicium",
          "published": "2025-12-26T08:40:30",
          "source": "r/MachineLearning",
          "source_type": "reddit",
          "tags": [
            "Project"
          ],
          "summary": "NOMA is a systems language and compiler implementing reverse-mode autodiff as a compiler pass from Rust to LLVM IR, producing standalone native binaries for neural network training",
          "importance_score": 72,
          "reasoning": "Innovative systems-level approach to ML that treats gradient-based training as a systems primitive, technically deep project with novel architecture",
          "themes": [
            "ML infrastructure",
            "compilers",
            "systems programming"
          ],
          "continuation": null
        },
        {
          "id": "94415860b3ce",
          "title": "Peter Gostev (LM Arena) shares 26 probability-weighted predictions for AI in 2026",
          "content": "AI capability analyst **Peter Gostev** (LM Arena) just now published a set of **26 predictions for 2026**, each framed as plausible rather than certain (roughly 5–60% confidence). The list spans models, agents, infrastructure and AI economics, focusing on capability trends rather than hype.\n\n**China:**\n1. A Chinese open model **leads** Web Dev Arena for 1+ months.\n2. Chinese labs open source **less** than 50% of their top models. \n3. Chinese labs take #1 spots in **both** image and video generation for at least 3 months.  \n\n**Media &amp; Multimodality:**\n\n4. No diffusion-only image models in the top 5 by mid-2026  \n5. Text, video, audio, music, and speech merge into a single model  \n6. Rapid growth in “edgy” applications like companions and erotica  \n7. First mainstream AI-generated short film gains major recognition\n\n**Agents:**\n\n8. Computer-use agents break through and go mainstream  \n9. A model productively works for over 48 hours on a real task  \n10. New product surfaces emerge to support long-running agents  \n\n**Research &amp; Capabilities:**\n\n11. First 1-GW-scale models reach 50%+ on hardest benchmarks (FrontierMath L4, ARC-AGI-3)  \n12. One fundamental issue gets solved (e.g. long-context reliability, hallucinations down 90%, or 10× data efficiency)  \n13. RL scaling in LLMs saturates, followed by a new scaling law  \n14. No major breakthroughs in small phone models, interpretability, diffusion-for-coding, or transformer alternatives  \n\n**Products &amp; Markets:**\n\n15. A new AI voice product hits 50M+ weekly active users  \n16. A solo founder reaches $50M ARR  \n17. SSI releases a product  \n18. Unexpected moves from Meta or Apple  \n19. OpenAI earns over 50% of revenue from ads, forcing a strategy shift  \n20. At least one prominent AI figure claims AGI has been reached  \n\n**Deals &amp; Industry Shifts:**\n\n21. AI labs spend $10B+ acquiring strong non-AI companies  \n22. A major lab spin-out (20+ people, $5B+ raise) occurs  \n23. Another “DeepSeek moment” briefly knocks NVIDIA stock down 10%+  \n\n**Infrastructure Constraints:**\n\n24. NVIDIA makes a major move into energy  \n25. A public fight over data-center expansion causes real delays  \n26. AI supply chains visibly strain, slowing deployment timelines  \n\nThese are not forecasts of inevitability, but **bounded bets** on where acceleration, constraints and economic pressure may surface next.\n\n**Source: Peter Gostev (LM Arena)**\n\n🔗: https://x.com/i/status/2004454044417343935\n",
          "url": "https://reddit.com/r/singularity/comments/1pvz6ei/peter_gostev_lm_arena_shares_26/",
          "author": "u/BuildwithVignesh",
          "published": "2025-12-26T03:10:39",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "The Singularity is Near"
          ],
          "summary": "AI capability analyst Peter Gostev shares 26 probability-weighted predictions for AI in 2026 covering Chinese models, open source, agents, and economics",
          "importance_score": 65,
          "reasoning": "Structured expert predictions with probability weights from LM Arena analyst, good engagement",
          "themes": [
            "AI predictions",
            "expert analysis",
            "industry forecasts"
          ],
          "continuation": null
        },
        {
          "id": "5c4c8caad761",
          "title": "Video Generation Models Trained on Only 2D Data Understand the 3D World",
          "content": "**Paper Title:** How Much 3D Do Video Foundation Models Encode?\n\n**Abstract:**\n\n&gt;Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. **In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data**. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
          "url": "https://reddit.com/r/singularity/comments/1pw9z5y/video_generation_models_trained_on_only_2d_data/",
          "author": "u/simulated-souls",
          "published": "2025-12-26T12:36:42",
          "source": "r/singularity",
          "source_type": "reddit",
          "tags": [
            "AI"
          ],
          "summary": "Research finding that video generation models trained only on 2D data develop understanding of 3D world properties",
          "importance_score": 62,
          "reasoning": "Important finding about emergent 3D understanding from 2D training, solid technical paper",
          "themes": [
            "video models",
            "emergent capabilities",
            "3D understanding"
          ],
          "continuation": null
        },
        {
          "id": "afeced4fe3d3",
          "title": "Genesis-152M-Instruct — Hybrid GLA + FoX + Test-Time Training at small scale",
          "content": "Hey everyone 👋\n\nI’m sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model — especially under **tight data constraints**.\n\n\n\nThis is **research-oriented**, not a production model or SOTA claim.\n\n\n\n\n\n🔍 **Why this might be interesting**\n\n\n\nMost recent architectures (GLA, FoX, TTT, µP, sparsity) are tested **in isolation** and usually at **large scale**.\n\nI wanted to answer a simpler question:\n\n\n\n*How much can architecture compensate for data at \\~150M parameters?*\n\n\n\nGenesis combines several **ICLR 2024–2025 ideas** into one model and evaluates the result.\n\n\n\n\n\n⚡ **TL;DR**\n\n• **152M parameters**\n\n• Trained on **\\~2B tokens** (vs \\~2T for SmolLM2)\n\n• Hybrid **GLA + FoX attention**\n\n• **Test-Time Training (TTT)** during inference\n\n• **Selective Activation (sparse FFN)**\n\n• **µP-scaled training**\n\n• Fully open-source (Apache 2.0)\n\n\n\n🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n📦 pip install genesis-llm\n\n\n\n\n\n📊 **Benchmarks (LightEval, Apple MPS)**\n\n\n\nARC-Easy     → 44.0%   (random: 25%)\n\nBoolQ        → 56.3%   (random: 50%)\n\nHellaSwag    → 30.2%   (random: 25%)\n\nSciQ         → 46.8%   (random: 25%)\n\nWinogrande   → 49.1%   (random: 50%)\n\n\n\n**Important context:**\n\nSmolLM2-135M was trained on **\\~2 trillion tokens**.\n\nGenesis uses **\\~2 billion tokens** — so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.\n\n\n\n\n\n🧠 **Architecture Overview**\n\n\n\n**Hybrid Attention (Qwen3-Next inspired)**\n\n\n\n**Layer** **%** **Complexity** **Role**\n\nGated DeltaNet (GLA) 75% O(n) Long-range efficiency\n\nFoX (Forgetting Attention) 25% O(n²) Precise retrieval\n\n\n\nGLA uses:\n\n• Delta rule memory updates\n\n• Mamba-style gating\n\n• L2-normalized Q/K\n\n• Short convolutions\n\n\n\nFoX adds:\n\n• Softmax attention\n\n• Data-dependent forget gate\n\n• Output gating\n\n\n\n\n\n**Test-Time Training (TTT)**\n\n\n\nInstead of frozen inference, Genesis can **adapt online**:\n\n• Dual-form TTT (parallel gradients)\n\n• Low-rank updates (rank=4)\n\n• Learnable inner learning rate\n\n\n\nPaper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)\n\n\n\n\n\n**Selective Activation (Sparse FFN)**\n\n\n\nSwiGLU FFNs with **top-k activation masking** (85% kept).\n\nCurrently acts as **regularization** — real speedups need sparse kernels.\n\n\n\n\n\n**µP Scaling + Zero-Centered RMSNorm**\n\n• Hyperparameters tuned on small proxy\n\n• Transferred via µP rules\n\n• Zero-centered RMSNorm for stable scaling\n\n\n\n\n\n⚠️ **Limitations (honest)**\n\n• Small training corpus (2B tokens)\n\n• TTT adds \\~5–10% inference overhead\n\n• No RLHF\n\n• Experimental, not production-ready\n\n\n\n\n\n📎 **Links**\n\n• 🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n• 📦 PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)\n\n\n\n\n\nI’d really appreciate feedback — especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.\n\n\n\n*Built by Orch-Mind Team*",
          "url": "https://reddit.com/r/deeplearning/comments/1pw9z98/genesis152minstruct_hybrid_gla_fox_testtime/",
          "author": "u/Kassanar",
          "published": "2025-12-26T12:36:48",
          "source": "r/deeplearning",
          "source_type": "reddit",
          "tags": [],
          "summary": "Introducing Genesis-152M-Instruct, an experimental small language model combining GLA, FoX, and Test-Time Training architectures to study how recent techniques interact under tight data constraints",
          "importance_score": 65,
          "reasoning": "Technical depth exploring architectural combinations (GLA, FoX, TTT, µP, sparsity) at small scale. Research-oriented with clear experimental motivation despite low engagement.",
          "themes": [
            "Model Architecture",
            "Small Language Models",
            "Research Experimentation",
            "Hybrid Architectures"
          ],
          "continuation": null
        }
      ]
    }
  }
}